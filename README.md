# MLDL2021

BERT is one of the most efficient language representa-tion models introduced in last years. Its transformer-basedarchitecture permits to create state-of-art models for a widerange of tasks by easily fine-tuning the pre-trained BERT.Conceptually simple and powerful, BERT is the strong ba-sis for solving many tasks, especially the question answer-ing one included in the functioning of search engines. Fur-thermore, proving it as a Markov Random Field LanguageModel is useful to consider BERT as a high-quality, fluenttext generator too. In this work, first its structure is analyzedand its results as a text generator are compared to other ex-isting models. Finally the power of BERT is extended in thespecific field of question answering task.

# Basic Usage

The main script https://github.com/annalisad98/MLDL2021/blob/main/MLDLproject.ipynb is a workable replica from https://github.com/nyu-dl/bert-gen, where BERT as a text generator is compared with OpeanAi GPT, Transfomer-XL and XL-Net [link to Google!](http://google.com)
 



Wiki1000sample:
It is a script used to randomly select 1000 phrases from the WikiText-103 dataset. 
At first the titles conteinded in the dataset are discarded, leaving only the phrases. Then the 1000 

# Implemented Models and Original Papers
