# MLDL2021

BERT is one of the most efficient language representa-tion models introduced in last years. Its transformer-basedarchitecture permits to create state-of-art models for a widerange of tasks by easily fine-tuning the pre-trained BERT.Conceptually simple and powerful, BERT is the strong ba-sis for solving many tasks, especially the question answer-ing one included in the functioning of search engines. Fur-thermore, proving it as a Markov Random Field LanguageModel is useful to consider BERT as a high-quality, fluenttext generator too. In this work, first its structure is analyzedand its results as a text generator are compared to other ex-isting models. Finally the power of BERT is extended in thespecific field of question answering task.




Wiki1000sample:
It is a script used to randomly select 1000 phrases from the WikiText-103 dataset. 
At first the titles conteinded in the dataset are discarded, leaving only the phrases. Then the 1000 

# prova
