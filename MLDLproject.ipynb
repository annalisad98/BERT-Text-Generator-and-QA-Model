{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MLDLproject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annalisad98/BERT-Text-Generator-and-QA-Model/blob/main/MLDLproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFD1LrTBwJA9"
      },
      "source": [
        "**PROJECT: NEURAL TEXT GENERATOR**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5n2KJCrwSo5"
      },
      "source": [
        "bert-babble script (colab) + evaluation (github)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbWONWLXhl0g"
      },
      "source": [
        "# **INTRODUCTORY PART**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiAUSbdcwbCS"
      },
      "source": [
        "!pip3 install pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvYpfbC2x50g"
      },
      "source": [
        "With this command the PyTorch pretrained bert package is installed. It contains many classes related to the BERT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Luf5l9PIxIbU"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXjdMzc4KkVh"
      },
      "source": [
        "#help(BertForMaskedLM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDFlbwO6xgwT"
      },
      "source": [
        "BertTokenizer: perform end-to-end tokenization, i.e. basic tokenization followed by WordPiece tokenization.\n",
        "\n",
        "BertModel: raw BERT Transformer model (fully pre-trained).\n",
        "\n",
        "BertForMaskedLM: BERT Transformer with the pre-trained masked language modeling head on top (fully pre-trained)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TI8IpRkzegj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab2e8a9-a349-421d-a504-5ebc62893be0"
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model_version = 'bert-base-uncased'  #'bert-large-uncased'\n",
        "model = BertForMaskedLM.from_pretrained(model_version)\n",
        "model.eval()\n",
        "cuda = torch.cuda.is_available()\n",
        "if cuda:\n",
        "    model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1248501532/1248501532 [00:31<00:00, 40086531.21B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48Nw6BSnpoUl"
      },
      "source": [
        "type(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxQuv4Gigpz-"
      },
      "source": [
        "cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuOM_3cKzfLz"
      },
      "source": [
        "from_pretrained: let you instantiate a model/configuration/tokenizer from a pretrained version (with the above command the pre-trained model 'bert-base-uncased' is installed).\n",
        "\n",
        "The line model.eval() is used to set the model in evaluation mode to deactivate the DropOut modules. It is IMPORTANT to have reproducible results during evaluation.\n",
        "\n",
        "With the last 3 lines of code we move our tensor to the GPU if available. Remember that PyTorch exploits GPU's power which has an increased level of parallelism w.r.t CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlOIpHYKKgHH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3738c98e-a8e8-4b02-d9c2-06c2b841e279"
      },
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 16698738.25B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEZBx8S-TaG5",
        "outputId": "dc086fb3-f6ba-4295-fb30-368a3eabeb94"
      },
      "source": [
        "print(tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<pytorch_pretrained_bert.tokenization.BertTokenizer object at 0x7f3acf131d90>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPrqV0-XKhMy"
      },
      "source": [
        "tokenizer is used for the tokenization of sentences/batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrIu5TrCKzUo"
      },
      "source": [
        "def tokenize_batch(batch):\n",
        "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzJ9DDaNK0Oy"
      },
      "source": [
        "The method convert_tokens_to_ids converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the vocabulary.\n",
        "The function defined above gives the possibility to tokenize batches of strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5C2649hMgqo"
      },
      "source": [
        "def untokenize_batch(batch):\n",
        "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sOd77e6MkPD"
      },
      "source": [
        "The method convert_ids_to_tokens converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and added tokens.\n",
        "The function defined above gives the possibility to untokenize batches of strings.\n",
        "\n",
        "Ids stays for indeces and token stays for word objects (words, points, ...)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYC1n1Zsyuwe"
      },
      "source": [
        "def detokenize(sent):\n",
        "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
        "    new_sent = []\n",
        "    for i, tok in enumerate(sent):\n",
        "        if tok.startswith(\"##\"):\n",
        "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
        "        else:\n",
        "            new_sent.append(tok)\n",
        "    return new_sent\n",
        "\n",
        "CLS = '[CLS]'\n",
        "SEP = '[SEP]'\n",
        "MASK = '[MASK]'\n",
        "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
        "sep_id = tokenizer.convert_tokens_to_ids([SEP])[0]\n",
        "cls_id = tokenizer.convert_tokens_to_ids([CLS])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv9oC-2WOM93"
      },
      "source": [
        "new_sent is at the beginning an empty list. Using the for cycle it is populated.\n",
        "\n",
        "At each step of the for cycle a new word is added to the list new_sent. If some tokens start with \"##\", it means they are part of a bigger word (without spaces), so we concatenate them.\n",
        "\n",
        "Remember BERT learns by pretraining on 2 supervised tasks simultaneously: Masked Language Model and Next Sentence Prediction.\n",
        "\n",
        "The Masked LM task is implemented by masking 15% of the words randomly in every sentence and training the model to predict them. This is why we introduced above an index for the token [MASK].\n",
        "\n",
        "For the next sentence task the goal is to understand if a generic sentence is after another sentence and to do this we need to specify the beginning of the sample (with [CLS]) and we need a special separator token ([SEP]) for example to separate questions/answers.\n",
        "__________\n",
        "__________\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwKrTTJ4i_6K"
      },
      "source": [
        "___________\n",
        "___________\n",
        "# **GENERATION PART (BERT)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaE46qfcjlGv"
      },
      "source": [
        "## GENERATION: Functions for generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKwpHV-Ox4j0"
      },
      "source": [
        "It follows the generation part, with all its important connected functions. The main generate function is GENERATE.\n",
        "It permits to generate sentences by applying one possible modality out of 3:\n",
        "\n",
        "    - parallel_sequential_generation\n",
        "\n",
        "    - sequential_generation\n",
        "    \n",
        "    - parallel_generation\n",
        "\n",
        "There are then some minor functions:\n",
        "\n",
        "    - generate_step\n",
        "\n",
        "    - get_init_text\n",
        "\n",
        "    - printer (and with Github also read_sents and write_sents)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7pRTruLONrR"
      },
      "source": [
        "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=True):\n",
        "    \"\"\" Generate a word from out[gen_idx]\n",
        "    \n",
        "    args:\n",
        "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
        "        - gen_idx (int): location for which to generate for\n",
        "        - top_k (int): if >0, only sample from the top k most probable words\n",
        "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
        "    \"\"\"\n",
        "    logits = out[:, gen_idx] \n",
        "    # array of dimension batch_size e vocabulary_size.\n",
        "    # this is a multidim array (matrix)\n",
        "    if temperature is not None:\n",
        "        logits = logits / temperature\n",
        "        # temperature is used to squeeze the matrix logits of the tensor out.\n",
        "        # smoothing parameter for the next word distribution. \n",
        "        # Higher means more like uniform; lower means more peaky.\n",
        "        # Closer to 1 means a more uniform distribution.\n",
        "    if top_k > 0:# in this case we sample from the top k most probable words.\n",
        "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
        "        # returns the k biggest entries of the input.\n",
        "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
        "        # The distributions package contains parameterizable\n",
        "        # probability distributions and sampling functions.\n",
        "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
        "    elif sample:# in this case we sample from all the distribution.\n",
        "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
        "        # The distributions package contains parameterizable\n",
        "        # probability distributions and sampling functions.\n",
        "        idx = dist.sample().squeeze(-1)\n",
        "    else:\n",
        "        idx = torch.argmax(logits, dim=-1)\n",
        "    return idx.tolist() if return_list else idx\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCrNCkf-m9-C"
      },
      "source": [
        "The function generate_step is applied to generate a word.\n",
        "\n",
        "The function generate_step above returns a list of indeces if specified in the input parameter return_list, otherwise returns idx. These indeces define words (see Generate function part).\n",
        "\n",
        "First of all the object \"logits\" is created, then it squeezed in case of a prespecified value of temperature (in order to have a more or uniform distribution or less).\n",
        "\n",
        "Then there is an if-elif-else block that is used to sample a word from the distribution of logits. If we prespecify that we want to sample from the set of most probable words, then the distribution will be built over them and then sampling is applied (using .sample()). In case we want to consider the full distribution we specify just all the logits multiarray when building the distribution. Finally in case of neither full nor subset distribution, the default sampling refers to the argmax of logits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOsVsVQhm-7E"
      },
      "source": [
        "def get_init_text(seed_text, max_len, batch_size = 1, rand_init=False):\n",
        "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
        "    # builds a text by adding to seed_text a sequence (of length max_len) \n",
        "    # of either masks or random words.\n",
        "    # Recall that seed_text is used as a sort of pointer from which we start\n",
        "    # adding masked or random words to generate the initialized batch (in the\n",
        "    # BERT setting the best seed is [CLS], as we can see in the GENERATE\n",
        "    # function part).\n",
        "    # max_len = length of sequence to add to seed_text.\n",
        "    batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]\n",
        "    # we are applying this operation a number of time equal to the size of batch\n",
        "    # (batch_size is the size of the batch).\n",
        "    \n",
        "    # before giving an output, the tokenization is applied.\n",
        "    return tokenize_batch(batch)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4lVmzoMAAOX"
      },
      "source": [
        "The function get_init_text generates a tokenized text of length max_len (which will be used as initial text in the more general generate function) starting from a seed_text and completing it with masks or random words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XnQxD2AA4SO"
      },
      "source": [
        "def printer(sent, should_detokenize=True):\n",
        "    if should_detokenize:\n",
        "        sent = detokenize(sent)[1:-1]\n",
        "    print(\" \".join(sent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_veT-q4B4QR"
      },
      "source": [
        "The function printer prints a sentences given as input (if specified, it is first detokenized)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHcdXrBDIOvq"
      },
      "source": [
        "The following code block is extracted from https://github.com/nyu-dl/bert-gen/blob/master/bert-babble.ipynb (file bert-babble). \n",
        "\n",
        "(with respect to the Colab demo, two more \"print\" function are given: read_sents, write_sents)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1plhG72IGrk"
      },
      "source": [
        "# Utility functions\n",
        "    \n",
        "def read_sents(in_file, should_detokenize=False):\n",
        "  # reads content from the in_file.\n",
        "    sents = [sent.strip().split() for sent in open(in_file).readlines()]\n",
        "    if should_detokenize:\n",
        "        sents = [detokenize(sent) for sent in sents]\n",
        "    return sents\n",
        "\n",
        "def write_sents(out_file, sents, should_detokenize=False):\n",
        "  # writes inside the out_file.\n",
        "    with open(out_file, \"w\") as out_fh:         \n",
        "        for sent in sents:\n",
        "            sent = detokenize(sent[1:-1]) if should_detokenize else sent\n",
        "            out_fh.write(\"%s\\n\" % \" \".join(sent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1DdZO9GzOE5"
      },
      "source": [
        "The function read_sents is used to read sentences from an external file, named in_file. If we should dekotenize them, we enter the if construction. After reading them, they are returned.\n",
        "\n",
        "The function write_sents is used to write the (generated) sentences inside an external file, named out_file. If we should detokenize, the previously defined detokenize function is applied. There's no return here, the function just writes in the out_file.\n",
        "\n",
        "_____________\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCp5r0fSOOO6"
      },
      "source": [
        "For the following part of code observe this: this is the core of the algorithm. The general idea is\n",
        "\n",
        "      1- start from all masks\n",
        "      2- repeatedly pick a location, mask the token at that location, and generate from the probability distribution given by BERT\n",
        "      3- stop when converged or tired of waiting\n",
        "We consider three \"modes\" of generating:\n",
        "\n",
        "      . generate a single token for a position chosen uniformly at random for a chosen number of time steps(** PARALLEL SEQUENTIAL GENERATION**)\n",
        "      . generate in sequential order (Left->Right), one token at a time(**SEQUENTIAL GENERATION**)\n",
        "      . generate for all positions at once for a chosen number of time steps (**PARALLEL GENERATION**)\n",
        "The generate function wraps and batches these three generation modes. In practice, we find that the first leads to the most fluent samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd1aF8rw0i2E"
      },
      "source": [
        "# Generation modes as functions\n",
        "import math\n",
        "import time\n",
        "# the time package above is used to measure the time required for\n",
        "# generating an entire sentence.\n",
        "\n",
        "def parallel_sequential_generation(seed_text, batch_size=10, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
        "                                   cuda=False, print_every=10, verbose=True):\n",
        "    \"\"\" Generate for one random position at a timestep\n",
        "    \n",
        "    args:\n",
        "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
        "    \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    # These first 2 lines are the same both in parallell_sequential_generation,\n",
        "    # parallel_generation and sequential_generation.\n",
        "    \n",
        "    for ii in range(max_iter):\n",
        "        kk = np.random.randint(0, max_len)\n",
        "        #choose a random position from 0 to maximal length where a word will be added\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = mask_id\n",
        "            # think jj as an index that moves over the rows.\n",
        "            # mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
        "            # in every batch sentence change the word in position [seed_len+kk] into a mask(?)\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        # using the above line, the inp object is transformed into a tensor,\n",
        "        # using GPU if prespecified.\n",
        "        out = model(inp)  # the pretrained model BertForMaskedLM is applied (see first\n",
        "        # code block).\n",
        "        topk = top_k if (ii >= burnin) else 0\n",
        "        # top_k : at each step, sample from the top_k most likely words \n",
        "        # but only if the iteration we're in is >= the burn-in, else topk=0\n",
        "        idxs = generate_step(out, gen_idx=seed_len+kk, top_k=topk, temperature=temperature, sample=(ii < burnin))\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = idxs[jj]\n",
        "            # think jj as an index that moves over the rows.\n",
        "            # think seed_len+kk as an index that moves over the columns.\n",
        "            # in this sense idxs is a sort of column vector whose entries are\n",
        "            # specified using the indeces jj.\n",
        "            # jj indicates the row (the sentence) inside the batch.\n",
        "            # seed_len+kk stays for the token inside the specified sentence.\n",
        "            # Remember that batch is a list of vectors, where each vector\n",
        "            # has entries corresponding to indeces for words inside \n",
        "            # the tokenizer (vocabulary).\n",
        "            \n",
        "        if verbose and np.mod(ii+1, print_every) == 0:\n",
        "            # if verbose is true and ii % print_every = 0, so ii = α * print_every\n",
        "            # we print an output message.\n",
        "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
        "            # batch[0] corresponds to the first vector of the list. Through the\n",
        "            # application of the tokenizer, this is exactly a sentence, the first\n",
        "            # sentence of the batch we are working on.\n",
        "            # remember that convert_ids_to_tokens converts indeces (numeric values)\n",
        "            # to tokens (that may be words) through the tokenizer (which is \n",
        "            # substantially a vocabulary).\n",
        "\n",
        "            for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
        "            # idea: use the + as concatenation and show with the \"(*)\" where we\n",
        "            # have sampled the kk in this last external for (ii) cycle.\n",
        "            # In this way, we know that the token we see before the \"(*)\" is the\n",
        "            # one we have just updated.\n",
        "\n",
        "            print(\"iter\", ii+1, \" \".join(for_print))     \n",
        "            # we could think this if and print as a command to show the user\n",
        "            # that the process is going on, the iterations are moving, and \n",
        "            # we show the first sentence of the batch to illustrate\n",
        "            # how the generated sentence is changing, how the process is\n",
        "            # modifying iteratively our first sentence of the batch.        \n",
        "    return untokenize_batch(batch)\n",
        "\n",
        "def parallel_generation(seed_text, batch_size=10, max_len=15, top_k=0, temperature=None, max_iter=300, sample=True, \n",
        "                        cuda=False, print_every=10, verbose=True):\n",
        "    \"\"\" Generate for all positions at a time step \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    # These first 2 lines are the same both in parallel_sequential_generation,\n",
        "    # parallel_generation and sequential_generation. \n",
        "    \n",
        "    for ii in range(max_iter):\n",
        "        # w.r.t the sequential_generation function, since now we generate all\n",
        "        # words at a time, we don't need the command inp = [sent[:seed_len+ii+leed_out_len]+[sep_id] for sent in batch]\n",
        "        \n",
        "        # so while in sequential_generation the generation process goes from\n",
        "        # the beginning of the batch up to the end, now there is\n",
        "        # a for loop with a max_iter number of iterations.\n",
        "        # For each iteration the process below is applied (NOTE that the\n",
        "        # value ii is not used in the generation process below. The only\n",
        "        # moment we use it is when we want to print a message to show the \n",
        "        # iteration the algorithm is workin on).\n",
        "\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        # using the above line, the inp object is transformed into a tensor,\n",
        "        # using GPU if prespecified.\n",
        "        out = model(inp) # the pretrained model BertForMaskedLM is applied (see first\n",
        "        # code block).\n",
        "        for kk in range(max_len):\n",
        "            idxs = generate_step(out, gen_idx=seed_len+kk, top_k=top_k, temperature=temperature, sample=sample)\n",
        "            for jj in range(batch_size):\n",
        "                batch[jj][seed_len+kk] = idxs[jj]\n",
        "                # think jj as an index that moves over the rows.\n",
        "                # think seed_len+kk as an index that moves over the columns.\n",
        "                # in this sense idxs is a sort of column vector whose entries are\n",
        "                # specified using the indeces jj.\n",
        "                # jj indicates the row (the sentence) inside the batch.\n",
        "                # seed_len+kk stays for the token inside the specified sentence.\n",
        "                # Remember that batch is a list of vectors, where each vector\n",
        "                # has entries corresponding to indeces for words inside \n",
        "                # the tokenizer (vocabulary).\n",
        "            \n",
        "        if verbose and np.mod(ii, print_every) == 0:\n",
        "            # if verbose is true and ii % print_every = 0, so ii = α * print_every\n",
        "            # we print an output message.\n",
        "            print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(batch[0])))\n",
        "            # batch[0] corresponds to the first vector of the list. Through the\n",
        "            # application of the tokenizer, this is exactly a sentence, the first\n",
        "            # sentence of the batch we are working on.\n",
        "\n",
        "            # we could think this if and print as a command to show the user\n",
        "            # that the process is going on, the iterations are moving, and \n",
        "            # we show the first sentence of the batch to illustrate\n",
        "            # how the generated sentence is changing, how the process is\n",
        "            # modifying iteratively our first sentence of the batch.\n",
        "\n",
        "            # remember that convert_ids_to_tokens converts indeces (numeric values)\n",
        "            # to tokens (that may be words) through the tokenizer (which is \n",
        "            # substantially a vocabulary).\n",
        "    \n",
        "    return untokenize_batch(batch)\n",
        "            \n",
        "def sequential_generation(seed_text, batch_size=10, max_len=15, leed_out_len=15, \n",
        "                          top_k=0, temperature=None, sample=True, cuda=False):\n",
        "    \"\"\" Generate one word at a time, in L->R order \"\"\"# from left to right.\n",
        "    # This function is called inside the GENERATION function (which is the main\n",
        "    # generation function to call), to generate a batch of words.\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "\n",
        "    # Recall that with get_init_text we build a text by adding to seed_text a \n",
        "    # sequence (of length max_len) of either masks or random words.\n",
        "    # with get_init_text we initialize the batch, then through this function\n",
        "    # it is updated.\n",
        "    # max_len = length of sequence to add to seed_text (we can consider roughly\n",
        "    # as the maximal length of each sentence composing the text).\n",
        "    # Recall seed_text is the prefix to generate for (it was found crucial to \n",
        "    # start with the CLS token). It is somehow the prefix to add when generating\n",
        "    # sentences, it stays for the beginning of a sentence.\n",
        "    # batch_size is the size of the batch.\n",
        "\n",
        "    # so the object batch will contain our text (a structure containing sentences).\n",
        "    \n",
        "    for ii in range(max_len):\n",
        "        inp = [sent[:seed_len+ii+leed_out_len]+[sep_id] for sent in batch]\n",
        "        # the above command says that for each sentences present in the batch\n",
        "        # we save a growing portion of the sentence inside the object inp\n",
        "        # (we say growing portion because the outer for cycle cycles over ii, that\n",
        "        # is used as an index for sent).\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        # using the above line, the inp object is transformed into a tensor,\n",
        "        # using GPU if prespecified.\n",
        "        out = model(inp) # the pretrained model BertForMaskedLM is applied (see first\n",
        "        # code block).\n",
        "        idxs = generate_step(out, gen_idx=seed_len+ii, top_k=top_k, temperature=temperature, sample=sample)\n",
        "        # recall from some previous code blocks that the GENERATE_STEP IS APPLIED\n",
        "        # TO GENERATE A WORD (in this case idxs is a column vector).\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+ii] = idxs[jj]\n",
        "            # think jj as an index that moves over the rows.\n",
        "            # think seed_len+ii as an index that moves over the columns.\n",
        "            # in this sense idxs is a sort of column vector whose entries are\n",
        "            # specified using the indeces jj.\n",
        "            # jj indicates the row (the sentence) inside the batch.\n",
        "            # seed_len+kk stays for the token inside the specified sentence.\n",
        "            # Remember that batch is a list of vectors, where each vector\n",
        "            # has entries corresponding to indeces for words inside \n",
        "            # the tokenizer (vocabulary).\n",
        "    return untokenize_batch(batch)\n",
        "\n",
        "\n",
        "def generate(n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
        "             generation_mode=\"parallel-sequential\",\n",
        "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
        "             cuda=False, print_every=1):\n",
        "    # main generation function to call\n",
        "\n",
        "    # n_samples = number of samples.\n",
        "    # math.ceil is used to round a number upward to its nearest integer, in\n",
        "    # this case it is applied to define the number of batches, n_batches.\n",
        "    # sentences is the list of generated words.\n",
        "    # print_every is just used to specify after how many batches generations to\n",
        "    # output the time required.\n",
        "    # seed_text stays for the first token of every sequence (in this\n",
        "    # case it is the [CLS] token)\n",
        "    sentences = []\n",
        "    n_batches = math.ceil(n_samples / batch_size)\n",
        "    start_time = time.time()\n",
        "    # for each batch, depending on the generation_mode, a specific generation\n",
        "    # function is applied (parallel_sequential_generation, or sequential_generation, \n",
        "    # or parallel_generation).\n",
        "    # the final \"if\" checks \n",
        "    # at the end of each for iteration the generated batch is added to sentences.\n",
        "    for batch_n in range(n_batches):\n",
        "        # ma\n",
        "        if generation_mode == \"parallel-sequential\":\n",
        "            batch = parallel_sequential_generation(seed_text, batch_size=batch_size, max_len=max_len, top_k=top_k,\n",
        "                                                   temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
        "                                                   cuda=cuda, verbose=False)\n",
        "        elif generation_mode == \"sequential\":\n",
        "            batch = sequential_generation(seed_text, batch_size=batch_size, max_len=max_len, top_k=top_k, \n",
        "                                          temperature=temperature, leed_out_len=leed_out_len, sample=sample,\n",
        "                                          cuda=cuda)\n",
        "        elif generation_mode == \"parallel\":\n",
        "            batch = parallel_generation(seed_text, batch_size=batch_size,\n",
        "                                        max_len=max_len, top_k=top_k, temperature=temperature, \n",
        "                                        sample=sample, max_iter=max_iter, \n",
        "                                        cuda=cuda, verbose=False)\n",
        "        \n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "          # if a number of batches equal to \"print_every\" has been\n",
        "          # generated, then an output message is shown giving the time required.\n",
        "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "        \n",
        "        sentences += batch\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "859dnAFa4LKN"
      },
      "source": [
        "The piece of code above contains some smaller generation functions (parallel_sequential_generation, or sequential_generation, or parallel_generation) and one bigger, more general, for generating sentences.\n",
        "\n",
        "For each batch one of the 3 above generation methods is chosen.\n",
        "\n",
        "\n",
        "Sequential_generation. We use this function to generate a batch of sentences (that will be added to the final set of sentences). A sort of initialized text is generated and it is used for the process of generation: consider a generic sentence of the initialized batch, a growing portion of it is taken (inp) and given as input to the pretrained model BertForMaskedLM, the output (out) is used as a parameter to the generate_step function to generate a word. Intuitively we use a growing portion of the initialized batch to write sentences with some sense.\n",
        "\n",
        "Parallel_generation. While in sequential_generation the generation process goes from the beginning of the batch up to the end, now there is a for loop with a max_iter number of iterations.\n",
        "For each iteration the process, similar to the one for the sequential_generation, is applied (NOTE that the value ii is not used in the generation process below. The only moment we use it is when we want to print a message to show the iteration the algorithm is workin on).\n",
        "\n",
        "Parallel_sequential_generation. This function generates a single token for a position chosen uniformly at random for a chosen number of time steps. As in sequential_generation, we fill in the batch one idx (token) at a time but the position of this idx in the batch's vectors (sentences) is sampled uniformly at random instead of using the iteration number as in sequential_generation. Here we give the full batch as input (inp) to the pretrained model BertForMaskedLM.\n",
        "At each step, the model (generate_step function) samples the words from the top_k most likely words, but if the iteration we're in is <= the burn-in, it doesn't do that anymore. \n",
        "\n",
        "_____\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBprgGGA0j3_"
      },
      "source": [
        "## APPLICATION OF THE GENERATION FUNCTION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3q7Pd0uijZq"
      },
      "source": [
        "### Application Example "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GM_g7avojS8"
      },
      "source": [
        "In the following we consider an extracted code from https://github.com/nyu-dl/bert-gen/blob/master/bert-babble.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2A_ipOfKdqA"
      },
      "source": [
        "n_samples = 1000 \n",
        "batch_size = 50 \n",
        "max_len = 40\n",
        "top_k = 100\n",
        "temperature = 0.7\n",
        "\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "\n",
        "# Choose the prefix context\n",
        "seed_text = \"[CLS]\".split()\n",
        "\n",
        "for temp in [1.0]:\n",
        "    bert_sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
        "                          sample=sample, top_k=top_k, temperature=temp, burnin=burnin, max_iter=max_iter,\n",
        "                          cuda=True)\n",
        "    out_file = \"Bert_using_pytorch.txt\"\n",
        "    write_sents(out_file, bert_sents, should_detokenize=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgUFStfvjMYB"
      },
      "source": [
        "write_sents(out_file, bert_sents, should_detokenize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KitCB-nrLlmo"
      },
      "source": [
        "in_file = \"Bert_using_pytorch.txt\"\n",
        "bert_sents = read_sents(in_file, should_detokenize=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDPTMyMwMBtU",
        "outputId": "b7f2ac7f-b70b-4012-af6f-b6d993f5e08e"
      },
      "source": [
        "for i in range(50):\n",
        "    printer(bert_sents[i], should_detokenize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it is true that the world is not just governed by one force , but by some other force , then you will select where the events begin and when they end , that is what you say\n",
            "in britain , temp is fondly remembered for this , because just a month later several of his men had been gang - raped , while the two men living close to the camp were tortured\n",
            "they were real ugly . ) . . our beckoned . bambi , clearly ugly as well , had been believed to be a wanker for this character because of her matted dark hair\n",
            "king , professional boxer . stephen jones , jon jones , john jones ( the waste had been wiped clean , cobbled up into garbage ) . brian jones , john jones ( man in black )\n",
            "hurry \" - said one of the women . \" alright , alright , alright \" - nobody could hurt her . bearl could not . the dead woman and lissy were at least twenty\n",
            "by ( author and illustrator ) e . mccready , univ . and philosophical research institute , n . y . c . [ revised and enlarged ] philadelphia : w . a . t\n",
            "see list below ) samuel richard smith : the road was named after him at credit . ansel a mitchell : the seat is currently represented mp by hon . kathleen mitchell ( an federal deputy )\n",
            "er ! ' answered the secretary . ' did you see her ? ' ' she had just been here ten minutes . ' while james thought it was maiwenn , he almost recognized her instantly\n",
            "glen as david duncan luca amoretti as professor douglas duncan lynn redgrave as ellie duncan bois - du - bois as jim myre lorna keefe as genevieve joubert | | |\n",
            "cast included anthony in \" the fabulous reynolds brothers \" and the other black guy in \" the ghola \" , to write for new york studios and role corrina in \" what ?\n",
            "the car was not exactly cold either . \" \" so were we . \" a dissociative murmur . \" this is ms . collins . \" 3 the car smelled of sizzling rubber\n",
            "... ] - as planned , the shop goes . - inside the shop - guy and people climb over the crib to buy more food . - outside the shop guri stays in the flat\n",
            "columbia river course . \" \" we . . . . removed the rafting course , and we . . . . . . widened it so that the source of moon lake was located . .\n",
            ", and dance , children , and dance ! she was dressed like a young woman in fashionable clothes and beautifully lit , her arms clasped , legs crossed under bonnier - wood shoes and closed lips\n",
            "- present . burbach and his wife melinda philco continue touring in central , eastern and northern california , particularly wachikoma and across the silverton valley and muskokite\n",
            "word concept however has a certain subliterature both in french and in french or arabic / persian . al - qadri , sabait ( ed . ) , ( 2010 )\n",
            "2007 ) 2054 produced a ( \" talking \" ) book , the atomic bombs : leaders of the world . ( 2007 ) 2054 published in at least a dozen countries from west to east\n",
            "from 23 february early on between grandstands started flashing ' the double lever ' . the ' triple lever ' was then called a lever which many teams had used to call their upfield shots\n",
            "thing that has grown more somehow , as a whole ... these are not the things we read . these are not ordinary things created by birth or child , but a thing that grows as it grows\n",
            "had all road track races professionally in asia pacific ( daisad ( now defunct ) ) . in may 2006 , he took out the zkm - adac - the spanish professional mountain championship\n",
            "* * 42 . the violets . the dark . the darkness . only us . only us . only me and her . * * * 48 . the dream . another time ... another nightmare\n",
            "letter told he had two sons , his weight more but not less , and toyed with it - which i am sure he never knew , as the two scraps were ripped from his hands\n",
            "drives an electrical / industrial truck . in 2013 , a solo exhibition of hers was at design ( sculpture ) , an art space in the szczerzprzystalskie art district\n",
            "each visited seeley robertson , who , himself , was \" in booneville \" , and sam gilman , commander of the meo and a kia . robertson later was \" in lexington \"\n",
            "the \" starling \" , an incredibly happy era comes for survivors . they look for their lost grandfather , who was almost killed by superman . it is also the final film of the film trilogy\n",
            "no thanks . \" he hit a body and mitch ' s foot nearly hit the floor . he turned back to where mitch was standing . walk away from me . walk away from that bitch again\n",
            "during its progress , with the help of his wife , harriet rowley - bennett , john g . macintyre occupied government buildings at fifty - third & third streets in nashville ( 1871 )\n",
            "include \" enough time to go \" to enter bbc radio 2 , \" a young married man \" ( four minor uk singles along with rick leach and robert skinner ) \" the first good look \"\n",
            "founder and first president was john reid ( chairman ) . staff in 1895 included reid ( later appointed as director ) , b . white ( secretary ) , and george frederick thorp ( treasurer )\n",
            "activities ( instruments ) might also have included large horses , furniture , horses and boats ( seized by the irish volunteers in 1649 ) ; notes about chemical substances ( water , earth , wine ) obtained\n",
            "takes jimmy out for a walk to the local fishing lake , watching him lose control of his rackets . later that night most of the guys that took it all arrive and get back them up\n",
            "down inside , he had been cursed . he was the keeper of dead hearts ; between life and death . acheron stood up as he remembered that last message . it was 2 : 34 : 57\n",
            "- fu tsui reunited in may / june 2010 with justin liming , ang chi - li and jimmy yang . the live album featured singer - songwriter justin liming produced by james howarth\n",
            "mr . robert ross , these are your co - authors whose books are the best ever published ( and have a professional writing staff that can tell you how many more than books are ever published )\n",
            "the areas of popular interest are nash city , home to the nash hospital and nash town , a rallying point for new - age rock art , promoted as the first concert in the nash square\n",
            "this little bird named brock , who would have jumped at him just now . thank god , he would have lived . and maybe like brock , jack had seen it . both himself and will had\n",
            "won the region of origin of australia award for the 2006 - 2007 editions , and the prestigious \" the english - australian \" award in 2014 ( ( adelaide : 171 , melbourne : 105 ) )\n",
            "the right wall were cameras . they were full of glitter , hot gobs , purple stickers , pet lights , even as much as possible to film antique cars in the nearest drive - by\n",
            "is no formal management structure such as master management nor is any of these facilities relevant to the national security advisor general per directive 46 / 14 while approvals listed here are dse or dpp\n",
            "seed and night : twenty five minutes , before dimming lights ... and before respiration ... or worse : forty five minutes , allowing the bleeding to stop . blood dripped from nine tiny red veins\n",
            "took the first step down the hall and came up to the front landing , for some fresh air . the house was exceptionally quiet , with closed front doors , for no more than about five stories\n",
            "i met you : the story of the man i never met is a special 4 issue limited series published by hyperion books that came out in 1997 ( starting with shadows of tomorrow # 14 )\n",
            "want me to change the subject ? \" silence , silence was enough . \" what the fuck ? \" logan said . hours later and hours later was the sudden arrival of my best friend , logan\n",
            "was neither played , in principle , by the sidelight of summer , nor even at the end of autumn , and it did not bring him success , until it was taken over by herbert read\n",
            "st john , michael st . john and joseph cockey were among the remigians and were the founding monks while still in monastic life . thomas digby later became justice of the peace\n",
            ", known simply as louise , wrote the screenplay for the broadway musicals lili , ruby , gypsy , blow - out and she wrote , the red carpet , the agassiz girl and boo\n",
            "day since getting a quick divorce or becoming a rich wife , all twenty or so homes had lofts being air - conditioned kinds of lofts made for the construction and the real estate business here\n",
            "yourself drawing me toward you , sparing my hand , and under my luceria . give me opportunities as thou hast before . give me something , well , that will never be dreamed of\n",
            ". rush takes the child in to the small room where there is no but says , ' please bid farewell to my family and to arachne , pandora . you truly must be very frightened\n",
            "stake - a - boo also had wheels with the player ' s main tool ( mine was up and not down ) . the wagon , which had twenty - six wooden walking wheels , was similar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqqgDCvopM7S"
      },
      "source": [
        "_______________\n",
        "_______________\n",
        "# **GENERATION PART (OpenAI GPT)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UUfZ-ZYNjNw"
      },
      "source": [
        "**Comparing to existing models**\n",
        "\n",
        "The OpenAI Generative Pretraining Transformer is another pretrained model successfully used for transfer learning. Since the model is a unidirectional language model, we can straightforwardly generate from the model. See this repo by Thomas Wolf at Huggingface for instructions for setting up the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdjHFwwrAIEy"
      },
      "source": [
        "!git clone https://github.com/huggingface/pytorch-openai-transformer-lm.git 'OpenAi'\n",
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjE82mwdFuWX"
      },
      "source": [
        "!git clone https://github.com/openai/finetune-transformer-lm.git\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbDUIrVNl3h1"
      },
      "source": [
        "In order to run the next cells, we need to move the folder \"Model\" from \"finetune-transformer-lm\" to \"OpenAi\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxPqC2FaBtbY"
      },
      "source": [
        "!pip install ftfy\n",
        "!pip install tqdm\n",
        "!pip install sklearn\n",
        "!pip install spacy\n",
        "!pip install pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKY0cWFWN0Nd"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.insert(1, os.path.join(\".\", \"OpenAi\"))  #pytorch-openai-transformer-lm\n",
        "\n",
        "from OpenAi.model_pytorch import LMModel, load_openai_pretrained_model, DEFAULT_CONFIG\n",
        "from OpenAi.text_utils import TextEncoder\n",
        "\n",
        "def load_openai_gpt(n_special=1, n_ctx=512):\n",
        "    text_encoder = TextEncoder(\"/content/OpenAi/model/encoder_bpe_40000.json\", \n",
        "                               \"/content/OpenAi/model/vocab_40000.bpe\")\n",
        "    encoder = text_encoder.encoder\n",
        "    n_vocab = len(text_encoder.encoder)\n",
        "    vocab = n_vocab + n_special + n_ctx\n",
        "\n",
        "    args = DEFAULT_CONFIG\n",
        "    lm_model = LMModel(args, vocab, n_ctx, return_probs=True)\n",
        "    load_openai_pretrained_model(lm_model.transformer, n_ctx=n_ctx, n_special=n_special,\n",
        "                                 path=\"/content/OpenAi/model/\",\n",
        "                                 path_names=\"/content/OpenAi/\")\n",
        "    #lm_model.to(device)\n",
        "    lm_model.return_probs = False\n",
        "    lm_model.eval()\n",
        "    return lm_model, text_encoder\n",
        "\n",
        "def make_batch(X, n_vocab, n_special, batch_size):\n",
        "    X = np.array(X)\n",
        "    assert X.ndim in [1, 2]\n",
        "    if X.ndim == 1:\n",
        "        X = np.expand_dims(X, axis=0)\n",
        "    pos_enc = np.arange(n_vocab + n_special, n_vocab + n_special + X.shape[-1])\n",
        "    pos_enc = np.tile(pos_enc, (batch_size, pos_enc.shape[-1])) #np.expand_dims(pos_enc, axis=0)\n",
        "    batch = np.stack([X, pos_enc], axis=-1)\n",
        "    batch = torch.tensor(batch, dtype=torch.long)#.to(device)\n",
        "    return batch\n",
        "\n",
        "def append_batch(X, next_idx):\n",
        "    next_pos = X[:, -1:, 1] + 1\n",
        "    next_x = torch.cat((next_idx, next_pos), -1).unsqueeze(1)\n",
        "    return torch.cat((X, next_x), 1)\n",
        "\n",
        "def _generate_sentence_openai(model, text_encoder, seed_text, batch_size=10, gen_len=20, \n",
        "                             topk=100, sample=True, n_special=0):\n",
        "    n_vocab = len(text_encoder.encoder)\n",
        "    #X = np.random.randint(n_vocab, size=(batch_size, 1)).tolist()\n",
        "    #sents = [[text_encoder.decoder[X[i][0]]].replace('</w>', '') for i in range(batch_size)]\n",
        "    X = [[n_vocab - 1] for _ in range(batch_size)]\n",
        "    sents = [[] for _ in range(batch_size)]\n",
        "    if seed_text:\n",
        "        seed_ids = text_encoder.encode([seed_text,])\n",
        "        X = [X[i] + seed_ids[0] for i in range(batch_size)]\n",
        "        sents = [[seed_text] for _ in range(batch_size)]\n",
        "    XMB = make_batch(X, n_vocab, n_special, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    for step_n in range(gen_len):\n",
        "        out = model(XMB) + model.pos_emb_mask\n",
        "        next_idxs = generate_step(out, gen_idx=step_n, top_k=topk, sample=sample, return_list=False)\n",
        "        idxs = next_idxs.tolist()\n",
        "        for i in range(batch_size):\n",
        "            next_token = idxs[i]\n",
        "            if next_token == n_vocab:\n",
        "                next_token = \"<EOS>\"\n",
        "            else:\n",
        "                next_token = text_encoder.decoder[next_token].replace('</w>', '')\n",
        "            sents[i].append(next_token)\n",
        "        XMB = append_batch(XMB, next_idxs.unsqueeze(-1))\n",
        "        \n",
        "    return [[tok for tok in sent if tok != '\\n'] for sent in sents]\n",
        "\n",
        "def generate_openai(model, text_encoder, n_samples, seed_text, \n",
        "                    batch_size=10, gen_len=20, \n",
        "                    topk=100, temperature=0.7, sample=True,\n",
        "                    n_special=0, print_every=1):\n",
        "    sents = []\n",
        "    start_time = time.time()\n",
        "    n_batches = math.ceil(n_samples / batch_size)\n",
        "    for batch_n in range(n_batches):\n",
        "        batch_sents = _generate_sentence_openai(model, text_encoder, seed_text,\n",
        "                                                batch_size=batch_size, gen_len=gen_len, \n",
        "                                                topk=topk, sample=sample,\n",
        "                                                n_special=n_special)\n",
        "        sents += batch_sents\n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "            print(\"Generated batch %d of %d in %.3fs\" % (batch_n + 1, n_batches, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "    return sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcqQicwHN8Mi",
        "outputId": "07ec7253-d998-4687-b674-1ee38cdc9896"
      },
      "source": [
        "import json\n",
        "\n",
        "gpt_model, gpt_text_encoder = load_openai_gpt(n_special=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading weights...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scwtp-ueN9BH"
      },
      "source": [
        "Loading weights..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrgceSdL9Mck"
      },
      "source": [
        "### Application of OpenAI GPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYlUdyecN__R",
        "outputId": "3a881334-5c85-44fa-bf12-892071061675"
      },
      "source": [
        "n_samples = 1000\n",
        "batch_size = 50\n",
        "max_len = 40\n",
        "top_k = 100\n",
        "temperature = 0.7\n",
        "\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "\n",
        "openai_sents = generate_openai(gpt_model, gpt_text_encoder, seed_text=\"\", \n",
        "                               n_samples=n_samples, batch_size=batch_size, gen_len=max_len,\n",
        "                               topk=top_k, temperature=temperature, sample=sample,\n",
        "                               n_special=1, print_every=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated batch 1 of 20 in 147.889s\n",
            "Generated batch 2 of 20 in 144.145s\n",
            "Generated batch 3 of 20 in 143.557s\n",
            "Generated batch 4 of 20 in 143.098s\n",
            "Generated batch 5 of 20 in 142.696s\n",
            "Generated batch 6 of 20 in 143.003s\n",
            "Generated batch 7 of 20 in 143.652s\n",
            "Generated batch 8 of 20 in 143.495s\n",
            "Generated batch 9 of 20 in 143.918s\n",
            "Generated batch 10 of 20 in 143.643s\n",
            "Generated batch 11 of 20 in 143.896s\n",
            "Generated batch 12 of 20 in 142.684s\n",
            "Generated batch 13 of 20 in 144.480s\n",
            "Generated batch 14 of 20 in 142.908s\n",
            "Generated batch 15 of 20 in 142.632s\n",
            "Generated batch 16 of 20 in 142.571s\n",
            "Generated batch 17 of 20 in 142.476s\n",
            "Generated batch 18 of 20 in 143.346s\n",
            "Generated batch 19 of 20 in 144.636s\n",
            "Generated batch 20 of 20 in 143.657s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ye0xLVFVNK7"
      },
      "source": [
        "out_file = \"openaitext.txt\"\n",
        "    #out_file = \"data/%s-len%d-burnin%d-topk%d-temp%.3f.txt\" % (model_version, max_len, burnin, top_k, temp)\n",
        "write_sents(out_file, openai_sents, should_detokenize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5G7x3Cr2PqR"
      },
      "source": [
        "In order to get more values related to the table shown in the paper \"BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model\", we sample 1000 sentences from the training split of the datasets WT103, and check the values of corpus bleu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_6hOpThkt5q"
      },
      "source": [
        "___________\n",
        "___________\n",
        "# **EVALUATION PART**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXJuO4AU0xA2"
      },
      "source": [
        "Evaluation methods for unconditional generation aren't perfect. We'll measure the diversity of our generated samples via _self-BLEU_: we compute _corpus BLEU_ where for each generated sentence, we compute BLEU treating the other sentences as references. \n",
        "\n",
        "We also compute the percentage of  _n-grams_ that are unique among the generations. \n",
        "\n",
        "(From Wikipedia: an n-gram is an n-elements subsequence of a sequence).\n",
        "\n",
        "We try some other strategies, including comparing to outside models, in our report, and you can see some of the code for that here (SEE SECTION TEXYGEN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0zIi0rXPXHC"
      },
      "source": [
        "The following part is extracted from link https://github.com/nyu-dl/bert-gen/blob/master/bert-babble.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oi3xfMxMzbs"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l149KV2RnF24"
      },
      "source": [
        "!pip3 install nltk==3.6.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QgQ0jADM-tu"
      },
      "source": [
        "from nltk.translate import bleu_score as bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56LjqElSM_o-"
      },
      "source": [
        "## Quality Measures: Corpus-BLEU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuHKGTyo-JFv"
      },
      "source": [
        "We want to know how similar are the generated sentences to the original training data (Toronto Book Corpus and Wikipedia dumps). We follow Yu et al., (2017) and compute the BLEU between the generations and the test sets of both corpora by treating the test set as the references for each generation. The tests sets are large; we subsample 5000 examples from each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTfFV1F5inUm"
      },
      "source": [
        "#help(bleu.corpus_bleu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZx_5by5NWcf"
      },
      "source": [
        "def prepare_data(data_file, replacements={}, uncased=True):\n",
        "    \"\"\" Prepare data to compute the BLEU score, since we use corpus_bleu each \n",
        "        sentence has to be a list of a list of tokens\n",
        "    \"\"\"\n",
        "    data = [d.strip().split() for d in open(data_file, 'r').readlines()]\n",
        "    # strip() to remove spaces, split ['splits', 'a', 'string', 'into', 'a', 'list']\n",
        "    # done for each line in the data_file\n",
        "    if uncased:\n",
        "        data = [[t.lower() for t in sent] for sent in data]\n",
        "        # lower case for every word\n",
        "        \n",
        "    for k, v in replacements.items():\n",
        "        # example from \"prepare_wiki\": replace \"@@unknown@@\"(k) with  \"[UNK]\"(v)\n",
        "        data = [[t if t != k else v for t in sent] for sent in data]\n",
        "        # if the token t is different from k (what we have to change), then leave t\n",
        "        # otherwise, if t is = k, replace t with v \n",
        "        \n",
        "        # recall: replacements is a dictionary that connects tokens to be substituted\n",
        "        # and token that substitute, e.g. \"@@unknown@@\" with \"[UNK]\"\n",
        " \n",
        "        # at the end the data are ready to be used in corpus_bleu\n",
        "    return data\n",
        "\n",
        "def prepare_wiki(data_file, uncased=True):\n",
        "    \"\"\" prepare the data from wiki103 so we can use these phrases as \n",
        "    references in the corpus bleu function \"\"\"\n",
        "    replacements = {\"@@unknown@@\": \"[UNK]\"}\n",
        "    return prepare_data(data_file, replacements=replacements, uncased=uncased)\n",
        "\n",
        "def prepare_tbc(data_file):     \n",
        "    \"\"\" prepare the data from tbc so we can use these phrases as \n",
        "    references in the corpus bleu function \"\"\"   \n",
        "    replacements = {\"``\": \"\\\"\", \"\\'\\'\": \"\\\"\"}\n",
        "    return prepare_data(data_file, replacements=replacements)\n",
        "\n",
        "def corpus_bleu(generated, references):\n",
        "    \"\"\" Compute similarity between two corpora as measured by\n",
        "    comparing each sentence of `generated` against all sentences in `references` \n",
        "    \n",
        "    args:\n",
        "        - generated (List[List[str]]): list of sentences (split into tokens)\n",
        "        - references (List[List[str]]): list of sentences (split into tokens)\n",
        "        \n",
        "    returns:\n",
        "        - bleu (float)\n",
        "    \"\"\"    \n",
        "    # generated is a list of sentences, where each sentence is represented as a list\n",
        "    # of tokens.\n",
        "    # references have the same basis structure of generated.\n",
        "    return bleu.corpus_bleu([references for _ in range(len(generated))], generated)\n",
        "    # compare each sentence of 'generated' against all sentences in 'references'\n",
        "    # corpus_bleu -> ([['reference'], ['reference'],...(|generated| times)], ['generated'])\n",
        "    # corpus_bleu analyzes each sentences of the list \"generated\" with all the others\n",
        "    # of \"references\", and then averages (not a simple averaging)\n",
        "    # while when we analyze self_bleu we are comparing against the other generated\n",
        "    # sentences, not with some reference sentences!\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X8xDX-9qYf1"
      },
      "source": [
        "Function _prepare_data_ is used to prepare the data when computing the BLEU score. In particular, each sentence of the data_file is transformed into a list of lists of tokens.\n",
        "Then functions _prepare_wiki_ and _prepare_tbc_ are applied to prepare the training data (of respectively Wikipedia dumps and Toronto Book Corpus) to study the similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UADQGhlFNavl"
      },
      "source": [
        "wiki103_file = 'datawiki103.5k.txt'\n",
        "#this comes from wikitext103 test set\n",
        "tbc_file = 'tbc.5k.txt'\n",
        "\n",
        "wiki_data = prepare_wiki(wiki103_file)\n",
        "tbc_data = prepare_tbc(tbc_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6LCzPp0NWGP",
        "outputId": "f859d118-ca14-42ad-bfcf-3fe0b31f6fd1"
      },
      "source": [
        "# Some initializations for the table of corpus-BLEU\n",
        "print(model_version)# THIS WHOLE CODE BLOCK HAS TO BE REPEATED FOR\n",
        "# THE OTHER BERT MODEL VERSION TOO\n",
        "\n",
        "TITLE_CORPUS = ['Model', 'Corpus-BLEU against WT103', 'Corpus-BLEU against TBC']\n",
        "# values_corpus_bleu is a list of 3 lists (one with the model name, two for corpus-BLEU)\n",
        "# each one with 4 elements (first element refers to BERTlarge, second element is for\n",
        "# BERTbase, third element for GPT, fourth element for WT103)\n",
        "# initialization:\n",
        "values_corpus_bleu = [[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
        "values_corpus_bleu[0] = ['BERTlarge', 'BERTbase', 'GPT', 'WT103']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert-base-uncased\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulkXvxDCPCyk"
      },
      "source": [
        "The following code block has to be repeated two times, one for 'bert-base-uncased', one for 'bert-large-uncased'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syLYzH0YNdy3",
        "outputId": "0199c051-43cf-4d27-b596-0bb5fa4dae0a"
      },
      "source": [
        "value = corpus_bleu(bert_sents, tbc_data)\n",
        "print(\"BERT-TBC BLEU: %.2f\" % (100 * value))\n",
        "if model_version == 'bert-base-uncased':\n",
        "  # paper value: 7.06\n",
        "  values_corpus_bleu[2][1] = 100 * value\n",
        "else: #'bert-large-uncased'\n",
        "  # paper value: 7.60\n",
        "  values_corpus_bleu[2][0] = 100 * value\n",
        "\n",
        "value = corpus_bleu(bert_sents, wiki_data)\n",
        "print(\"BERT-Wiki103 BLEU: %.2f\" % (100 * value ))\n",
        "if model_version == 'bert-base-uncased':\n",
        "  # paper value: 7.80\n",
        "  values_corpus_bleu[1][1] = 100 * value\n",
        "else: #'bert-large-uncased'\n",
        "  # paper value: 5.05\n",
        "  values_corpus_bleu[1][0] = 100 * value\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-TBC BLEU: 7.04\n",
            "BERT-Wiki103 BLEU: 8.51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkTWO7833RJD",
        "outputId": "3ff786bc-8fbb-432d-bca8-3a5c64f2c989"
      },
      "source": [
        "import random\n",
        "wiki1000_file = 'wiki_train_1000_samples.txt'\n",
        "##this comes from wikitext103 training set\n",
        "wiki1000_data = prepare_wiki(wiki1000_file)\n",
        "\n",
        "value = corpus_bleu(wiki1000_data, wiki_data)\n",
        "print(\"Wiki103_train-Wiki103 BLEU: %.2f\" % (100 * value))\n",
        "# paper value: 17.48\n",
        "values_corpus_bleu[1][3] = 100 * value\n",
        "\n",
        "\n",
        "value = corpus_bleu(wiki1000_data, tbc_data)\n",
        "print(\"Wiki103_train-TBC BLEU: %.2f\" % (100 * value))\n",
        "# paper value: 6.57\n",
        "values_corpus_bleu[2][3] = 100 * value\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wiki103_train-Wiki103 BLEU: 15.18\n",
            "Wiki103_train-TBC BLEU: 6.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQlDAZCJOOAI",
        "outputId": "acd02fab-3e42-46af-d27b-ef519bdd2d50"
      },
      "source": [
        "value = corpus_bleu(openai_sents, tbc_data)\n",
        "print(\"GPT-TBC BLEU: %.2f\" % (100 * value))\n",
        "# paper value 30.75\n",
        "values_corpus_bleu[2][2] = 100 * value\n",
        "\n",
        "value = corpus_bleu(openai_sents, wiki_data)\n",
        "print(\"GPT-Wiki103 BLEU: %.2f\" % (100 * value))\n",
        "# paper value: 10.81\n",
        "values_corpus_bleu[1][2] = 100 * value\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPT-TBC BLEU: 30.02\n",
            "GPT-Wiki103 BLEU: 11.32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd3F8xsLOQzm"
      },
      "source": [
        "## Diversity measures: Self-BLEU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMP9oz7ryuMo"
      },
      "source": [
        "Self-BLEU: treat each sentence as a hypothesis and treat rest of corpus as reference. Lower is better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqKa9k1mOdPg"
      },
      "source": [
        "#help(bleu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzg-4ECOjJ_m"
      },
      "source": [
        "The following function implements the self_bleu measure for diversity between one sentence and all the others in the document.\n",
        "Recall the difference between BLEU and SELF-BLEU. Since BLEU aims to assess how similar two sentences are, it can also be used to evaluate how one sentence resembles the rest in a generated collection. Regarding one sentence as hypothesis and the others as reference, we can calculate BLEU score for every generated sentence, and define the average BLEU score to be the Self-BLEU of the document.\n",
        "\n",
        "So the difference between BLEU and SELF-BLEU is that BLEU analyzes a group of generated sentences against a group of reference sentences. On the other hand, SELF-BLEU compares sentences of the same type (e.g. generated words)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SBZ_lSrjIz2"
      },
      "source": [
        "def self_bleu(sents):\n",
        "  # this function computes the scoring for comparing diversity between one sentence\n",
        "  # and all the others in the document.\n",
        "  # higher self-bleu score indicates less diversity in the project.\n",
        "    return bleu.corpus_bleu([[s for (j, s) in enumerate(sents) if j != i] for i in range(len(sents))], sents)\n",
        "  # function corpus_bleu(): for calculating the BLEU score for multiple sentences such as a paragraph or a document.\n",
        "  # https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
        "  # Self-BLEU, a metric to evaluate the diversity \n",
        "  # of the generated data. Since BLEU aims to assess how similar\n",
        "  # two sentences are, it can also be used to evaluate how one sentence \n",
        "  # resembles the rest in a generated collection. Regarding one sentence \n",
        "  # as hypothesis and the others as reference,\n",
        "  # we can calculate BLEU score for every generated sentence, \n",
        "  # and define the average BLEU score to be the Self-BLEU of the document.\n",
        "  # Self-BLEU: treat each sentence as a hypothesis and treat rest of corpus \n",
        "  # as reference. Lower is better."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iesVmPqo4Id3",
        "outputId": "9ac26df1-0a3e-4729-c802-8707df0ae1b5"
      },
      "source": [
        "# Some initializations for the table of self-BLEU\n",
        "print(model_version)# THIS WHOLE CODE BLOCK HAS TO BE REPEATED FOR\n",
        "# THE OTHER BERT MODEL VERSION TOO\n",
        "\n",
        "TITLE_SELF = ['Model', 'Self-BLEU']\n",
        "# values_self_bleu is a list of 2 lists (one with the model name, one for self-BLEU)\n",
        "# each one with 4 elements (first element refers to BERTlarge, second element is for\n",
        "# BERTbase, third element for GPT, fourth element for WT103)\n",
        "# initialization:\n",
        "values_self_bleu = [[0,0,0,0],[0,0,0,0]]\n",
        "values_self_bleu[0] = ['BERTlarge', 'BERTbase', 'GPT', 'WT103']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert-base-uncased\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J97zdsSvOoel",
        "outputId": "754d73da-f5c1-425a-b9d4-0b09dced41b8"
      },
      "source": [
        "value = self_bleu(bert_sents)\n",
        "print(\"BERT self-BLEU: %.2f\" % (100 * value))\n",
        "# paper value: 10,06\n",
        "if model_version == 'bert-base-uncased':\n",
        "  values_self_bleu[1][1] = 100 * value\n",
        "else:\n",
        "  values_self_bleu[1][0] = 100 * value\n",
        "\n",
        "value = self_bleu(openai_sents)\n",
        "print(\"OpenAI self-BLEU: %.2f\" % (100 * value))\n",
        "values_self_bleu[1][2] = 100 * value\n",
        "# paper value: 40.02\n",
        "\n",
        "value = self_bleu(wiki1000_data)\n",
        "print(\"Wiki103_train SELF-BLEU: %.2f\" % (100 * value))  \n",
        "# paper value: 9.80\n",
        "values_self_bleu[1][3] = 100 * value"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT self-BLEU: 8.49\n",
            "OpenAI self-BLEU: 38.09\n",
            "Wiki103_train SELF-BLEU: 17.42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okYwvVJAt8b-"
      },
      "source": [
        "## Diversity measures: n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcn28lZ0OVsg"
      },
      "source": [
        "from collections import Counter\n",
        "from nltk.util import ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRQt0X8dO8-L"
      },
      "source": [
        "Class Counter: Dict subclass for counting hashable items.  Sometimes called a bag or multiset.  Elements are stored as dictionary keys and their counts are stored as dictionary values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZp15ZojPRyo"
      },
      "source": [
        "Function ngrams: Return the ngrams generated from a sequence of items, as an iterator.\n",
        "For example:\n",
        "    \n",
        "    >>> from nltk.util import ngrams\n",
        "    >>> list(ngrams([1,2,3,4,5], 3))\n",
        "        [(1, 2, 3), (2, 3, 4), (3, 4, 5)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W28FYXammQFg"
      },
      "source": [
        "Other interesting measures are those regarding n-grams. In the following part we define _get_ngram_counts_ , _ref_unique_ngrams_ , _self_unique_ngrams_ ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti4_jlACOjim"
      },
      "source": [
        "def get_ngram_counts(sents, max_n=4):\n",
        "    size2count = {} #empty dictionary\n",
        "    for i in range(1, max_n + 1):\n",
        "        size2count[i] = Counter([n for sent in sents for n in ngrams(sent, i)])\n",
        "    return size2count\n",
        "    # size2count is a dictionary whose keys are \"i\" and for each i a counter is\n",
        "    # applied. For key 1, this counter counts all the occurrences of the 1-grams\n",
        "    # inside the sentences, while for key 2, this counter counts all the occurrences\n",
        "    # of the 2-grams (i.e. two consecutive words) inside the sentences, and \n",
        "    # so on for all the other keys up to max_n\n",
        "\n",
        "def ref_unique_ngrams(preds, refs, max_n=4):\n",
        "    # get # of *distinct* pred ngrams that don't appear in ref\n",
        "    pct_unique = {}\n",
        "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
        "    # builds the ngrams of the generated sentences\n",
        "    ref_ngrams = get_ngram_counts(refs, max_n)\n",
        "    # builds the ngrams of the reference sentences\n",
        "    for i in range(1, max_n + 1):\n",
        "        pred_ngram_counts = set(pred_ngrams[i].keys())\n",
        "        # with the above command we save the keys of the i-th dictionary (w.r.t \n",
        "        # our predicted sentences) inside\n",
        "        # a set.\n",
        "        total = sum(pred_ngrams[i].values())\n",
        "        # with the above command we compute the sum of all the occurrences\n",
        "        # of the grams of length i (i-grams).\n",
        "        ref_ngram_counts = set(ref_ngrams[i].keys())\n",
        "        # with the above command we save the keys of the i-th dictionary (w.r.t \n",
        "        # the reference sentences) inside\n",
        "        # a set.\n",
        "        pct_unique[i] = len(pred_ngram_counts.difference(ref_ngram_counts)) / total\n",
        "        # we measure the proportion of predicted i-grams that don't appear\n",
        "        # in the reference i-grams\n",
        "    return pct_unique\n",
        "        \n",
        "def self_unique_ngrams(preds, max_n=4):\n",
        "    # get # of pred ngrams with count 1\n",
        "    pct_unique = {}\n",
        "    # empty set\n",
        "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
        "    # build the set of dictionaries  where each dictionary contains the \n",
        "    # i-grams and the number of occurrences w.r.t the generated sentences (i.e.\n",
        "    # the predicted sentences).\n",
        "    for i in range(1, max_n + 1):\n",
        "        n_unique = len([k for k, v in pred_ngrams[i].items() if v == 1])\n",
        "        # n_unique is the number of i-grams that are unique \n",
        "        # in the i-th dictionary\n",
        "        total = sum(pred_ngrams[i].values())\n",
        "        pct_unique[i] = n_unique / total\n",
        "        # we measure the proportion of generated i-grams that are unique\n",
        "        # (i.e. which occure just one single time)\n",
        "    return pct_unique"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-SI5itbRmQr"
      },
      "source": [
        "_ref_unique_ngrams_: We use this function to count how many ngrams (in %) appear in preds  (which in our case is bert_sents, our generated sentences) and don't appearin refs (which in our case is wiki_data, our 5000 sentences from wiki103). The results are in table 2 in the paper.\n",
        "\n",
        "_self_unique_ngrams_: We count how many ngrams (in %) appear only 1 time in preds (bert_sents). The results are in table 2 in the paper\n",
        "\n",
        "_get_ngram_counts_: We need this function in order to define the 2 functions above. It creates a set of four dictionaries: each of them contains all the (1 or 2 or 3 or 4) ngrams with the respective number of occurrences.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iosYbHjhsnC",
        "outputId": "83150878-74c6-4cf6-8e1f-ca1093a7ecd3"
      },
      "source": [
        "max_n = 4\n",
        "print(model_version)\n",
        "\n",
        "TITLE2 = ['Model', '% unique 2-grams vs WT103', '% unique 3-grams vs WT103', '% unique 4-grams vs WT103']\n",
        "TITLE1 = ['Model', '% unique 2-grams vs Self', '% unique 3-grams vs Self', '% unique 4-grams vs Self']\n",
        "TITLE3 = ['Model', '% unique 2-grams vs TBC', '% unique 3-grams vs TBC', '% unique 4-grams vs TBC']\n",
        "# values_grams_VS_SELF is a list of 4 lists (one with the model name, one for\n",
        "# n=2, one for n=3, one for n=4) each one with 4 elements (first element is \n",
        "# the percentage of unique n-grams of BERTlarge VS ITSELF, second element is the \n",
        "# percentage of unique n-grams of BERTbase VS ITSELF, third element is the percentage\n",
        "# of unique n-grams of GPT VS ITSELF, fourth element is the percentage of unique\n",
        "# n-grams of WT103 VS ITSELF)\n",
        "# initialization:\n",
        "values_grams_VS_SELF = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
        "values_grams_VS_SELF[0] = ['BERTlarge', 'BERTbase', 'GPT', 'WT103']\n",
        "\n",
        "# values_grams_VS_WT103 is a list of 3 lists (one for\n",
        "# n=2, one for n=3, one for n=4) each one with 4 elements (first element is \n",
        "# the percentage of unique n-grams of BERTlarge VS WT103, second element is the \n",
        "# percentage of unique n-grams of BERTbase VS WT103, third element is the percentage\n",
        "# of unique n-grams of GPT VS WT103, fourth element is the percentage of unique\n",
        "# n-grams of WT103 VS WT103)\n",
        "# initialization:\n",
        "values_grams_VS_WT103 = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
        "values_grams_VS_WT103[0] = ['BERTlarge', 'BERTbase', 'GPT', 'WT103']\n",
        "\n",
        "# values_grams_VS_TBC is a list of 3 lists (one for\n",
        "# n=2, one for n=3, one for n=4) each one with 4 elements (first element is \n",
        "# the percentage of unique n-grams of BERTlarge VS TBC, second element is the \n",
        "# percentage of unique n-grams of BERTbase VS TBC, third element is the percentage\n",
        "# of unique n-grams of GPT VS TBC, fourth element is the percentage of unique\n",
        "# n-grams of WT103 VS TBC)\n",
        "# initialization:\n",
        "values_grams_VS_TBC = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
        "values_grams_VS_TBC[0] = ['BERTlarge', 'BERTbase', 'GPT', 'WT103']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert-base-uncased\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccreOUDgOsi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0feff281-de5e-4d7e-bb4f-96c14f613cf0"
      },
      "source": [
        "# BERT VS WT103\n",
        "pct_uniques = ref_unique_ngrams(bert_sents, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1) and (model_version == 'bert-base-uncased'): #'bert-large-uncased'\n",
        "      values_grams_VS_WT103[i-1][1] = 100 * pct_uniques[i]\n",
        "    elif (i != 1):  #'bert-large-uncased'\n",
        "      values_grams_VS_WT103[i-1][0] = 100 * pct_uniques[i]\n",
        "\n",
        "# BERT VS TBC\n",
        "pct_uniques = ref_unique_ngrams(bert_sents, tbc_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to TBC: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1) and (model_version == 'bert-base-uncased'): #'bert-large-uncased'\n",
        "      values_grams_VS_TBC[i-1][1] = 100 * pct_uniques[i]\n",
        "    elif (i != 1):  #'bert-large-uncased'\n",
        "      values_grams_VS_TBC[i-1][0] = 100 * pct_uniques[i]\n",
        "\n",
        "# BERT VS BERT\n",
        "pct_uniques = self_unique_ngrams(bert_sents, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1) and (model_version == 'bert-base-uncased'): #'bert-large-uncased'\n",
        "      values_grams_VS_SELF[i-1][1] = 100 * pct_uniques[i]\n",
        "    elif (i != 1):  #'bert-large-uncased'\n",
        "      values_grams_VS_SELF[i-1][0] = 100 * pct_uniques[i]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT unique 1-grams relative to Wiki: 9.42\n",
            "BERT unique 2-grams relative to Wiki: 59.05\n",
            "BERT unique 3-grams relative to Wiki: 91.80\n",
            "BERT unique 4-grams relative to Wiki: 98.60\n",
            "BERT unique 1-grams relative to TBC: 12.40\n",
            "BERT unique 2-grams relative to TBC: 62.68\n",
            "BERT unique 3-grams relative to TBC: 92.53\n",
            "BERT unique 4-grams relative to TBC: 98.67\n",
            "BERT unique 1-grams relative to self: 12.38\n",
            "BERT unique 2-grams relative to self: 63.13\n",
            "BERT unique 3-grams relative to self: 92.38\n",
            "BERT unique 4-grams relative to self: 98.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBMJXP99_2Vg"
      },
      "source": [
        "We understand from the table that the BERT with a higher number of parameters (BERT Large) gives better results than BERT with a standard number of parameters (BERT Base), infact the percentage of unique n-grams is always higher (for both n=2, n=3 and n=4), meaning more diverse generated sentences. With the same considerations we conclude that the generated words are more diverse with BERT than using GPT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Way1aShUOyMT",
        "outputId": "0a51366a-6bcf-4a0f-f902-c56a9baaef51"
      },
      "source": [
        "# GPT VS WT103\n",
        "pct_uniques = ref_unique_ngrams(openai_sents, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"GPT unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1):\n",
        "      values_grams_VS_WT103[i-1][2] = 100 * pct_uniques[i]\n",
        "\n",
        "# GPT VS TBC\n",
        "pct_uniques = ref_unique_ngrams(openai_sents, tbc_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"GPT unique %d-grams relative to TBC: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1):\n",
        "      values_grams_VS_TBC[i-1][2] = 100 * pct_uniques[i]\n",
        "\n",
        "# GPT VS GPT\n",
        "pct_uniques = self_unique_ngrams(openai_sents, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"GPT unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1):\n",
        "      values_grams_VS_SELF[i-1][2] = 100 * pct_uniques[i]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPT unique 1-grams relative to Wiki: 3.15\n",
            "GPT unique 2-grams relative to Wiki: 34.11\n",
            "GPT unique 3-grams relative to Wiki: 73.93\n",
            "GPT unique 4-grams relative to Wiki: 91.90\n",
            "GPT unique 1-grams relative to TBC: 2.10\n",
            "GPT unique 2-grams relative to TBC: 26.05\n",
            "GPT unique 3-grams relative to TBC: 66.06\n",
            "GPT unique 4-grams relative to TBC: 89.14\n",
            "GPT unique 1-grams relative to self: 4.42\n",
            "GPT unique 2-grams relative to self: 31.83\n",
            "GPT unique 3-grams relative to self: 68.60\n",
            "GPT unique 4-grams relative to self: 88.60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5I9WwAF5xs3"
      },
      "source": [
        "In the following block we fill the last row of table 2 (regarding WT103). In particular remember that the WT103 on the rows is a sample of 1000 sentences from the training dataset (we sampled after removing the titles from the original training dataset). In the column WT103 we consider a sample of 5000 words from the test set (see link https://github.com/nyu-dl/bert-gen/blob/master/data/wiki103.5k.txt )."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snUpf9qp58JR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fafbd2be-8254-4cc8-ec58-e9e16699d1e9"
      },
      "source": [
        "# WT103 (1000) VS WT103 (1000) (SELF)\n",
        "pct_uniques = self_unique_ngrams(wiki1000_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"WT103 unique %d-grams relative to self(1000): %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1):\n",
        "      values_grams_VS_SELF[i-1][3] = 100 * pct_uniques[i]\n",
        "\n",
        "# WT103 (1000) VS WT103 (5000)\n",
        "pct_uniques = ref_unique_ngrams(wiki1000_data, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"WT103 unique %d-grams relative to WT103(5000): %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1):\n",
        "      values_grams_VS_WT103[i-1][3] = 100 * pct_uniques[i]\n",
        "\n",
        "# WT103 (1000) VS TBC\n",
        "pct_uniques = ref_unique_ngrams(wiki1000_data,  tbc_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"WT103 unique %d-grams relative to TBC: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1):\n",
        "      values_grams_VS_TBC[i-1][3] = 100 * pct_uniques[i]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WT103 unique 1-grams relative to self(1000): 7.05\n",
            "WT103 unique 2-grams relative to self(1000): 51.79\n",
            "WT103 unique 3-grams relative to self(1000): 85.86\n",
            "WT103 unique 4-grams relative to self(1000): 96.76\n",
            "WT103 unique 1-grams relative to WT103(5000): 7.52\n",
            "WT103 unique 2-grams relative to WT103(5000): 50.67\n",
            "WT103 unique 3-grams relative to WT103(5000): 85.55\n",
            "WT103 unique 4-grams relative to WT103(5000): 96.81\n",
            "WT103 unique 1-grams relative to TBC: 10.18\n",
            "WT103 unique 2-grams relative to TBC: 57.59\n",
            "WT103 unique 3-grams relative to TBC: 89.34\n",
            "WT103 unique 4-grams relative to TBC: 97.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnUg1u_4fula"
      },
      "source": [
        "__________\n",
        "__________\n",
        "# TABLES\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y5ExNaU4q6e"
      },
      "source": [
        "Since we want to replicate the results of the original paper, we can build a table where to put them, and we do that using the following code block. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cCC1IK_95KtH",
        "outputId": "54797034-8f76-4fae-fb7e-8636078b1570"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "# Three tables of the n-grams percentage agains SELF, WT103, TBC.\n",
        "Table2_n_grams_SELF = go.Figure(\n",
        "    data=[go.Table(\n",
        "        header=dict(values=TITLE1),\n",
        "        cells=dict(values=values_grams_VS_SELF))\n",
        "                     ])\n",
        "Table2_n_grams_SELF.show()\n",
        "# values_grams_VS_SELF is completed during the process.\n",
        "\n",
        "Table2_n_grams_WT103 = go.Figure(\n",
        "    data=[go.Table(\n",
        "        header=dict(values=TITLE2),\n",
        "        cells=dict(values=values_grams_VS_WT103))\n",
        "                     ])\n",
        "Table2_n_grams_WT103.show()\n",
        "# values_grams_VS_WT103 is completed during the process.\n",
        "\n",
        "Table2_n_grams_TBC = go.Figure(\n",
        "    data=[go.Table(\n",
        "        header=dict(values=TITLE3),\n",
        "        cells=dict(values=values_grams_VS_TBC))\n",
        "                     ])\n",
        "Table2_n_grams_TBC.show()\n",
        "# values_grams_VS_TBC is completed during the process."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"243c594d-6991-466b-bd0e-38f3a9bd7bf5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"243c594d-6991-466b-bd0e-38f3a9bd7bf5\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '243c594d-6991-466b-bd0e-38f3a9bd7bf5',\n",
              "                        [{\"cells\": {\"values\": [[\"BERTlarge\", \"BERTbase\", \"GPT\", \"WT103\"], [0, 63.13419723259389, 31.827446600586345, 51.7874707155248], [0, 92.38087172538391, 68.59974149073676, 85.85515928508919], [0, 98.24250522891006, 88.59973384592637, 96.75639753475288]]}, \"header\": {\"values\": [\"Model\", \"% unique 2-grams vs Self\", \"% unique 3-grams vs Self\", \"% unique 4-grams vs Self\"]}, \"type\": \"table\"}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('243c594d-6991-466b-bd0e-38f3a9bd7bf5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"78aa3616-0802-452d-9784-a0137fd6afc4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"78aa3616-0802-452d-9784-a0137fd6afc4\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '78aa3616-0802-452d-9784-a0137fd6afc4',\n",
              "                        [{\"cells\": {\"values\": [[\"BERTlarge\", \"BERTbase\", \"GPT\", \"WT103\"], [0, 59.05172413793104, 34.10582158313556, 50.665271742046244], [0, 91.79652213188798, 73.9279046388051, 85.55472351789533], [0, 98.59690913316291, 91.90004435901227, 96.80919824847287]]}, \"header\": {\"values\": [\"Model\", \"% unique 2-grams vs WT103\", \"% unique 3-grams vs WT103\", \"% unique 4-grams vs WT103\"]}, \"type\": \"table\"}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('78aa3616-0802-452d-9784-a0137fd6afc4');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"1e56c5bb-b01a-4f8d-939b-cbf0cafc12d3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"1e56c5bb-b01a-4f8d-939b-cbf0cafc12d3\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '1e56c5bb-b01a-4f8d-939b-cbf0cafc12d3',\n",
              "                        [{\"cells\": {\"values\": [[\"BERTlarge\", \"BERTbase\", \"GPT\", \"WT103\"], [0, 62.681199209312545, 26.053329610498395, 57.587138079653776], [0, 92.52766485998193, 66.06347838575327, 89.33678578839577], [0, 98.66662793399954, 89.13795652816798, 97.9416825220534]]}, \"header\": {\"values\": [\"Model\", \"% unique 2-grams vs TBC\", \"% unique 3-grams vs TBC\", \"% unique 4-grams vs TBC\"]}, \"type\": \"table\"}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1e56c5bb-b01a-4f8d-939b-cbf0cafc12d3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiCnhAAP8Ewo"
      },
      "source": [
        "We also add a table concerning Self-BLEU and one for Corpus-BLEU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUibpj258O7g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e7169f1-4868-4f6d-c8e3-c9a0ab29c60d"
      },
      "source": [
        "# Self-BLEU\n",
        "Table2_self_bleu = go.Figure(\n",
        "    data=[go.Table(\n",
        "        header=dict(values=TITLE_SELF),\n",
        "        cells=dict(values=values_self_bleu))\n",
        "                     ])\n",
        "Table2_self_bleu.show()\n",
        "# values_self_bleu is completed during the process.\n",
        "\n",
        "# Corpus - BLEU\n",
        "Table3_corpus_bleu = go.Figure(\n",
        "    data=[go.Table(\n",
        "        header=dict(values=TITLE_CORPUS),\n",
        "        cells=dict(values=values_corpus_bleu))\n",
        "                     ])\n",
        "Table3_corpus_bleu.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"54f8e8a9-7e22-4d01-8444-82b8a23d01d7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"54f8e8a9-7e22-4d01-8444-82b8a23d01d7\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '54f8e8a9-7e22-4d01-8444-82b8a23d01d7',\n",
              "                        [{\"cells\": {\"values\": [[\"BERTlarge\", \"BERTbase\", \"GPT\", \"WT103\"], [0, 8.489368030007881, 38.08559860514548, 17.41572486098709]]}, \"header\": {\"values\": [\"Model\", \"Self-BLEU\"]}, \"type\": \"table\"}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('54f8e8a9-7e22-4d01-8444-82b8a23d01d7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"9cc39200-1d55-4747-8eb2-dccee68fbd97\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"9cc39200-1d55-4747-8eb2-dccee68fbd97\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '9cc39200-1d55-4747-8eb2-dccee68fbd97',\n",
              "                        [{\"cells\": {\"values\": [[\"BERTlarge\", \"BERTbase\", \"GPT\", \"WT103\"], [0, 8.509974777023338, 11.317101000095223, 15.183261709738813], [0, 7.043918694090595, 30.01719221013679, 6.039053452386266]]}, \"header\": {\"values\": [\"Model\", \"Corpus-BLEU against WT103\", \"Corpus-BLEU against TBC\"]}, \"type\": \"table\"}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9cc39200-1d55-4747-8eb2-dccee68fbd97');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwkD7Dq3_Vr5"
      },
      "source": [
        "# TEXYGEN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "104-3-zuIaCy"
      },
      "source": [
        "## Introductory commands"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YoeQObGBUNg"
      },
      "source": [
        "!git clone https://github.com/geek-ai/Texygen.git\n",
        "%cd Texygen\n",
        "# we clone the Texygen repository from github"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtJePk2KBXlS"
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "#these are the libraries required for the Texygen models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBrG9KdbNwd9"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "#Some functions of Texygen require the old version of tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MT3GBMGO6qk"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdWBcQ6YRN04"
      },
      "source": [
        "## Texygen tutorial:\n",
        "\n",
        "python main.py -g GAN type -t training method -d data location\n",
        "\n",
        "  -g GAN type : \n",
        "    specify the GAN type in the experiment\n",
        "\n",
        "    (GAN type = seqgan | maligan | rankgan | leakgan | gsgan | textgan | mle)\n",
        "\n",
        "  -t training method :\n",
        "    specify the traning method in the experiment\n",
        "\n",
        "    (training method = oracle | cfg | real  ;  default is oracle)\n",
        "\n",
        "  -d data location : \n",
        "    use user's own dataset only avaiable with real data training \n",
        "    (default is 'data/image_coco.txt')\n",
        "\n",
        "more details: https://github.com/geek-ai/Texygen/blob/master/docs/doc.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr9cKc2fGSG1"
      },
      "source": [
        "The models inside Texygen are: \n",
        "- seqgan \n",
        "- maligan \n",
        "- rankgan \n",
        "- leakgan \n",
        "- gsgan \n",
        "- textgan \n",
        "- mle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge3JRH5rIuuM"
      },
      "source": [
        "## Texygen models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_iBlRIEBZlb"
      },
      "source": [
        "!python main.py -g mle -t real -d 'wiki_train_1000_samples.txt'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y91ghKZ8Ipsk"
      },
      "source": [
        "## Texygen metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SbLcB6VWHmh"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.insert(1, os.path.join(\".\", \"Texygen/utils\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OT2b8nAUpfu"
      },
      "source": [
        "import os\n",
        "from multiprocessing import Pool\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import SmoothingFunction                    \n",
        "\n",
        "# import Texygen metrics\n",
        "from utils.metrics.Metrics import Metrics\n",
        "\n",
        "from Texygen.utils.metrics.Bleu import Bleu\n",
        "from Texygen.utils.metrics.SelfBleu import SelfBleu\n",
        "from Texygen.utils.metrics.EmbSim import EmbSim\n",
        "from Texygen.utils.metrics.Nll import Nll\n",
        "from Texygen.utils.metrics.UniqueGram import UniqueGram\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_bJwDfreFaT"
      },
      "source": [
        "**BERT VALUES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ3IqVgdiIXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb88bba4-ecb0-47ad-cdd8-16470b68ee74"
      },
      "source": [
        "print(\"BERT-WIKI BLEU: %.2f\" % (100 *Bleu.get_score(Bleu('Bert_using_pytorch.txt', wiki103_file))))\n",
        "print(\"BERT-self-BLEU: %.2f\" % (100 *SelfBleu.get_score(SelfBleu('Bert_using_pytorch.txt')))) \n",
        "print(\"BERT-unique4grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('Bert_using_pytorch.txt',4))))\n",
        "print(\"BERT-unique3grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('Bert_using_pytorch.txt',3))))\n",
        "print(\"BERT-unique2grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('Bert_using_pytorch.txt',2))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-WIKI BLEU: 8.43\n",
            "BERT-self-BLEU: 16.27\n",
            "BERT-unique4grams: 3410.90\n",
            "BERT-unique3grams: 3383.60\n",
            "BERT-unique2grams: 2664.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q23EJZbceL9F"
      },
      "source": [
        "**GPT VALUES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrZpDMNWcv6T",
        "outputId": "8be9a5c8-4a9c-46ec-a0e8-34c185d60b80"
      },
      "source": [
        "print(\"GPT-WIKI BLEU: %.2f\" % (100 *Bleu.get_score(Bleu('openaitext.txt', wiki103_file))))\n",
        "print(\"GPT-self-BLEU: %.2f\" % (100 *SelfBleu.get_score(SelfBleu('openaitext.txt')))) \n",
        "print(\"GPT-unique4grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('openaitext.txt',4))))\n",
        "print(\"GPT-unique3grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('openaitext.txt',3))))\n",
        "print(\"GPT-unique2grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('openaitext.txt',2))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPT-WIKI BLEU: 9.98\n",
            "GPT-self-BLEU: 49.60\n",
            "GPT-unique4grams: 3132.60\n",
            "GPT-unique3grams: 2688.70\n",
            "GPT-unique2grams: 1548.80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CmjIFLiePnS"
      },
      "source": [
        "**MLE VALUES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q1Qkkbgf4uE",
        "outputId": "36a3eb59-bc8c-4fbe-ea0f-0b044f22ea03"
      },
      "source": [
        "print(\"MLE-WIKI BLEU: %.2f\" % (100 *Bleu.get_score(Bleu('/content/Texygen/save/test_file.txt', wiki103_file))))\n",
        "print(\"MLE-self-BLEU: %.2f\" % (100 *SelfBleu.get_score(SelfBleu('/content/Texygen/save/test_file.txt')))) \n",
        "print(\"MLE-unique4grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('/content/Texygen/save/test_file.txt',4))))\n",
        "print(\"MLE-unique3grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('/content/Texygen/save/test_file.txt',3))))\n",
        "print(\"MLE-unique2grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('/content/Texygen/save/test_file.txt',2))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLE-WIKI BLEU: 12.61\n",
            "MLE-self-BLEU: 25.08\n",
            "MLE-unique4grams: 5390.45\n",
            "MLE-unique3grams: 4743.94\n",
            "MLE-unique2grams: 2420.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsLu-Kg-1Jn8"
      },
      "source": [
        "ATTENTION on Self-BLEU: We observe some different results from those of the paper implementation. The reason is the fact that when using self-BLEU for the BERT model (those functions contained in the generation part of the BERT model) we compute self-BLEU by averaging the results of Corpus-BLEU for each sentence as hypothesis against all the other as references. Instead, in the self-BLEU inside the Texygen platform we have another definition of self-BLEU (slightly different, but brings to different results), infact the command sentence bleu is used.\n",
        "\n",
        "In the following we will use the Texygen implementation, but be aware of this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc1t5ZTtUqld"
      },
      "source": [
        "ATTENTION on unique n-grams: as in the case of self-BLEU, also in the unique-grams definition there are some differences of Texygen with respect to BERT measures. One of the main reasons is that the value at the denominator for the Texygen implementation corresponds to the number of sentences, whereas in the \"correct\" definition of unique-grams it should be the total number of n-grams, which is clearly a higher number (in BERT this second definition is proposed). As a consequence, in the case of Texygen we have really higher results that those obtained for BERT unique-grams definition.\n",
        "\n",
        "In the following we will use the Texygen implementation, but be aware of this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kufjGFoFsNuH"
      },
      "source": [
        "____________________\n",
        "____________________\n",
        "# COMPARISON WITH TRANSFORMER XL AND XL-NET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Beq_WzXQcaGc"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2PGQUQEccgR"
      },
      "source": [
        "from transformers import pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLnWtAbvce_N"
      },
      "source": [
        "transfo_xl_generator = pipeline('text-generation', model='transfo-xl-wt103')\n",
        "xlnet_generator = pipeline('text-generation', model='xlnet-base-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx9YZtZ6cmg3"
      },
      "source": [
        "with open('wt40.txt') as f:\n",
        "  content = f.readlines()\n",
        "f2 = open('transfxl_gen.txt', 'w')\n",
        "f3 = open('xlnet_gen.txt', 'w')\n",
        "\n",
        "for line in content:\n",
        "  #prompt = line[:-1]\n",
        "  prompt = line.rstrip('\\n')\n",
        "  res2 = transfo_xl_generator(prompt, max_length=40, do_sample=True, temperature=0.9)\n",
        "  print('TRANSFO_XL:'+res2[0]['generated_text'])\n",
        "  res3 = transfo_xl_generator(prompt, max_length=40, do_sample=True, temperature=0.9)\n",
        "  print('XL_NET:'+res3[0]['generated_text']+'\\n')\n",
        "  f2.write(res2[0]['generated_text']+\"\\n\")\n",
        "  f3.write(res3[0]['generated_text']+\"\\n\")\n",
        "\n",
        "f2.close()\n",
        "f3.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCfOxjyUgp5c"
      },
      "source": [
        "##Comparison using Texygen metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "henoT71n98lU",
        "outputId": "09931a51-007e-4fd0-93a3-d6a38646bb7d"
      },
      "source": [
        "print(\"transfoxl-WIKI BLEU: %.2f\" % (100 *Bleu.get_score(Bleu('transfxl_gen.txt', wiki103_file))))\n",
        "print(\"transfoxl-self-BLEU: %.2f\" % (100 *SelfBleu.get_score(SelfBleu('transfxl_gen.txt')))) \n",
        "print(\"transfoxl-unique4grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('transfxl_gen.txt',4))))\n",
        "print(\"transfoxl-unique3grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('transfxl_gen.txt',3))))\n",
        "print(\"transfoxl-unique2grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('transfxl_gen.txt',2))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "transfoxl-WIKI BLEU: 11.65\n",
            "transfoxl-self-BLEU: 12.26\n",
            "transfoxl-unique4grams: 2985.00\n",
            "transfoxl-unique3grams: 3030.00\n",
            "transfoxl-unique2grams: 2805.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj2iGPMAIv0e",
        "outputId": "2942ba97-188e-4ba3-c565-21966ab5461c"
      },
      "source": [
        "print(\"xlnet-WIKI BLEU: %.2f\" % (100 *Bleu.get_score(Bleu('xlnet_gen.txt', wiki103_file))))\n",
        "print(\"xlnet-self-BLEU: %.2f\" % (100 *SelfBleu.get_score(SelfBleu('xlnet_gen.txt')))) \n",
        "print(\"xlnet-unique4grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('xlnet_gen.txt',4))))\n",
        "print(\"xlnet-unique3grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('xlnet_gen.txt',3))))\n",
        "print(\"xlnet-unique2grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('xlnet_gen.txt',2))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xlnet-WIKI BLEU: 13.71\n",
            "xlnet-self-BLEU: 11.78\n",
            "xlnet-unique4grams: 3015.00\n",
            "xlnet-unique3grams: 3057.50\n",
            "xlnet-unique2grams: 2800.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpnbsHrCg1-L"
      },
      "source": [
        "##Comparison using metrics from the Evaluation part above\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gVkML33htIH"
      },
      "source": [
        "#wiki103_file = 'datawiki103.5k.txt'\n",
        "#wiki_data = prepare_wiki(wiki103_file)\n",
        "transfxl_data = prepare_data('transfxl_gen.txt')\n",
        "xlnet_data = prepare_data('xlnet_gen.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlrm8xRAiSXb",
        "outputId": "b3b233e8-bbd9-4aaf-8cad-e3ed075e5810"
      },
      "source": [
        "#TRANSFORMER-XL EVALUATION (VS WIKI AND SELF)\n",
        "\n",
        "value = corpus_bleu(transfxl_data, wiki_data)\n",
        "print(\"transfoXL-WIKI BLEU: %.2f\" % (100 * value))\n",
        "value = self_bleu(transfxl_data)\n",
        "print(\"transfoXL self-BLEU: %.2f\" % (100 * value))\n",
        "\n",
        "pct_uniques = ref_unique_ngrams(transfxl_data, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"transfoXL unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "\n",
        "pct_uniques = self_unique_ngrams(transfxl_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"transfoXL unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "transfoXL-WIKI BLEU: 12.08\n",
            "transfoXL self-BLEU: 5.25\n",
            "transfoXL unique 1-grams relative to Wiki: 18.77\n",
            "transfoXL unique 2-grams relative to Wiki: 62.11\n",
            "transfoXL unique 3-grams relative to Wiki: 90.89\n",
            "transfoXL unique 4-grams relative to Wiki: 98.76\n",
            "transfoXL unique 1-grams relative to self: 41.39\n",
            "transfoXL unique 2-grams relative to self: 86.42\n",
            "transfoXL unique 3-grams relative to self: 97.61\n",
            "transfoXL unique 4-grams relative to self: 99.43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJF7TejDmZYE",
        "outputId": "6bce01f1-e3f7-4274-acdf-213a98d5ae59"
      },
      "source": [
        "#XL-NET EVALUATION (VS WIKI AND SELF)\n",
        "\n",
        "value = corpus_bleu(xlnet_data, wiki_data)\n",
        "print(\"XLNet-WIKI BLEU: %.2f\" % (100 * value))\n",
        "value = self_bleu(xlnet_data)\n",
        "print(\"XLNet self-BLEU: %.2f\" % (100 * value))\n",
        "\n",
        "pct_uniques = ref_unique_ngrams(xlnet_data, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"XLNet unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "\n",
        "pct_uniques = self_unique_ngrams(xlnet_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"XLNet unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XLNet-WIKI BLEU: 14.35\n",
            "XLNet self-BLEU: 0.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "XLNet unique 1-grams relative to Wiki: 18.96\n",
            "XLNet unique 2-grams relative to Wiki: 61.54\n",
            "XLNet unique 3-grams relative to Wiki: 89.92\n",
            "XLNet unique 4-grams relative to Wiki: 98.29\n",
            "XLNet unique 1-grams relative to self: 42.44\n",
            "XLNet unique 2-grams relative to self: 85.94\n",
            "XLNet unique 3-grams relative to self: 98.17\n",
            "XLNet unique 4-grams relative to self: 99.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cit155gmjt1",
        "outputId": "ef90ffb8-e052-4f81-a06a-22aa9a4f2d0c"
      },
      "source": [
        "#XL-NET vs TRANSFORMER-XL\n",
        "\n",
        "value = corpus_bleu(xlnet_data, transfxl_data)\n",
        "print(\"XLNet-transfoXL BLEU: %.2f\" % (100 * value))\n",
        "value = corpus_bleu(transfxl_data, xlnet_data)\n",
        "print(\"transfoXL-XLNet BLEU: %.2f\" % (100 * value))\n",
        "\n",
        "pct_uniques = ref_unique_ngrams(xlnet_data, transfxl_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"XLNet unique %d-grams relative to transfoXL: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    pct_uniques = ref_unique_ngrams(transfxl_data, xlnet_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"transfoXL unique %d-grams relative to XLNet: %.2f\" % (i, 100 * pct_uniques[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XLNet-transfoXL BLEU: 24.84\n",
            "transfoXL-XLNet BLEU: 25.87\n",
            "XLNet unique 1-grams relative to transfoXL: 31.00\n",
            "XLNet unique 2-grams relative to transfoXL: 68.68\n",
            "XLNet unique 3-grams relative to transfoXL: 81.69\n",
            "XLNet unique 4-grams relative to transfoXL: 86.63\n",
            "transfoXL unique 1-grams relative to XLNet: 29.48\n",
            "transfoXL unique 2-grams relative to XLNet: 68.68\n",
            "transfoXL unique 3-grams relative to XLNet: 81.69\n",
            "transfoXL unique 4-grams relative to XLNet: 86.63\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}