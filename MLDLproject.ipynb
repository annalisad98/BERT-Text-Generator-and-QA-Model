{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MLDLproject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "97f2e42c4ad0443fb54074c7b10163f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a82a2033ed2c4d87a55b4943b1b79807",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_73dad25b4ee74026b7b9c1dd2fbe53de",
              "IPY_MODEL_984fab6eeae74104979b253b5bba6d90"
            ]
          }
        },
        "a82a2033ed2c4d87a55b4943b1b79807": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "73dad25b4ee74026b7b9c1dd2fbe53de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2e711981b5cc4f95a29538a153dde9bb",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 856,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 856,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b29e0e813c8b47f39b60a21e660eecac"
          }
        },
        "984fab6eeae74104979b253b5bba6d90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a25344d53dd24bcfb547d93d4555944e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 856/856 [00:00&lt;00:00, 3.82kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f4054d0270d14193bdcc4be53df32940"
          }
        },
        "2e711981b5cc4f95a29538a153dde9bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b29e0e813c8b47f39b60a21e660eecac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a25344d53dd24bcfb547d93d4555944e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f4054d0270d14193bdcc4be53df32940": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "54a8ed4c8669495ba133471d2a666d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_520e41a087ca462b8950ca30615886d7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ab7a83feccfb4db78c3602c22c857135",
              "IPY_MODEL_bf63514875ce438e849836fe98493c98"
            ]
          }
        },
        "520e41a087ca462b8950ca30615886d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab7a83feccfb4db78c3602c22c857135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2d2c821f8ee44afe9af64490c5461f26",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1140884800,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1140884800,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_34b7e5e6f16549a0922ca6f827c3fdec"
          }
        },
        "bf63514875ce438e849836fe98493c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5c77298c1f2e417b986f8135df1dcf41",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.14G/1.14G [00:51&lt;00:00, 22.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec2d603f2c344259a684beba6832dff1"
          }
        },
        "2d2c821f8ee44afe9af64490c5461f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "34b7e5e6f16549a0922ca6f827c3fdec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c77298c1f2e417b986f8135df1dcf41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec2d603f2c344259a684beba6832dff1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6f2255cc9044fd680637c7ae003bc06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4e3b7e9b085542f08a14b4ac0f1b0eee",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ad13f591260245d2ae46c0c963e19e84",
              "IPY_MODEL_0abcab9090964aba89d9196dfc9e1a0b"
            ]
          }
        },
        "4e3b7e9b085542f08a14b4ac0f1b0eee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad13f591260245d2ae46c0c963e19e84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2c7d2ec6e563464791b30c87b477ffd8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9143470,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9143470,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_69b91256452b48b5845101bb79a9b77b"
          }
        },
        "0abcab9090964aba89d9196dfc9e1a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5817e10c0dbf45219a31ff237c8ec3f1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9.14M/9.14M [00:01&lt;00:00, 7.64MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c002a579ded949339d22948de24d88ca"
          }
        },
        "2c7d2ec6e563464791b30c87b477ffd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "69b91256452b48b5845101bb79a9b77b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5817e10c0dbf45219a31ff237c8ec3f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c002a579ded949339d22948de24d88ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0f7c73a3c114afca07baca94de99725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f0096069ced449edb44706d423232738",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f9c1f82d4e95477b9fccb1c8537e973d",
              "IPY_MODEL_ec96899043a14aa284db8a8d90d9f9d6"
            ]
          }
        },
        "f0096069ced449edb44706d423232738": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f9c1f82d4e95477b9fccb1c8537e973d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d4e32612458d4217b8a1ed658cd868b9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 9143613,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 9143613,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b289c6738a574c15b065d147e1ec8535"
          }
        },
        "ec96899043a14aa284db8a8d90d9f9d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5ef10844d1654073900dbde3977498f6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9.14M/9.14M [00:13&lt;00:00, 693kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7a9730d4298e45ef8cf1f95f6807dc8f"
          }
        },
        "d4e32612458d4217b8a1ed658cd868b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b289c6738a574c15b065d147e1ec8535": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ef10844d1654073900dbde3977498f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7a9730d4298e45ef8cf1f95f6807dc8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9f466c8ea3e147abb01b14d46e1aa75f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_74c9b6ab13604d85823ecb061cf0131e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7ea6ac39ed1e46c19fbb7c0c388831f5",
              "IPY_MODEL_bdb61c1733684d2abb534a2b3897f5e6"
            ]
          }
        },
        "74c9b6ab13604d85823ecb061cf0131e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7ea6ac39ed1e46c19fbb7c0c388831f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_74db024bc472408487bb66d3b7bb904c",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 760,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 760,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d09702fa85f54c35b2876e2b51a58ede"
          }
        },
        "bdb61c1733684d2abb534a2b3897f5e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2d03a401fb074debba71ee51cc807a35",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 760/760 [00:00&lt;00:00, 846B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_59b4d6737dd84c96b862a23384f2b20e"
          }
        },
        "74db024bc472408487bb66d3b7bb904c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d09702fa85f54c35b2876e2b51a58ede": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d03a401fb074debba71ee51cc807a35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "59b4d6737dd84c96b862a23384f2b20e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eccc4a015fb541558eea559ff3c1d3f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6527094f05b2445ebcdac49c76c8472c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c5df7587b112422e921b2fea53e52605",
              "IPY_MODEL_de24f3b7ee5540cba9cdabf677a00eaa"
            ]
          }
        },
        "6527094f05b2445ebcdac49c76c8472c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c5df7587b112422e921b2fea53e52605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_682f24b019de4e44a04447c33ba50a8f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 467042463,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 467042463,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_92e97af53b9a4e02a66756cf2292a831"
          }
        },
        "de24f3b7ee5540cba9cdabf677a00eaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_58f58e699398417e8d7fe600a93bbeab",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 467M/467M [00:09&lt;00:00, 47.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_05e33c3b29ca46828e1cee85a4c5a44f"
          }
        },
        "682f24b019de4e44a04447c33ba50a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "92e97af53b9a4e02a66756cf2292a831": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "58f58e699398417e8d7fe600a93bbeab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "05e33c3b29ca46828e1cee85a4c5a44f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ef93fcbaf1eb405892e00f476f39e74b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b26875cbcb84435f92130cfe6010cb81",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_877b7adb5d9a4e179c57223159ba0d57",
              "IPY_MODEL_186088f797f0433e9f98b0e20cc75274"
            ]
          }
        },
        "b26875cbcb84435f92130cfe6010cb81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "877b7adb5d9a4e179c57223159ba0d57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e2a522ae6b8447f7a5d8a02e9a7d31d3",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 798011,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 798011,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b8ac8b8affb049e38222ae57be44a524"
          }
        },
        "186088f797f0433e9f98b0e20cc75274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1d6fc6b716e64b719cd5accc444cb655",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 798k/798k [00:00&lt;00:00, 995kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c1eef5dc7184fb1bf8ed248b828df08"
          }
        },
        "e2a522ae6b8447f7a5d8a02e9a7d31d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b8ac8b8affb049e38222ae57be44a524": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1d6fc6b716e64b719cd5accc444cb655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c1eef5dc7184fb1bf8ed248b828df08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d70a78f384a4e5b9249f63567987ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_89a74e752354461c89c9d22514744626",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4acc9247c1be4eb6a3fbc662973e4956",
              "IPY_MODEL_b493ea3b2a5c4a50b4d1d907380b9c12"
            ]
          }
        },
        "89a74e752354461c89c9d22514744626": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4acc9247c1be4eb6a3fbc662973e4956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bf3881f732064e74a449696c31ba70c4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1382015,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1382015,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_16c460f8a4b84e20bca66b3551cc0c7a"
          }
        },
        "b493ea3b2a5c4a50b4d1d907380b9c12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0303512240364a00a52460b4f5b691f1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.38M/1.38M [00:14&lt;00:00, 94.7kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_232e7fa1015e41cb91909cee4b32e71b"
          }
        },
        "bf3881f732064e74a449696c31ba70c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "16c460f8a4b84e20bca66b3551cc0c7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0303512240364a00a52460b4f5b691f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "232e7fa1015e41cb91909cee4b32e71b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annalisad98/MLDL2021/blob/main/MLDLproject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFD1LrTBwJA9"
      },
      "source": [
        "**PROJECT: NEURAL TEXT GENERATOR**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5n2KJCrwSo5"
      },
      "source": [
        "bert-babble script (colab) + evaluation (github)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbWONWLXhl0g"
      },
      "source": [
        "# **INTRODUCTORY PART**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiAUSbdcwbCS",
        "outputId": "1ccd2de9-04ed-4d57-d529-4904f2d80181"
      },
      "source": [
        "!pip3 install pytorch_pretrained_bert"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.17.110)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.110 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.20.110)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.4.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.5.30)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.110->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.110->boto3->pytorch_pretrained_bert) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvYpfbC2x50g"
      },
      "source": [
        "With this command the PyTorch pretrained bert package is installed. It contains many classes related to the BERT model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Luf5l9PIxIbU"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXjdMzc4KkVh"
      },
      "source": [
        "#help(BertForMaskedLM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDFlbwO6xgwT"
      },
      "source": [
        "BertTokenizer: perform end-to-end tokenization, i.e. basic tokenization followed by WordPiece tokenization.\n",
        "\n",
        "BertModel: raw BERT Transformer model (fully pre-trained).\n",
        "\n",
        "BertForMaskedLM: BERT Transformer with the pre-trained masked language modeling head on top (fully pre-trained)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TI8IpRkzegj"
      },
      "source": [
        "# Load pre-trained model (weights)\n",
        "model_version = 'bert-base-uncased' #'bert-large-uncased'\n",
        "model = BertForMaskedLM.from_pretrained(model_version)\n",
        "model.eval()\n",
        "cuda = torch.cuda.is_available()\n",
        "if cuda:\n",
        "    model = model.cuda()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48Nw6BSnpoUl",
        "outputId": "aa4d00ad-7ef8-4b8f-ba04-4518dcbbbcb8"
      },
      "source": [
        "type(model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pytorch_pretrained_bert.modeling.BertForMaskedLM"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxQuv4Gigpz-",
        "outputId": "a23e099b-320a-4b84-d421-91b42a53f11c"
      },
      "source": [
        "cuda"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuOM_3cKzfLz"
      },
      "source": [
        "from_pretrained: let you instantiate a model/configuration/tokenizer from a pretrained version (with the above command the pre-trained model 'bert-base-uncased' is installed).\n",
        "\n",
        "The line model.eval() is used to set the model in evaluation mode to deactivate the DropOut modules. It is IMPORTANT to have reproducible results during evaluation.\n",
        "\n",
        "With the last 3 lines of code we move our tensor to the GPU if available. Remember that PyTorch exploits GPU's power which has an increased level of parallelism w.r.t CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlOIpHYKKgHH"
      },
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEZBx8S-TaG5",
        "outputId": "dc086fb3-f6ba-4295-fb30-368a3eabeb94"
      },
      "source": [
        "print(tokenizer)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<pytorch_pretrained_bert.tokenization.BertTokenizer object at 0x7f3acf131d90>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPrqV0-XKhMy"
      },
      "source": [
        "tokenizer is used for the tokenization of sentences/batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrIu5TrCKzUo"
      },
      "source": [
        "def tokenize_batch(batch):\n",
        "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzJ9DDaNK0Oy"
      },
      "source": [
        "The method convert_tokens_to_ids converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the vocabulary.\n",
        "The function defined above gives the possibility to tokenize batches of strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5C2649hMgqo"
      },
      "source": [
        "def untokenize_batch(batch):\n",
        "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sOd77e6MkPD"
      },
      "source": [
        "The method convert_ids_to_tokens converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and added tokens.\n",
        "The function defined above gives the possibility to untokenize batches of strings.\n",
        "\n",
        "Ids stays for indeces and token stays for word objects (words, points, ...)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYC1n1Zsyuwe"
      },
      "source": [
        "def detokenize(sent):\n",
        "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
        "    new_sent = []\n",
        "    for i, tok in enumerate(sent):\n",
        "        if tok.startswith(\"##\"):\n",
        "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
        "        else:\n",
        "            new_sent.append(tok)\n",
        "    return new_sent\n",
        "\n",
        "CLS = '[CLS]'\n",
        "SEP = '[SEP]'\n",
        "MASK = '[MASK]'\n",
        "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
        "sep_id = tokenizer.convert_tokens_to_ids([SEP])[0]\n",
        "cls_id = tokenizer.convert_tokens_to_ids([CLS])[0]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv9oC-2WOM93"
      },
      "source": [
        "new_sent is at the beginning an empty list. Using the for cycle it is populated.\n",
        "\n",
        "At each step of the for cycle a new word is added to the list new_sent. If some tokens start with \"##\", it means they are part of a bigger word (without spaces), so we concatenate them.\n",
        "\n",
        "Remember BERT learns by pretraining on 2 supervised tasks simultaneously: Masked Language Model and Next Sentence Prediction.\n",
        "\n",
        "The Masked LM task is implemented by masking 15% of the words randomly in every sentence and training the model to predict them. This is why we introduced above an index for the token [MASK].\n",
        "\n",
        "For the next sentence task the goal is to understand if a generic sentence is after another sentence and to do this we need to specify the beginning of the sample (with [CLS]) and we need a special separator token ([SEP]) for example to separate questions/answers.\n",
        "__________\n",
        "__________\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwKrTTJ4i_6K"
      },
      "source": [
        "___________\n",
        "___________\n",
        "# **GENERATION PART (BERT)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaE46qfcjlGv"
      },
      "source": [
        "## GENERATION: Functions for generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKwpHV-Ox4j0"
      },
      "source": [
        "It follows the generation part, with all its important connected functions. The main generate function is GENERATE.\n",
        "It permits to generate sentences by applying one possible modality out of 3:\n",
        "\n",
        "    - parallel_sequential_generation\n",
        "\n",
        "    - sequential_generation\n",
        "    \n",
        "    - parallel_generation\n",
        "\n",
        "There are then some minor functions:\n",
        "\n",
        "    - generate_step\n",
        "\n",
        "    - get_init_text\n",
        "\n",
        "    - printer (and with Github also read_sents and write_sents)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7pRTruLONrR"
      },
      "source": [
        "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=True):\n",
        "    \"\"\" Generate a word from out[gen_idx]\n",
        "    \n",
        "    args:\n",
        "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
        "        - gen_idx (int): location for which to generate for\n",
        "        - top_k (int): if >0, only sample from the top k most probable words\n",
        "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
        "    \"\"\"\n",
        "    logits = out[:, gen_idx] \n",
        "    # array of dimension batch_size e vocabulary_size.\n",
        "    # this is a multidim array (matrix)\n",
        "    if temperature is not None:\n",
        "        logits = logits / temperature\n",
        "        # temperature is used to squeeze the matrix logits of the tensor out.\n",
        "        # smoothing parameter for the next word distribution. \n",
        "        # Higher means more like uniform; lower means more peaky.\n",
        "        # Closer to 1 means a more uniform distribution.\n",
        "    if top_k > 0:# in this case we sample from the top k most probable words.\n",
        "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
        "        # returns the k biggest entries of the input.\n",
        "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
        "        # The distributions package contains parameterizable\n",
        "        # probability distributions and sampling functions.\n",
        "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
        "    elif sample:# in this case we sample from all the distribution.\n",
        "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
        "        # The distributions package contains parameterizable\n",
        "        # probability distributions and sampling functions.\n",
        "        idx = dist.sample().squeeze(-1)\n",
        "    else:\n",
        "        idx = torch.argmax(logits, dim=-1)\n",
        "    return idx.tolist() if return_list else idx\n",
        "  "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCrNCkf-m9-C"
      },
      "source": [
        "The function generate_step is applied to generate a word.\n",
        "\n",
        "The function generate_step above returns a list of indeces if specified in the input parameter return_list, otherwise returns idx. These indeces define words (see Generate function part).\n",
        "\n",
        "First of all the object \"logits\" is created, then it squeezed in case of a prespecified value of temperature (in order to have a more or uniform distribution or less).\n",
        "\n",
        "Then there is an if-elif-else block that is used to sample a word from the distribution of logits. If we prespecify that we want to sample from the set of most probable words, then the distribution will be built over them and then sampling is applied (using .sample()). In case we want to consider the full distribution we specify just all the logits multiarray when building the distribution. Finally in case of neither full nor subset distribution, the default sampling refers to the argmax of logits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOsVsVQhm-7E"
      },
      "source": [
        "def get_init_text(seed_text, max_len, batch_size = 1, rand_init=False):\n",
        "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
        "    # builds a text by adding to seed_text a sequence (of length max_len) \n",
        "    # of either masks or random words.\n",
        "    # Recall that seed_text is used as a sort of pointer from which we start\n",
        "    # adding masked or random words to generate the initialized batch (in the\n",
        "    # BERT setting the best seed is [CLS], as we can see in the GENERATE\n",
        "    # function part).\n",
        "    # max_len = length of sequence to add to seed_text.\n",
        "    batch = [seed_text + [MASK] * max_len + [SEP] for _ in range(batch_size)]\n",
        "    # we are applying this operation a number of time equal to the size of batch\n",
        "    # (batch_size is the size of the batch).\n",
        "    \n",
        "    # before giving an output, the tokenization is applied.\n",
        "    return tokenize_batch(batch)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4lVmzoMAAOX"
      },
      "source": [
        "The function get_init_text generates a tokenized text of length max_len (which will be used as initial text in the more general generate function) starting from a seed_text and completing it with masks or random words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XnQxD2AA4SO"
      },
      "source": [
        "def printer(sent, should_detokenize=True):\n",
        "    if should_detokenize:\n",
        "        sent = detokenize(sent)[1:-1]\n",
        "    print(\" \".join(sent))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_veT-q4B4QR"
      },
      "source": [
        "The function printer prints a sentences given as input (if specified, it is first detokenized)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHcdXrBDIOvq"
      },
      "source": [
        "THE FOLLOWING CODE SECTION IS FROM GITHUB: The following part comes from GITHUB (the file bert-babble). With respect to the Colab demo, two more \"print\" function are given: read_sents, write_sents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1plhG72IGrk"
      },
      "source": [
        "# Utility functions\n",
        "    \n",
        "def read_sents(in_file, should_detokenize=False):\n",
        "  # reads content from the in_file.\n",
        "    sents = [sent.strip().split() for sent in open(in_file).readlines()]\n",
        "    if should_detokenize:\n",
        "        sents = [detokenize(sent) for sent in sents]\n",
        "    return sents\n",
        "\n",
        "def write_sents(out_file, sents, should_detokenize=False):\n",
        "  # writes inside the out_file.\n",
        "    with open(out_file, \"w\") as out_fh:         \n",
        "        for sent in sents:\n",
        "            sent = detokenize(sent[1:-1]) if should_detokenize else sent\n",
        "            out_fh.write(\"%s\\n\" % \" \".join(sent))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1DdZO9GzOE5"
      },
      "source": [
        "The function read_sents is used to read sentences from an external file, named in_file. If we should dekotenize them, we enter the if construction. After reading them, they are returned.\n",
        "\n",
        "The function write_sents is used to write the (generated) sentences inside an external file, named out_file. If we should detokenize, the previously defined detokenize function is applied. There's no return here, the function just writes in the out_file.\n",
        "\n",
        "_____________\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCp5r0fSOOO6"
      },
      "source": [
        "For the following part of code observe this: this is the meat of the algorithm. The general idea is\n",
        "\n",
        "      1- start from all masks\n",
        "      2- repeatedly pick a location, mask the token at that location, and generate from the probability distribution given by BERT\n",
        "      3- stop when converged or tired of waiting\n",
        "We consider three \"modes\" of generating:\n",
        "\n",
        "      . generate a single token for a position chosen uniformly at random for a chosen number of time steps(** PARALLEL SEQUENTIAL GENERATION**)\n",
        "      . generate in sequential order (Left->Right), one token at a time(**SEQUENTIAL GENERATION**)\n",
        "      . generate for all positions at once for a chosen number of time steps (**PARALLEL GENERATION**)\n",
        "The generate function wraps and batches these three generation modes. In practice, we find that the first leads to the most fluent samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd1aF8rw0i2E"
      },
      "source": [
        "# Generation modes as functions\n",
        "import math\n",
        "import time\n",
        "# the time package above is used to measure the time required for\n",
        "# generating an entire sentence.\n",
        "\n",
        "def parallel_sequential_generation(seed_text, batch_size=10, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
        "                                   cuda=False, print_every=10, verbose=True):\n",
        "    \"\"\" Generate for one random position at a timestep\n",
        "    \n",
        "    args:\n",
        "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
        "    \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    # These first 2 lines are the same both in parallell_sequential_generation,\n",
        "    # parallel_generation and sequential_generation. THIS IS IMPORTANT.\n",
        "    \n",
        "    for ii in range(max_iter):\n",
        "        kk = np.random.randint(0, max_len)\n",
        "        #choose a random position from 0 to maximal length where a word will be added\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = mask_id\n",
        "            # think jj as an index that moves over the rows.\n",
        "            # mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
        "            # in every batch sentence change the word in position [seed_len+kk] into a mask(?)\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        # using the above line, the inp object is transformed into a tensor,\n",
        "        # using GPU if prespecified.\n",
        "        out = model(inp)  # the pretrained model BertForMaskedLM is applied (see first\n",
        "        # code block).\n",
        "        topk = top_k if (ii >= burnin) else 0\n",
        "        # top_k : at each step, sample from the top_k most likely words \n",
        "        # but only if the iteration we're in is >= the burn-in, else topk=0\n",
        "        idxs = generate_step(out, gen_idx=seed_len+kk, top_k=topk, temperature=temperature, sample=(ii < burnin))\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+kk] = idxs[jj]\n",
        "            # think jj as an index that moves over the rows.\n",
        "            # think seed_len+kk as an index that moves over the columns.\n",
        "            # in this sense idxs is a sort of column vector whose entries are\n",
        "            # specified using the indeces jj.\n",
        "            # jj indicates the row (the sentence) inside the batch.\n",
        "            # seed_len+kk stays for the token inside the specified sentence.\n",
        "            # Remember that batch is a list of vectors, where each vector\n",
        "            # has entries corresponding to indeces for words inside \n",
        "            # the tokenizer (vocabulary).\n",
        "            \n",
        "        if verbose and np.mod(ii+1, print_every) == 0:\n",
        "            # if verbose is true and ii % print_every = 0, so ii = α * print_every\n",
        "            # we print an output message.\n",
        "            for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
        "            # batch[0] corresponds to the first vector of the list. Through the\n",
        "            # application of the tokenizer, this is exactly a sentence, the first\n",
        "            # sentence of the batch we are working on.\n",
        "            # remember that convert_ids_to_tokens converts indeces (numeric values)\n",
        "            # to tokens (that may be words) through the tokenizer (which is \n",
        "            # substantially a vocabulary).\n",
        "\n",
        "            for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
        "            # idea: use the + as concatenation and show with the \"(*)\" where we\n",
        "            # have sampled the kk in this last external for (ii) cycle.\n",
        "            # In this way, we know that the token we see before the \"(*)\" is the\n",
        "            # one we have just updated.\n",
        "\n",
        "            print(\"iter\", ii+1, \" \".join(for_print))     \n",
        "            # we could think this if and print as a command to show the user\n",
        "            # that the process is going on, the iterations are moving, and \n",
        "            # we show the first sentence of the batch to illustrate\n",
        "            # how the generated sentence is changing, how the process is\n",
        "            # modifying iteratively our first sentence of the batch.        \n",
        "    return untokenize_batch(batch)\n",
        "\n",
        "def parallel_generation(seed_text, batch_size=10, max_len=15, top_k=0, temperature=None, max_iter=300, sample=True, \n",
        "                        cuda=False, print_every=10, verbose=True):\n",
        "    \"\"\" Generate for all positions at a time step \"\"\"\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "    # These first 2 lines are the same both in parallel_sequential_generation,\n",
        "    # parallel_generation and sequential_generation. THIS IS IMPORTANT.\n",
        "    \n",
        "    for ii in range(max_iter):\n",
        "        # w.r.t the sequential_generation function, since now we generate all\n",
        "        # words at a time, we don't need the command inp = [sent[:seed_len+ii+leed_out_len]+[sep_id] for sent in batch]\n",
        "        \n",
        "        # so while in sequential_generation the generation process goes from\n",
        "        # the beginning of the batch up to the end, now there is\n",
        "        # a for loop with a max_iter number of iterations.\n",
        "        # For each iteration the process below is applied (NOTE that the\n",
        "        # value ii is not used in the generation process below. The only\n",
        "        # moment we use it is when we want to print a message to show the \n",
        "        # iteration the algorithm is workin on).\n",
        "\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        # using the above line, the inp object is transformed into a tensor,\n",
        "        # using GPU if prespecified.\n",
        "        out = model(inp) # the pretrained model BertForMaskedLM is applied (see first\n",
        "        # code block).\n",
        "        for kk in range(max_len):\n",
        "            idxs = generate_step(out, gen_idx=seed_len+kk, top_k=top_k, temperature=temperature, sample=sample)\n",
        "            for jj in range(batch_size):\n",
        "                batch[jj][seed_len+kk] = idxs[jj]\n",
        "                # think jj as an index that moves over the rows.\n",
        "                # think seed_len+kk as an index that moves over the columns.\n",
        "                # in this sense idxs is a sort of column vector whose entries are\n",
        "                # specified using the indeces jj.\n",
        "                # jj indicates the row (the sentence) inside the batch.\n",
        "                # seed_len+kk stays for the token inside the specified sentence.\n",
        "                # Remember that batch is a list of vectors, where each vector\n",
        "                # has entries corresponding to indeces for words inside \n",
        "                # the tokenizer (vocabulary).\n",
        "            \n",
        "        if verbose and np.mod(ii, print_every) == 0:\n",
        "            # if verbose is true and ii % print_every = 0, so ii = α * print_every\n",
        "            # we print an output message.\n",
        "            print(\"iter\", ii+1, \" \".join(tokenizer.convert_ids_to_tokens(batch[0])))\n",
        "            # batch[0] corresponds to the first vector of the list. Through the\n",
        "            # application of the tokenizer, this is exactly a sentence, the first\n",
        "            # sentence of the batch we are working on.\n",
        "\n",
        "            # we could think this if and print as a command to show the user\n",
        "            # that the process is going on, the iterations are moving, and \n",
        "            # we show the first sentence of the batch to illustrate\n",
        "            # how the generated sentence is changing, how the process is\n",
        "            # modifying iteratively our first sentence of the batch.\n",
        "\n",
        "            # remember that convert_ids_to_tokens converts indeces (numeric values)\n",
        "            # to tokens (that may be words) through the tokenizer (which is \n",
        "            # substantially a vocabulary).\n",
        "    \n",
        "    return untokenize_batch(batch)\n",
        "            \n",
        "def sequential_generation(seed_text, batch_size=10, max_len=15, leed_out_len=15, \n",
        "                          top_k=0, temperature=None, sample=True, cuda=False):\n",
        "    \"\"\" Generate one word at a time, in L->R order \"\"\"# from left to right.\n",
        "    # This function is called inside the GENERATION function (which is the main\n",
        "    # generation function to call), to generate a batch of words.\n",
        "    seed_len = len(seed_text)\n",
        "    batch = get_init_text(seed_text, max_len, batch_size)\n",
        "\n",
        "    # Recall that with get_init_text we build a text by adding to seed_text a \n",
        "    # sequence (of length max_len) of either masks or random words.\n",
        "    # with get_init_text we initialize the batch, then through this function\n",
        "    # it is updated.\n",
        "    # max_len = length of sequence to add to seed_text (we can consider roughly\n",
        "    # as the maximal length of each sentence composing the text).\n",
        "    # Recall seed_text is the prefix to generate for (it was found crucial to \n",
        "    # start with the CLS token). It is somehow the prefix to add when generating\n",
        "    # sentences, it stays for the beginning of a sentence.\n",
        "    # batch_size is the size of the batch.\n",
        "\n",
        "    # so the object batch will contain our text (a structure containing sentences).\n",
        "    \n",
        "    for ii in range(max_len):\n",
        "        inp = [sent[:seed_len+ii+leed_out_len]+[sep_id] for sent in batch]\n",
        "        # the above command says that for each sentences present in the batch\n",
        "        # we save a growing portion of the sentence inside the object inp\n",
        "        # (we say growing portion because the outer for cycle cycles over ii, that\n",
        "        # is used as an index for sent).\n",
        "        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
        "        # using the above line, the inp object is transformed into a tensor,\n",
        "        # using GPU if prespecified.\n",
        "        out = model(inp) # the pretrained model BertForMaskedLM is applied (see first\n",
        "        # code block).\n",
        "        idxs = generate_step(out, gen_idx=seed_len+ii, top_k=top_k, temperature=temperature, sample=sample)\n",
        "        # recall from some previous code blocks that the GENERATE_STEP IS APPLIED\n",
        "        # TO GENERATE A WORD (in this case idxs is a column vector).\n",
        "        for jj in range(batch_size):\n",
        "            batch[jj][seed_len+ii] = idxs[jj]\n",
        "            # think jj as an index that moves over the rows.\n",
        "            # think seed_len+ii as an index that moves over the columns.\n",
        "            # in this sense idxs is a sort of column vector whose entries are\n",
        "            # specified using the indeces jj.\n",
        "            # jj indicates the row (the sentence) inside the batch.\n",
        "            # seed_len+kk stays for the token inside the specified sentence.\n",
        "            # Remember that batch is a list of vectors, where each vector\n",
        "            # has entries corresponding to indeces for words inside \n",
        "            # the tokenizer (vocabulary).\n",
        "    return untokenize_batch(batch)\n",
        "\n",
        "\n",
        "def generate(n_samples, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
        "             generation_mode=\"parallel-sequential\",\n",
        "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
        "             cuda=False, print_every=1):\n",
        "    # main generation function to call\n",
        "\n",
        "    # n_samples = number of samples.\n",
        "    # math.ceil is used to round a number upward to its nearest integer, in\n",
        "    # this case it is applied to define the number of batches, n_batches.\n",
        "    # sentences is the list of generated words.\n",
        "    # print_every is just used to specify after how many batches generations to\n",
        "    # output the time required.\n",
        "    # seed_text stays for the first token of every sequence (in this\n",
        "    # case it is the [CLS] token)\n",
        "    sentences = []\n",
        "    n_batches = math.ceil(n_samples / batch_size)\n",
        "    start_time = time.time()\n",
        "    # for each batch, depending on the generation_mode, a specific generation\n",
        "    # function is applied (parallel_sequential_generation, or sequential_generation, \n",
        "    # or parallel_generation).\n",
        "    # the final \"if\" checks \n",
        "    # at the end of each for iteration the generated batch is added to sentences.\n",
        "    for batch_n in range(n_batches):\n",
        "        # ma\n",
        "        if generation_mode == \"parallel-sequential\":\n",
        "            batch = parallel_sequential_generation(seed_text, batch_size=batch_size, max_len=max_len, top_k=top_k,\n",
        "                                                   temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
        "                                                   cuda=cuda, verbose=False)\n",
        "        elif generation_mode == \"sequential\":\n",
        "            batch = sequential_generation(seed_text, batch_size=batch_size, max_len=max_len, top_k=top_k, \n",
        "                                          temperature=temperature, leed_out_len=leed_out_len, sample=sample,\n",
        "                                          cuda=cuda)\n",
        "        elif generation_mode == \"parallel\":\n",
        "            batch = parallel_generation(seed_text, batch_size=batch_size,\n",
        "                                        max_len=max_len, top_k=top_k, temperature=temperature, \n",
        "                                        sample=sample, max_iter=max_iter, \n",
        "                                        cuda=cuda, verbose=False)\n",
        "        \n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "          # if a number of batches equal to \"print_every\" has been\n",
        "          # generated, then an output message is shown giving the time required.\n",
        "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "        \n",
        "        sentences += batch\n",
        "    return sentences"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "859dnAFa4LKN"
      },
      "source": [
        "The piece of code above contains some smaller generation functions (parallel_sequential_generation, or sequential_generation, or parallel_generation) and one bigger, more general, for generating sentences.\n",
        "\n",
        "For each batch one of the 3 above generation methods is chosen.\n",
        "\n",
        "\n",
        "Sequential_generation. We use this function to generate a batch of sentences (that will be added to the final set of sentences). A sort of initialized text is generated and it is used for the process of generation: consider a generic sentence of the initialized batch, a growing portion of it is taken (inp) and given as input to the pretrained model BertForMaskedLM, the output (out) is used as a parameter to the generate_step function to generate a word. Intuitively we use a growing portion of the initialized batch to write sentences with some sense.\n",
        "\n",
        "Parallel_generation. While in sequential_generation the generation process goes from the beginning of the batch up to the end, now there is a for loop with a max_iter number of iterations.\n",
        "For each iteration the process, similar to the one for the sequential_generation, is applied (NOTE that the value ii is not used in the generation process below. The only moment we use it is when we want to print a message to show the iteration the algorithm is workin on).\n",
        "\n",
        "Parallel_sequential_generation. This function generates a single token for a position chosen uniformly at random for a chosen number of time steps. As in sequential_generation, we fill in the batch one idx (token) at a time but the position of this idx in the batch's vectors (sentences) is sampled uniformly at random instead of using the iteration number as in sequential_generation. Here we give the full batch as input (inp) to the pretrained model BertForMaskedLM.\n",
        "At each step, the model (generate_step function) samples the words from the top_k most likely words, but if the iteration we're in is <= the burn-in, it doesn't do that anymore. \n",
        "\n",
        "_____\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBprgGGA0j3_"
      },
      "source": [
        "## APPLICATION OF THE GENERATION FUNCTION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBQemYcbiMkF"
      },
      "source": [
        "### Application: Example 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZzX_Gb_kVMU"
      },
      "source": [
        "Let's call the actual generation function! We'll use the following settings.\n",
        "\n",
        "max_len (40): length of sequence to generate\n",
        "\n",
        "top_k (100): at each step, sample from the top_k most likely words\n",
        "\n",
        "temperature (1.0): smoothing parameter for the next word distribution. Higher means more like uniform; lower means more peaky\n",
        "\n",
        "burnin (250): for non-sequential generation, for the first burnin steps, sample from the entire next word distribution, instead of top_k\n",
        "\n",
        "max_iter (500): number of iterations to run for\n",
        "\n",
        "seed_text ([\"CLS\"]): prefix to generate for. We found it crucial to start with the CLS token; you can try adding to it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8abGGm7e0oSl",
        "outputId": "e5b9f63b-4e29-42c4-cab5-137cb77b4245"
      },
      "source": [
        "# example from colab\n",
        "\n",
        "n_samples = 5\n",
        "batch_size = 5\n",
        "max_len = 40\n",
        "top_k = 100\n",
        "temperature = 1.0\n",
        "generation_mode = \"parallel-sequential\"\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "\n",
        "# Choose the prefix context\n",
        "seed_text = \"[CLS]\".split()\n",
        "bert_sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
        "                      generation_mode=generation_mode, sample=sample, top_k=top_k, temperature=temperature, burnin=burnin, max_iter=max_iter,\n",
        "                      cuda=cuda)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finished batch 1 in 12.077s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAg0NU1q0tFJ",
        "outputId": "df02cad3-bdf2-4d25-b2ba-dc12c9faad0f"
      },
      "source": [
        "for sent in bert_sents:\n",
        "  printer(sent, should_detokenize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "he fell in the 1983 election - repeating methods used in later elections - but won over former manchester patriotic and debating society member richard johnston , subsequently being included in the recent inquiry from the manchester city council .\n",
            "but most importantly , the materials are issued annually . so what ish ? there are so many versions of what they were written . there are so many contradictions , so many errors , many errors .\n",
            "the current live version ; 1987 ) at capes , doin ' me part 3 ( new studio version ) * \" it works , the magic way \" ( live ) ( live version ; 1987 ) .\n",
            "( where at the right time ? ) strange tales ( a strange tales version ) heart of gold nightside star in the west the love of magdy is available as both cd 1 and cd 2 .\n",
            "the two had become friends ; they knew him , as jules arbert and also jean - baptiste - arbert . the marriage ended in divorce , and pierrot \" pere \" took the leading role .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3q7Pd0uijZq"
      },
      "source": [
        "### Application: Example 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GM_g7avojS8"
      },
      "source": [
        "THE FOLLOWING 3 CODE SECTIONS IS FROM GITHUB: The following section of code shows an example from GITHUB where text is generated and written using the functions read_sents and write_sents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2A_ipOfKdqA"
      },
      "source": [
        "n_samples = 1000 #1000\n",
        "batch_size = 50 #50\n",
        "max_len = 40\n",
        "top_k = 100\n",
        "temperature = 0.7\n",
        "\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "\n",
        "# Choose the prefix context\n",
        "seed_text = \"[CLS]\".split()\n",
        "\n",
        "for temp in [1.0]:\n",
        "    bert_sents = generate(n_samples, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
        "                          sample=sample, top_k=top_k, temperature=temp, burnin=burnin, max_iter=max_iter,\n",
        "                          cuda=True)\n",
        "    out_file = \"Bert_using_pytorch.txt\"\n",
        "    write_sents(out_file, bert_sents, should_detokenize=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgUFStfvjMYB"
      },
      "source": [
        "write_sents(out_file, bert_sents, should_detokenize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KitCB-nrLlmo"
      },
      "source": [
        "in_file = \"Bert_using_pytorch.txt\"\n",
        "bert_sents = read_sents(in_file, should_detokenize=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDPTMyMwMBtU",
        "outputId": "b7f2ac7f-b70b-4012-af6f-b6d993f5e08e"
      },
      "source": [
        "for i in range(50):\n",
        "    printer(bert_sents[i], should_detokenize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it is true that the world is not just governed by one force , but by some other force , then you will select where the events begin and when they end , that is what you say\n",
            "in britain , temp is fondly remembered for this , because just a month later several of his men had been gang - raped , while the two men living close to the camp were tortured\n",
            "they were real ugly . ) . . our beckoned . bambi , clearly ugly as well , had been believed to be a wanker for this character because of her matted dark hair\n",
            "king , professional boxer . stephen jones , jon jones , john jones ( the waste had been wiped clean , cobbled up into garbage ) . brian jones , john jones ( man in black )\n",
            "hurry \" - said one of the women . \" alright , alright , alright \" - nobody could hurt her . bearl could not . the dead woman and lissy were at least twenty\n",
            "by ( author and illustrator ) e . mccready , univ . and philosophical research institute , n . y . c . [ revised and enlarged ] philadelphia : w . a . t\n",
            "see list below ) samuel richard smith : the road was named after him at credit . ansel a mitchell : the seat is currently represented mp by hon . kathleen mitchell ( an federal deputy )\n",
            "er ! ' answered the secretary . ' did you see her ? ' ' she had just been here ten minutes . ' while james thought it was maiwenn , he almost recognized her instantly\n",
            "glen as david duncan luca amoretti as professor douglas duncan lynn redgrave as ellie duncan bois - du - bois as jim myre lorna keefe as genevieve joubert | | |\n",
            "cast included anthony in \" the fabulous reynolds brothers \" and the other black guy in \" the ghola \" , to write for new york studios and role corrina in \" what ?\n",
            "the car was not exactly cold either . \" \" so were we . \" a dissociative murmur . \" this is ms . collins . \" 3 the car smelled of sizzling rubber\n",
            "... ] - as planned , the shop goes . - inside the shop - guy and people climb over the crib to buy more food . - outside the shop guri stays in the flat\n",
            "columbia river course . \" \" we . . . . removed the rafting course , and we . . . . . . widened it so that the source of moon lake was located . .\n",
            ", and dance , children , and dance ! she was dressed like a young woman in fashionable clothes and beautifully lit , her arms clasped , legs crossed under bonnier - wood shoes and closed lips\n",
            "- present . burbach and his wife melinda philco continue touring in central , eastern and northern california , particularly wachikoma and across the silverton valley and muskokite\n",
            "word concept however has a certain subliterature both in french and in french or arabic / persian . al - qadri , sabait ( ed . ) , ( 2010 )\n",
            "2007 ) 2054 produced a ( \" talking \" ) book , the atomic bombs : leaders of the world . ( 2007 ) 2054 published in at least a dozen countries from west to east\n",
            "from 23 february early on between grandstands started flashing ' the double lever ' . the ' triple lever ' was then called a lever which many teams had used to call their upfield shots\n",
            "thing that has grown more somehow , as a whole ... these are not the things we read . these are not ordinary things created by birth or child , but a thing that grows as it grows\n",
            "had all road track races professionally in asia pacific ( daisad ( now defunct ) ) . in may 2006 , he took out the zkm - adac - the spanish professional mountain championship\n",
            "* * 42 . the violets . the dark . the darkness . only us . only us . only me and her . * * * 48 . the dream . another time ... another nightmare\n",
            "letter told he had two sons , his weight more but not less , and toyed with it - which i am sure he never knew , as the two scraps were ripped from his hands\n",
            "drives an electrical / industrial truck . in 2013 , a solo exhibition of hers was at design ( sculpture ) , an art space in the szczerzprzystalskie art district\n",
            "each visited seeley robertson , who , himself , was \" in booneville \" , and sam gilman , commander of the meo and a kia . robertson later was \" in lexington \"\n",
            "the \" starling \" , an incredibly happy era comes for survivors . they look for their lost grandfather , who was almost killed by superman . it is also the final film of the film trilogy\n",
            "no thanks . \" he hit a body and mitch ' s foot nearly hit the floor . he turned back to where mitch was standing . walk away from me . walk away from that bitch again\n",
            "during its progress , with the help of his wife , harriet rowley - bennett , john g . macintyre occupied government buildings at fifty - third & third streets in nashville ( 1871 )\n",
            "include \" enough time to go \" to enter bbc radio 2 , \" a young married man \" ( four minor uk singles along with rick leach and robert skinner ) \" the first good look \"\n",
            "founder and first president was john reid ( chairman ) . staff in 1895 included reid ( later appointed as director ) , b . white ( secretary ) , and george frederick thorp ( treasurer )\n",
            "activities ( instruments ) might also have included large horses , furniture , horses and boats ( seized by the irish volunteers in 1649 ) ; notes about chemical substances ( water , earth , wine ) obtained\n",
            "takes jimmy out for a walk to the local fishing lake , watching him lose control of his rackets . later that night most of the guys that took it all arrive and get back them up\n",
            "down inside , he had been cursed . he was the keeper of dead hearts ; between life and death . acheron stood up as he remembered that last message . it was 2 : 34 : 57\n",
            "- fu tsui reunited in may / june 2010 with justin liming , ang chi - li and jimmy yang . the live album featured singer - songwriter justin liming produced by james howarth\n",
            "mr . robert ross , these are your co - authors whose books are the best ever published ( and have a professional writing staff that can tell you how many more than books are ever published )\n",
            "the areas of popular interest are nash city , home to the nash hospital and nash town , a rallying point for new - age rock art , promoted as the first concert in the nash square\n",
            "this little bird named brock , who would have jumped at him just now . thank god , he would have lived . and maybe like brock , jack had seen it . both himself and will had\n",
            "won the region of origin of australia award for the 2006 - 2007 editions , and the prestigious \" the english - australian \" award in 2014 ( ( adelaide : 171 , melbourne : 105 ) )\n",
            "the right wall were cameras . they were full of glitter , hot gobs , purple stickers , pet lights , even as much as possible to film antique cars in the nearest drive - by\n",
            "is no formal management structure such as master management nor is any of these facilities relevant to the national security advisor general per directive 46 / 14 while approvals listed here are dse or dpp\n",
            "seed and night : twenty five minutes , before dimming lights ... and before respiration ... or worse : forty five minutes , allowing the bleeding to stop . blood dripped from nine tiny red veins\n",
            "took the first step down the hall and came up to the front landing , for some fresh air . the house was exceptionally quiet , with closed front doors , for no more than about five stories\n",
            "i met you : the story of the man i never met is a special 4 issue limited series published by hyperion books that came out in 1997 ( starting with shadows of tomorrow # 14 )\n",
            "want me to change the subject ? \" silence , silence was enough . \" what the fuck ? \" logan said . hours later and hours later was the sudden arrival of my best friend , logan\n",
            "was neither played , in principle , by the sidelight of summer , nor even at the end of autumn , and it did not bring him success , until it was taken over by herbert read\n",
            "st john , michael st . john and joseph cockey were among the remigians and were the founding monks while still in monastic life . thomas digby later became justice of the peace\n",
            ", known simply as louise , wrote the screenplay for the broadway musicals lili , ruby , gypsy , blow - out and she wrote , the red carpet , the agassiz girl and boo\n",
            "day since getting a quick divorce or becoming a rich wife , all twenty or so homes had lofts being air - conditioned kinds of lofts made for the construction and the real estate business here\n",
            "yourself drawing me toward you , sparing my hand , and under my luceria . give me opportunities as thou hast before . give me something , well , that will never be dreamed of\n",
            ". rush takes the child in to the small room where there is no but says , ' please bid farewell to my family and to arachne , pandora . you truly must be very frightened\n",
            "stake - a - boo also had wheels with the player ' s main tool ( mine was up and not down ) . the wagon , which had twenty - six wooden walking wheels , was similar\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqqgDCvopM7S"
      },
      "source": [
        "_______________\n",
        "_______________\n",
        "# **GENERATION PART (OpenAI GPT)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UUfZ-ZYNjNw"
      },
      "source": [
        "**Comparing to existing models**\n",
        "\n",
        "The OpenAI Generative Pretraining Transformer is another pretrained model successfully used for transfer learning. Since the model is a unidirectional language model, we can straightforwardly generate from the model. See this repo by Thomas Wolf at Huggingface for instructions for setting up the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdjHFwwrAIEy",
        "outputId": "c9d3ba92-dc47-4e7d-e6cd-f1990bd6b673"
      },
      "source": [
        "!git clone https://github.com/huggingface/pytorch-openai-transformer-lm.git 'OpenAi'\n",
        "%cd /content/"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'OpenAi'...\n",
            "remote: Enumerating objects: 262, done.\u001b[K\n",
            "remote: Total 262 (delta 0), reused 0 (delta 0), pack-reused 262\u001b[K\n",
            "Receiving objects: 100% (262/262), 271.29 KiB | 9.35 MiB/s, done.\n",
            "Resolving deltas: 100% (156/156), done.\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjE82mwdFuWX",
        "outputId": "eb541240-5320-4a5f-fc98-4b0cac719f4a"
      },
      "source": [
        "!git clone https://github.com/openai/finetune-transformer-lm.git\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'finetune-transformer-lm'...\n",
            "remote: Enumerating objects: 38, done.\u001b[K\n",
            "remote: Total 38 (delta 0), reused 0 (delta 0), pack-reused 38\u001b[K\n",
            "Unpacking objects: 100% (38/38), done.\n",
            "Checking out files: 100% (22/22), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxPqC2FaBtbY",
        "outputId": "857056a8-e524-4a41-f13f-fb7db7007c60"
      },
      "source": [
        "!pip install ftfy\n",
        "!pip install tqdm\n",
        "!pip install sklearn\n",
        "!pip install spacy\n",
        "!pip install pandas"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 21.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 28.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 21.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 17.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 51kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 61kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41935 sha256=caad70f92ecfb27f5a9114309776dbff92beef9a6ec96a329339299c16c855b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\n",
            "Successfully built ftfy\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.0.3\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (4.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKY0cWFWN0Nd"
      },
      "source": [
        "\"\"\"Before running this block, move the \"model\" folder from \n",
        "finetune-transformer-lm to OpenAi\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.insert(1, os.path.join(\".\", \"OpenAi\"))  #pytorch-openai-transformer-lm\n",
        "\n",
        "from OpenAi.model_pytorch import LMModel, load_openai_pretrained_model, DEFAULT_CONFIG\n",
        "from OpenAi.text_utils import TextEncoder\n",
        "\n",
        "def load_openai_gpt(n_special=1, n_ctx=512):\n",
        "    text_encoder = TextEncoder(\"/content/OpenAi/model/encoder_bpe_40000.json\", \n",
        "                               \"/content/OpenAi/model/vocab_40000.bpe\")\n",
        "    encoder = text_encoder.encoder\n",
        "    n_vocab = len(text_encoder.encoder)\n",
        "    vocab = n_vocab + n_special + n_ctx\n",
        "\n",
        "    args = DEFAULT_CONFIG\n",
        "    lm_model = LMModel(args, vocab, n_ctx, return_probs=True)\n",
        "    load_openai_pretrained_model(lm_model.transformer, n_ctx=n_ctx, n_special=n_special,\n",
        "                                 path=\"/content/OpenAi/model/\",\n",
        "                                 path_names=\"/content/OpenAi/\")\n",
        "    #lm_model.to(device)\n",
        "    lm_model.return_probs = False\n",
        "    lm_model.eval()\n",
        "    return lm_model, text_encoder\n",
        "\n",
        "def make_batch(X, n_vocab, n_special, batch_size):\n",
        "    X = np.array(X)\n",
        "    assert X.ndim in [1, 2]\n",
        "    if X.ndim == 1:\n",
        "        X = np.expand_dims(X, axis=0)\n",
        "    pos_enc = np.arange(n_vocab + n_special, n_vocab + n_special + X.shape[-1])\n",
        "    pos_enc = np.tile(pos_enc, (batch_size, pos_enc.shape[-1])) #np.expand_dims(pos_enc, axis=0)\n",
        "    batch = np.stack([X, pos_enc], axis=-1)\n",
        "    batch = torch.tensor(batch, dtype=torch.long)#.to(device)\n",
        "    return batch\n",
        "\n",
        "def append_batch(X, next_idx):\n",
        "    next_pos = X[:, -1:, 1] + 1\n",
        "    next_x = torch.cat((next_idx, next_pos), -1).unsqueeze(1)\n",
        "    return torch.cat((X, next_x), 1)\n",
        "\n",
        "def _generate_sentence_openai(model, text_encoder, seed_text, batch_size=10, gen_len=20, \n",
        "                             topk=100, sample=True, n_special=0):\n",
        "    n_vocab = len(text_encoder.encoder)\n",
        "    #X = np.random.randint(n_vocab, size=(batch_size, 1)).tolist()\n",
        "    #sents = [[text_encoder.decoder[X[i][0]]].replace('</w>', '') for i in range(batch_size)]\n",
        "    X = [[n_vocab - 1] for _ in range(batch_size)]\n",
        "    sents = [[] for _ in range(batch_size)]\n",
        "    if seed_text:\n",
        "        seed_ids = text_encoder.encode([seed_text,])\n",
        "        X = [X[i] + seed_ids[0] for i in range(batch_size)]\n",
        "        sents = [[seed_text] for _ in range(batch_size)]\n",
        "    XMB = make_batch(X, n_vocab, n_special, batch_size=batch_size)\n",
        "\n",
        "\n",
        "    for step_n in range(gen_len):\n",
        "        out = model(XMB) + model.pos_emb_mask\n",
        "        next_idxs = generate_step(out, gen_idx=step_n, top_k=topk, sample=sample, return_list=False)\n",
        "        idxs = next_idxs.tolist()\n",
        "        for i in range(batch_size):\n",
        "            next_token = idxs[i]\n",
        "            if next_token == n_vocab:\n",
        "                next_token = \"<EOS>\"\n",
        "            else:\n",
        "                next_token = text_encoder.decoder[next_token].replace('</w>', '')\n",
        "            sents[i].append(next_token)\n",
        "        XMB = append_batch(XMB, next_idxs.unsqueeze(-1))\n",
        "        \n",
        "    return [[tok for tok in sent if tok != '\\n'] for sent in sents]\n",
        "\n",
        "def generate_openai(model, text_encoder, n_samples, seed_text, \n",
        "                    batch_size=10, gen_len=20, \n",
        "                    topk=100, temperature=0.7, sample=True,\n",
        "                    n_special=0, print_every=1):\n",
        "    sents = []\n",
        "    start_time = time.time()\n",
        "    n_batches = math.ceil(n_samples / batch_size)\n",
        "    for batch_n in range(n_batches):\n",
        "        batch_sents = _generate_sentence_openai(model, text_encoder, seed_text,\n",
        "                                                batch_size=batch_size, gen_len=gen_len, \n",
        "                                                topk=topk, sample=sample,\n",
        "                                                n_special=n_special)\n",
        "        sents += batch_sents\n",
        "        if (batch_n + 1) % print_every == 0:\n",
        "            print(\"Generated batch %d of %d in %.3fs\" % (batch_n + 1, n_batches, time.time() - start_time))\n",
        "            start_time = time.time()\n",
        "    return sents"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcqQicwHN8Mi",
        "outputId": "07ec7253-d998-4687-b674-1ee38cdc9896"
      },
      "source": [
        "import json\n",
        "\n",
        "gpt_model, gpt_text_encoder = load_openai_gpt(n_special=1)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading weights...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB5z3J1OAZNi",
        "outputId": "053daa38-7f88-4ed1-8be6-60c6f49bb23c"
      },
      "source": [
        "print(gpt_text_encoder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<OpenAi.text_utils.TextEncoder object at 0x7f361f6110d0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scwtp-ueN9BH"
      },
      "source": [
        "Loading weights..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrgceSdL9Mck"
      },
      "source": [
        "### Application of OpenAI GPT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYlUdyecN__R",
        "outputId": "3a881334-5c85-44fa-bf12-892071061675"
      },
      "source": [
        "n_samples = 1000\n",
        "batch_size = 50\n",
        "max_len = 40\n",
        "top_k = 100\n",
        "temperature = 0.7\n",
        "\n",
        "leed_out_len = 5 # max_len\n",
        "burnin = 250\n",
        "sample = True\n",
        "max_iter = 500\n",
        "\n",
        "openai_sents = generate_openai(gpt_model, gpt_text_encoder, seed_text=\"\", \n",
        "                               n_samples=n_samples, batch_size=batch_size, gen_len=max_len,\n",
        "                               topk=top_k, temperature=temperature, sample=sample,\n",
        "                               n_special=1, print_every=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated batch 1 of 20 in 147.889s\n",
            "Generated batch 2 of 20 in 144.145s\n",
            "Generated batch 3 of 20 in 143.557s\n",
            "Generated batch 4 of 20 in 143.098s\n",
            "Generated batch 5 of 20 in 142.696s\n",
            "Generated batch 6 of 20 in 143.003s\n",
            "Generated batch 7 of 20 in 143.652s\n",
            "Generated batch 8 of 20 in 143.495s\n",
            "Generated batch 9 of 20 in 143.918s\n",
            "Generated batch 10 of 20 in 143.643s\n",
            "Generated batch 11 of 20 in 143.896s\n",
            "Generated batch 12 of 20 in 142.684s\n",
            "Generated batch 13 of 20 in 144.480s\n",
            "Generated batch 14 of 20 in 142.908s\n",
            "Generated batch 15 of 20 in 142.632s\n",
            "Generated batch 16 of 20 in 142.571s\n",
            "Generated batch 17 of 20 in 142.476s\n",
            "Generated batch 18 of 20 in 143.346s\n",
            "Generated batch 19 of 20 in 144.636s\n",
            "Generated batch 20 of 20 in 143.657s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ye0xLVFVNK7"
      },
      "source": [
        "out_file = \"openaitext.txt\"\n",
        "    #out_file = \"data/%s-len%d-burnin%d-topk%d-temp%.3f.txt\" % (model_version, max_len, burnin, top_k, temp)\n",
        "write_sents(out_file, openai_sents, should_detokenize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSZBJllQOKz3",
        "outputId": "4db4485d-2d3b-472a-9cd3-777d39032b6a"
      },
      "source": [
        "printer(openai_sents[9], should_detokenize=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\" would you watch a kid play ? \" i asked . \" and then get bored ? \" he nodded . \" sure . \" \" but not if you were watching for the kids and not\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5G7x3Cr2PqR"
      },
      "source": [
        "In order to get more values related to the table shown in the paper \"BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model\", we sample 1000 sentences from the training split of the datasets WT103, and check the values of corpus bleu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_6hOpThkt5q"
      },
      "source": [
        "___________\n",
        "___________\n",
        "# **EVALUATION PART**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXJuO4AU0xA2"
      },
      "source": [
        "Evaluation methods for unconditional generation aren't perfect. We'll measure the diversity of our generated samples via _self-BLEU_: we compute _corpus BLEU_ where for each generated sentence, we compute BLEU treating the other sentences as references. \n",
        "\n",
        "We also compute the percentage of  _n-grams_ that are unique among the generations. \n",
        "\n",
        "(From Wikipedia: an n-gram is an n-elements subsequence of a sequence).\n",
        "\n",
        "We try some other strategies, including comparing to outside models, in our report, and you can see some of the code for that here (SEE SECTION TEXYGEN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0zIi0rXPXHC"
      },
      "source": [
        "THE FOLLOWING SECTION IS THE EVALUATION SECTION FROM GITHUB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oi3xfMxMzbs"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l149KV2RnF24",
        "outputId": "de66e722-9f49-409c-c832-98a1c2274a6d"
      },
      "source": [
        "!pip3 install nltk==3.6.2"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.6.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\r\u001b[K     |▎                               | 10kB 19.6MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 25.5MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 19.6MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 16.1MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 8.5MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 8.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 9.1MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 10.1MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 10.2MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 8.3MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 8.3MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 8.3MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 8.3MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 8.3MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 8.3MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194kB 8.3MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204kB 8.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 276kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 327kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 337kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 348kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 368kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 378kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 389kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 399kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 419kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 430kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 440kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 460kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 471kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 624kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 645kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 655kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 665kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 675kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 686kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 696kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 706kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 716kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 737kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 747kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 757kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 768kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 778kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 788kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 798kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 808kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 829kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 839kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 849kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 860kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 870kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 880kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 890kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 901kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 921kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 931kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 942kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 952kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 962kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 972kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 983kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 993kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.0MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.0MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1MB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.2MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3MB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3MB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3MB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.3MB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.3MB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.3MB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.3MB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.4MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4MB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4MB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.2) (4.41.1)\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QgQ0jADM-tu"
      },
      "source": [
        "from nltk.translate import bleu_score as bleu"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56LjqElSM_o-"
      },
      "source": [
        "## Quality Measures: Corpus-BLEU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuHKGTyo-JFv"
      },
      "source": [
        "We want to know how similar are the generated sentences to the original training data (Toronto Book Corpus and Wikipedia dumps). We follow Yu et al., (2017) and compute the BLEU between the generations and the test sets of both corpora by treating the test set as the references for each generation. The tests sets are large; we subsample 5000 examples from each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTfFV1F5inUm"
      },
      "source": [
        "#help(bleu.corpus_bleu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZx_5by5NWcf"
      },
      "source": [
        "def prepare_data(data_file, replacements={}, uncased=True):\n",
        "    \"\"\" Prepare data to compute the BLEU score, since we use corpus_bleu each \n",
        "        sentence has to be a list of a list of tokens\n",
        "    \"\"\"\n",
        "    data = [d.strip().split() for d in open(data_file, 'r').readlines()]\n",
        "    # strip() to remove spaces, split ['splits', 'a', 'string', 'into', 'a', 'list']\n",
        "    # done for each line in the data_file\n",
        "    if uncased:\n",
        "        data = [[t.lower() for t in sent] for sent in data]\n",
        "        # lower case for every word\n",
        "        \n",
        "    for k, v in replacements.items():\n",
        "        # example from \"prepare_wiki\": replace \"@@unknown@@\"(k) with  \"[UNK]\"(v)\n",
        "        data = [[t if t != k else v for t in sent] for sent in data]\n",
        "        # if the token t is different from k (what we have to change), then leave t\n",
        "        # otherwise, if t is = k, replace t with v \n",
        "        \n",
        "        # recall: replacements is a dictionary that connects tokens to be substituted\n",
        "        # and token that substitute, e.g. \"@@unknown@@\" with \"[UNK]\"\n",
        " \n",
        "        # at the end the data are ready to be used in corpus_bleu\n",
        "    return data\n",
        "\n",
        "def prepare_wiki(data_file, uncased=True):\n",
        "    \"\"\" prepare the data from wiki103 so we can use these phrases as \n",
        "    references in the corpus bleu function \"\"\"\n",
        "    replacements = {\"@@unknown@@\": \"[UNK]\"}\n",
        "    return prepare_data(data_file, replacements=replacements, uncased=uncased)\n",
        "\n",
        "def prepare_tbc(data_file):     \n",
        "    \"\"\" prepare the data from tbc so we can use these phrases as \n",
        "    references in the corpus bleu function \"\"\"   \n",
        "    replacements = {\"``\": \"\\\"\", \"\\'\\'\": \"\\\"\"}\n",
        "    return prepare_data(data_file, replacements=replacements)\n",
        "\n",
        "def corpus_bleu(generated, references):\n",
        "    \"\"\" Compute similarity between two corpora as measured by\n",
        "    comparing each sentence of `generated` against all sentences in `references` \n",
        "    \n",
        "    args:\n",
        "        - generated (List[List[str]]): list of sentences (split into tokens)\n",
        "        - references (List[List[str]]): list of sentences (split into tokens)\n",
        "        \n",
        "    returns:\n",
        "        - bleu (float)\n",
        "    \"\"\"    \n",
        "    # generated is a list of sentences, where each sentence is represented as a list\n",
        "    # of tokens.\n",
        "    # references have the same basis structure of generated.\n",
        "    return bleu.corpus_bleu([references for _ in range(len(generated))], generated)\n",
        "    # compare each sentence of 'generated' against all sentences in 'references'\n",
        "    # corpus_bleu -> ([['reference'], ['reference'],...(|generated| times)], ['generated'])\n",
        "    # corpus_bleu analyzes each sentences of the list \"generated\" with all the others\n",
        "    # of \"references\", and then averages (not a simple averaging)\n",
        "    # while when we analyze self_bleu we are comparing against the other generated\n",
        "    # sentences, not with some reference sentences!\n",
        "    "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X8xDX-9qYf1"
      },
      "source": [
        "Function _prepare_data_ is used to prepare the data when computing the BLEU score. In particular, each sentence of the data_file is transformed into a list of lists of tokens.\n",
        "Then functions _prepare_wiki_ and _prepare_tbc_ are applied to prepare the training data (of respectively Wikipedia dumps and Toronto Book Corpus) to study the similarity.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UADQGhlFNavl"
      },
      "source": [
        "wiki103_file = 'datawiki103.5k.txt'\n",
        "#this comes from wikitext103 test set\n",
        "tbc_file = 'tbc.5k.txt'\n",
        "\n",
        "wiki_data = prepare_wiki(wiki103_file)\n",
        "tbc_data = prepare_tbc(tbc_file)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1X7aWXfB7u9"
      },
      "source": [
        "######\n",
        "bert_sents = prepare_data('Bert_using_pytorch.txt')\n",
        "openai_sents = prepare_data('openaitext.txt')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6LCzPp0NWGP",
        "outputId": "f859d118-ca14-42ad-bfcf-3fe0b31f6fd1"
      },
      "source": [
        "\"\"\"NON FARE GIRARE QUANDO UTILIZZIAMO BERT LARGE\"\"\"\n",
        "\n",
        "# Some initializations for the table of corpus-BLEU\n",
        "print(model_version)# THIS WHOLE CODE BLOCK HAS TO BE REPEATED FOR\n",
        "# THE OTHER BERT MODEL VERSION TOO\n",
        "\n",
        "TITLE_CORPUS = ['Model', 'Corpus-BLEU against WT103', 'Corpus-BLEU against TBC']\n",
        "# values_corpus_bleu is a list of 3 lists (one with the model name, two for corpus-BLEU)\n",
        "# each one with 4 elements (first element refers to BERTlarge, second element is for\n",
        "# BERTbase, third element for GPT, fourth element for WT103)\n",
        "# initialization:\n",
        "values_corpus_bleu = [[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
        "values_corpus_bleu[0] = ['BERTlarge', 'BERTbase', 'GPT', 'WT103']"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert-base-uncased\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulkXvxDCPCyk"
      },
      "source": [
        "The following code block has to be repeated two times, one for 'bert-base-uncased', one for 'bert-large-uncased'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syLYzH0YNdy3",
        "outputId": "0199c051-43cf-4d27-b596-0bb5fa4dae0a"
      },
      "source": [
        "# this code block has to be repeated two times, one for bert-large, one for bert-base \n",
        "value = corpus_bleu(bert_sents, tbc_data)\n",
        "print(\"BERT-TBC BLEU: %.2f\" % (100 * value))\n",
        "if model_version == 'bert-base-uncased':\n",
        "  # true value: 7.06\n",
        "  values_corpus_bleu[2][1] = 100 * value\n",
        "else: #'bert-large-uncased'\n",
        "  # true value: 7.60\n",
        "  values_corpus_bleu[2][0] = 100 * value\n",
        "\n",
        "value = corpus_bleu(bert_sents, wiki_data)\n",
        "print(\"BERT-Wiki103 BLEU: %.2f\" % (100 * value ))\n",
        "if model_version == 'bert-base-uncased':\n",
        "  # true value: 7.80\n",
        "  values_corpus_bleu[1][1] = 100 * value\n",
        "else: #'bert-large-uncased'\n",
        "  # true value: 5.05\n",
        "  values_corpus_bleu[1][0] = 100 * value\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-TBC BLEU: 7.04\n",
            "BERT-Wiki103 BLEU: 8.51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkTWO7833RJD",
        "outputId": "3ff786bc-8fbb-432d-bca8-3a5c64f2c989"
      },
      "source": [
        "import random\n",
        "wiki1000_file = '/content/wiki_train_1000_samples_new.txt'\n",
        "##this comes from wikitext103 training set\n",
        "wiki1000_data = prepare_wiki(wiki1000_file)\n",
        "\n",
        "value = corpus_bleu(wiki1000_data, wiki_data)\n",
        "print(\"Wiki103_train-Wiki103 BLEU: %.2f\" % (100 * value))\n",
        "#true value: 17.48\n",
        "values_corpus_bleu[1][3] = 100 * value\n",
        "## this should be the value of the entry WT103 - WT103 of Table3 of the paper\n",
        "\n",
        "value = corpus_bleu(wiki1000_data, tbc_data)\n",
        "print(\"Wiki103_train-TBC BLEU: %.2f\" % (100 * value))\n",
        "#true value: 6.57\n",
        "values_corpus_bleu[2][3] = 100 * value\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wiki103_train-Wiki103 BLEU: 15.18\n",
            "Wiki103_train-TBC BLEU: 6.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQlDAZCJOOAI",
        "outputId": "acd02fab-3e42-46af-d27b-ef519bdd2d50"
      },
      "source": [
        "value = corpus_bleu(openai_sents, tbc_data)\n",
        "print(\"GPT-TBC BLEU: %.2f\" % (100 * value))\n",
        "#true value 30.75\n",
        "values_corpus_bleu[2][2] = 100 * value\n",
        "\n",
        "value = corpus_bleu(openai_sents, wiki_data)\n",
        "print(\"GPT-Wiki103 BLEU: %.2f\" % (100 * value))\n",
        "#true value: 10.81\n",
        "values_corpus_bleu[1][2] = 100 * value\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPT-TBC BLEU: 30.02\n",
            "GPT-Wiki103 BLEU: 11.32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd3F8xsLOQzm"
      },
      "source": [
        "## Diversity measures: Self-BLEU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMP9oz7ryuMo"
      },
      "source": [
        "Self-BLEU: treat each sentence as a hypothesis and treat rest of corpus as reference. Lower is better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqKa9k1mOdPg"
      },
      "source": [
        "#help(bleu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzg-4ECOjJ_m"
      },
      "source": [
        "The following function implements the self_bleu measure for diversity between one sentence and all the others in the document.\n",
        "Recall the difference between BLEU and SELF-BLEU. Since BLEU aims to assess how similar two sentences are, it can also be used to evaluate how one sentence resembles the rest in a generated collection. Regarding one sentence as hypothesis and the others as reference, we can calculate BLEU score for every generated sentence, and define the average BLEU score to be the Self-BLEU of the document.\n",
        "\n",
        "So the difference between BLEU and SELF-BLEU is that BLEU analyzes a group of generated sentences against a group of reference sentences. On the other hand, SELF-BLEU compares sentences of the same type (e.g. generated words)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SBZ_lSrjIz2"
      },
      "source": [
        "def self_bleu(sents):\n",
        "  # this function computes the scoring for comparing diversity between one sentence\n",
        "  # and all the others in the document.\n",
        "  # higher self-bleu score indicates less diversity in the project.\n",
        "    return bleu.corpus_bleu([[s for (j, s) in enumerate(sents) if j != i] for i in range(len(sents))], sents)\n",
        "  # function corpus_bleu(): for calculating the BLEU score for multiple sentences such as a paragraph or a document.\n",
        "  # https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
        "  # We propose Self-BLEU, a metric to evaluate the diversity \n",
        "  # of the generated data. Since BLEU aims to assess how similar\n",
        "  # two sentences are, it can also be used to evaluate how one sentence \n",
        "  # resembles the rest in a generated collection. Regarding one sentence \n",
        "  # as hypothesis and the others as reference,\n",
        "  # we can calculate BLEU score for every generated sentence, \n",
        "  # and define the average BLEU score to be the Self-BLEU of the document.\n",
        "  # Self-BLEU: treat each sentence as a hypothesis and treat rest of corpus \n",
        "  # as reference. Lower is better."
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDoHe5MWMRm0",
        "outputId": "08f1ab82-fb73-4c76-9ca3-2c57ca0a19d0"
      },
      "source": [
        "#print(bert_sents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['and', 'he', 'hated', 'himself', 'for', 'it', '.', 'because', 'that', 'was', 'exactly', 'how', 'he', 'felt', '.', 'over', 'all', 'of', 'it', '.', 'over', 'this', 'gorgeous', 'man', '.', 'but', 'no', ',', 'not', 'all', 'of', 'it', '.', 'no', ',', 'not', 'all', 'of', 'it', '.'], ['first', ',', 'you', 'showed', 'him', 'letters', 'to', 'me', 'claiming', 'that', 'he', 'was', 'an', 'ex', '-', 'fascist', '.', 'then', 'you', 'told', 'me', 'how', 'fast', 'the', 'history', 'literature', 'courses', 'in', 'my', 'class', 'were', 'changing', 'my', 'thoughts', 'on', 'shooting', 'jews', 'next', 'door', '.'], ['tammany', 'and', 'the', 'others', 'were', 'uncomfortable', 'about', 'the', 'place', ';', 'and', 'long', 'before', 'they', ',', 'too', ',', 'finally', 'spoke', '.', 'eddie', 'listened', 'many', 'times', 'to', 'his', 'beloved', 'mel', '-chers', 'just', 'before', 'he', 'picked', 'up', 'on', 'her', '.'], ['on', 'the', 'latter', ',', 'viewers', 'were', 'unaware', 'of', 'what', 'had', 'been', 'done', ',', 'however', 'a', 'u', '-', 'shaped', 'area', 'covering', 'all', 'number', 'of', 'islands', 'had', 'long', 'been', 'used', 'for', 'the', 'backdrop', ',', 'just', 'as', 'it', 'was', 'originally', 'intended', 'to', '.'], ['love', 'you', ',', 'though', '-', 'love', 'you', '‖', 'c', '/', 'w', '/', '19', ':', '45', 'jez', '-', 'my', 'head', 'hurts', '.', '‖', '-', 'come', 'and', 'tell', 'me', 'this', '.', 'is', 'there', 'a', 'way', 'out', '?', '‖', '-', 'no', '.'], ['<', '<', '>', '⟨', 'xgh', '⟩', '(', 'x', '(', ')', 'xgh', '(', ')', ')', '=', '0', '/', '>', '\"', 'x', '(', ')', '\"', 'is', 'any', 'convex', 'map', 'from', 'edges', 'to', 'vertices', 'on', 'all', 'vertices', 'in', 'that', 'graph', '.'], ['it', 'is', 'possible', 'that', 'he', 'has', 'also', 'had', 'roles', 'in', 'various', 'fire', 'departments', 'for', 'the', 'village', 'where', 'these', 'outdoor', 'projects', ',', 'such', 'as', 'the', 'police', 'station', ',', 'fire', 'station', ',', 'and', 'iceberg', 'lake', 'interpretive', ',', 'are', 'located', '.'], ['amundsen', 'calls', 'the', 'tail', '(', 'level', '4', ')', 'first', 'he', '/', 'wolf', '/', 'wolf', ',', 'and', 'finally', 'he', '/', 'i', '/', 'pet', '(', 'level', '5', ')', '/', 'wolf', '(', 'aggression', 'and', 'aggression', 'levels', '1', '-', '4', ')', '.'], ['nominated', 'audience', 'award', 'for', 'best', 'screenplay', '(', 'screenplay', ')', 'at', 'the', 'toronto', 'film', 'festival', '2002', '.', 'audience', 'award', 'at', 'cannes', 'film', 'festival', '2002', '.', 'script', 'awards', '(', 'screenplay', ')', 'for', \"'\", 'flyout', \"'\", 'book', '(', 'debut', 'novel', ')', '.'], ['and', 'he', 'himself', ',', 'even', 'though', 'he', 'loved', 'painting', ',', 'might', 'still', 'paint', 'or', 'sketch', 'painting', ',', 'or', 'realist', 'painting', ',', 'or', 'speckle', 'painting', ',', 'or', '(', 'at', 'some', 'time', ')', '.', 'but', 'it', 'was', 'all', 'gone', '.'], ['(', '2', ')', '\"', 'yes', '!', 'yes', '!', 'yes', '!', '!', '\"', '*', '\"', 'hush', 'up', '.', '(', '3', ')', '!', '\"', 'women', 'cry', '.', '\"', 'she', 'was', 'scared', '.', '\"', '*', '*', 'it', 'is', 'my', 'first', 'visit', 'home', '.'], ['apprenticeship', 'programs', 'for', 'next', '-', 'generation', 'heathcliff', 'senior', 'children', 'including', 'school', ',', 'parent', '/', 'child', 'identity', 'and', 'learning', 'services', ';', 'an', 'online', 'after', '-', 'school', 'education', 'program', 'for', 'children', 'using', 'skills', 'during', 'work', 'preparation', 'and', 'family', 'development', 'programs', ';'], ['the', 'rough', 'trading', 'eventually', '\"', 'led', 'the', 'planet', 'to', 'extinction', '\"', '.', 'meanwhile', ',', 'captain', 'aio', '(', 'taitsu', 'takahashi', ')', 'is', '\"', 'founding', 'a', 'family', 'of', 'his', 'own', '\"', 'and', 'has', 'two', 'dogs', 'and', 'an', 'elderly', 'mother', '.'], ['provided', 'activities', 'in', 'physical', 'therapy', ',', 'hockey', 'and', 'other', 'sports', '(', 'including', 'swimming', ')', ';', 'participated', 'in', 'a', 'spring', 'festival', ';', 'hugged', 'a', 'woman', 'who', 'said', ':', '\"', 'just', 'a', 'moment', 'for', 'my', 'mother', '\"', 'while', 'she', 'was', 'pregnant', ';'], ['\"', 'sam', 'was', 'told', 'to', 'check', 'the', 'rear', '-', 'view', 'mirror', 'at', 'its', 'three', 'control', 'levels', '.', 'go', ',', 'interrupt', 'and', 'finalize', 'your', 'orders', ',', 'then', 'unleash', 'them', 'right', '.', '\"', 'enter', 'the', 'control', 'panel', 'first', '.'], ['they', 'deserved', 'a', 'woman', 'like', 'ellie', '.', 'three', ',', 'two', 'four', ',', 'two', 'three', 'three', 'as', 'the', 'two', 'girls', 'left', ',', 'ellie', 'whispered', ',', '\"', 'please', 'come', 'inside', '.', 'i', 'hope', 'that', 'you', 'can', 'spend', 'some', 'time', 'with', 'me', '.'], ['ath', '.', 'dr', '.', 'leonhard', 'thuringiusse', 'composed', 'early', 'scores', 'for', 'television', 'and', 'provided', 'the', 'score', 'and', 'background', 'music', 'for', 'the', '1983', 'miniseries', 'murder', 'in', 'the', 'cathedral', 'and', '1989', 'film', 'i', 'was', 'falling', 'in', 'love', '.'], ['2009', '-', '|', '|', '|', '|', 'peter', 'frenke', ',', 'organist', 'and', 'deacon', 'of', 'the', 'glenfield', 'church', '.', '2009', '-', '|', '|', '|', 'george', 'wilson', ',', 'bass', 'musician', 'tim', 'baker', '(', 'born', '1980', ')', 'is', 'from', 'preston', '.'], ['she', 'replies', 'to', 'her', 'overly', 'cheerful', 'sister', '.', 'the', 'beach', 'party', 'is', 'filled', 'with', 'the', 'wonderfulst', 'people', ';', 'me', ';', 'my', 'brother', ',', 'mother', ',', 'sister', ',', 'and', 'best', 'friend', 'who', 'know', 'how', 'to', 'spin', 'a', 'wedding', 'cake', ';'], ['\"', 'in', 'honor', 'of', 'allan', 'poe', '\"', ',', '1999', '.', 'emc', 'series', '\"', 'award', 'for', 'historical', 'writing', 'edgar', 'allan', 'poe', ':', '\"', 'edgar', 'allan', 'poe', 'by', 'himself', '\"', ',', '2009', '.', 'with', 'j', '.', 'l', '.', 'nyman', '.'], ['this', 'implies', 'taking', 'each', 'element', 'at', ',', ',', 'and', 'each', 'of', 'both', 'old', 'sequences', 'with', 'and', ',', 'taking', 'with', '.', 'therefore', ',', 'taking', 'with', 'both', 'plus', 'and', 'minus', 'implies', 'taking', 'of', 'both', 'old', 'sequences', 'with', 'and', 'from', 'each', 'other', '.'], ['josh', 'homme', '-', 'bass', 'craig', 'harris', '-', 'drums', 'wire', '-', 'primary', 'producer', 'brad', 'johnston', '-', 'lead', 'guitar', 'moby', 'grape', '-', 'producer', 'somewhere', 'close', 'to', 'you', 'was', 'the', 'first', 'punk', 'rock', 'box', 'set', 'released', 'since', 'their', 'extended', 'play', 'recordings', '.'], ['\"', 'good', 'idea', '.', 'maybe', 'some', 'sparring', 'a', 'bit', '.', 'something', 'about', 'haute', 'couture', '.', 'is', 'everything', 'alright', '?', '\"', '\"', 'very', 'well', ',', 'miss', 'turner', '.', '\"', 'her', 'first', 'visit', 'to', 'rome', 'had', 'been', 'spectacular', '.'], ['david', 'blackwood', 'originally', 'discovered', 'ieee64', '/', 'xo', '(', 'x', ')', 'input', 'files', 'and', 'used', 'them', 'by', 'bluewave', '.', 'ibm', '(', 'in', 'wap', ')', '-', 'most', 'complex', 'computer', 'hardware', 'able', 'to', 'easily', 'import', 'input', 'files', '.'], ['the', 'acute', '/', 'chronic', 'battle', 'injury', 'unit', '(', 'the', 'specialist', '/', 'specialist', 'unit', ')', 'or', 'pais', 'cagliatrici', '(', 'ppv', ')', 'is', 'a', 'medical', 'or', 'surgical', 'unit', '(', 'mhc', 'unit', ')', 'of', 'italian', 'army', '.'], ['frankie', 'enlists', 'stella', ',', 'who', 'has', 'yet', 'to', 'integrate', 'a', 'child', 'into', 'life', ';', 'cosmo', ',', 'who', 'does', 'know', '\"', 'magical', 'powers', '\"', ';', 'his', 'mother', ',', 'who', 'spends', 'the', 'best', 'part', 'of', 'her', 'spare', '-', 'time', ';'], ['\"', 'it', 'already', 'is', ',', '\"', 'he', 'said', 'simply', ',', 'music', 'vibrating', 'against', 'her', 'ears', '.', 'the', 'phone', 'line', 'was', 'gone', ',', 'and', 'david', 'pulled', 'down', ',', 'pulling', 'her', 'to', 'face', 'him', 'as', 'his', 'mouth', 'moved', 'closer', 'and', 'closer', '.'], ['unlike', 'his', 'own', '.', 'it', 'had', 'nothing', 'on', 'the', 'outside', '.', 'hand', '-', 'painted', 'tapestries', 'by', 'the', 'longtime', 'caretakers', 'of', 'this', 'place', 'were', 'not', 'a', 'task', 'that', 'he', 'would', 'take', 'lightly', '.', 'that', 'had', 'been', 'easy', 'enough', '.'], ['raoul', 'wallenberg', ':', 'around', 'the', 'world', '(', 'pbs', ')', 'in', 'the', 'first', 'half', 'of', '1997', '.', 'jawad', 'faraghi', ':', '(', 'tba', ')', '1999', '.', 'abc', '(', 'chicago', 'tribune', ')', ',', 'voice', 'of', 'the', 'americas', '.'], ['s', '.', 'yatz', '(', '1919', ',', 'in', 'collaboration', 'with', 'gordon', 'brown', ')', '-', 'leonard', 'farber', 'oktoberfest', 'chapter', 'on', 'pre', '-', 'war', 'comics', '(', '1919', '-', '1933', ',', 'front', 'page', 'of', 'gordon', 'brown', ')', 'help', '!'], ['glancing', 'down', ',', 'she', 'opened', 'her', 'eyes', ',', 'and', 'saw', '-', 'she', 'leaped', 'into', 'the', 'narrow', 'tunnel', 'and', 'dashed', 'for', 'the', 'door', ',', 'her', 'movements', 'untethered', '.', 'the', 'thought', 'died', ',', 'kat', \"'\", 's', 'mind', 'a', 'fog', '.'], ['1792', '.', 'the', 'letters', 'of', 'cartographer', 'william', 'wallace', 'and', 'the', 'letters', 'of', 'william', 'wallace', '.', 'with', 'a', 'commentary', '.', 'edinburgh', ':', 'thomas', 'f', '.', 'f', '.', 'a', '.', ',', '1478', '.', 'templeton', ',', 'b', '.', 'p', '.'], ['she', 'loved', 'him', '.', 'she', 'belonged', 'to', 'him', '.', 'when', 'she', 'asked', ',', 'he', 'had', 'done', 'exactly', 'what', 'she', 'asked', '.', 'even', 'though', 'she', 'of', 'course', 'hated', 'his', 'guts', '.', 'she', 'remembered', 'the', 'way', 'they', 'had', 'acted', 'over', 'time', '.'], ['no', 'information', 'can', 'be', 'given', 'about', 'george', ',', 'however', ',', 'turning', 'on', 'poorhe', 'in', 'secret', 'hinted', 'at', 'other', 'adages', 'very', 'close', 'to', 'poorhe', 'in', 'that', 'george', 'was', 'first', 'to', 'murder', 'of', 'him', 'and', 'then', 'poison', 'him', '.'], ['and', 'the', 'nimbus', 'whispered', 'in', 'her', 'mind', '...', 'of', 'the', 'suffering', 'of', 'the', 'angels', ',', 'all', 'of', 'them', ',', 'who', 'shall', 'saw', 'and', 'hear', '.', 'all', 'that', 'going', 'on', '...', 'the', 'little', 'stuff', '-', 'all', 'of', 'it', '.'], ['together', ',', 'they', 'had', 'two', 'children', ',', 'thomas', 'and', 'rebecca', 'mary', '.', 'they', 'named', 'their', 'sons', 'two', 'daughters', ',', 'two', 'sons', '&', 'a', 'daughter', ',', 'also', 'called', 'john', ',', 'older', ',', 'or', 'william', ',', 'younger', ',', 'after', 'brother', 'william', '.'], ['as', 'he', 'co', '-', 'designed', 'the', 'covers', 'for', 'the', 'critically', 'well', '-', 'received', 'anthology', 'mystery', 'comics', '#', '9', ',', 'carollie', 'found', 'a', 'sense', 'of', 'place', 'in', 'his', 'creative', 'hot', 'streak', ',', 'fueling', 'readers', \"'\", 'overt', 'interest', ';'], ['iain', 'russell', '-', 'former', 'controller', 'of', 'channel', '4', ',', 'general', 'manager', 'of', 'itv', 'plc', 'robin', 'walker', '-', 'television', 'presenter', 'martyn', 'h', '.', 'smith', '-', 'assistant', 'far', 'east', 'divisional', 'manager', ',', 'presenter', 'of', 'saturday', 'night', 'live', 'in', 'the', '1990s', ';'], ['soon', 'kher', '(', '2006', ')', ',', 'the', 'legendary', 'all', '-', 'girl', 'pop', 'star', ',', 'hover', 'as', 'an', 'advocate', 'of', 'various', 'freedoms', 'and', 'standards', 'of', 'morality', ',', 'under', 'the', 'supervision', 'of', 'a', 'poor', 'young', 'schoolfriend', '.'], ['there', 'you', 'are', 'drifting', 'by', 'night', 'and', 'lost', 'in', 'bed', 'another', 'day', '-', 'in', 'bed', ',', 'on', 'your', 'own', '.', 'there', 'are', 'the', 'veggies', 'out', 'in', 'the', 'junkyard', ',', 'the', 'one', 'never', 'made', 'humpty', '.'], ['the', 'billy', 'graham', 'show', 'primarily', 'documented', 'christmas', ':', 'ken', ',', 'charlie', 'and', 'jim', 'each', 'presented', 'every', 'other', 'week', 'with', 'special', 'guests', 'to', 'show', 'live', '-', 'in', 'music', '-', 'videos', ',', 'animations', ',', 'comedy', 'videos', 'and', 'motion', '-', 'pictures', '.'], ['the', 'educational', 'requirements', 'for', 'institutions', 'affiliated', 'with', 'the', 'bfa', 'include', ':', 'short', 'films', ',', 'all', 'new', 'short', 'films', 'and', 'their', 'real', '-', 'life', 'counterparts', '(', 'often', 'on', 'a', 'higher', 'level', ')', ',', 'all', 'upcoming', 'experimental', '/', 'experimental', 'films', '.'], ['maindead', 'teams', 'driven', 'by', 'john', 'thomas', 'swainson', 'teams', 'were', 'affected', 'by', 'that', 'competition', ',', 'which', 'included', 'd2', ',', 'd1', 'world', ',', 'olympic', ',', 'and', 'u', '+', '19', 'world', 'maindead', 'team', 'racing', '.'], ['the', 'hiv', 'centre', 'was', 'a', 'facility', 'owned', 'by', '25', 'women', 'with', 'hiv', 'and', ',', 'at', 'the', 'same', 'time', ',', 'softball', 'and', 'soccer', 'would', 'game', 'in', 'the', 'victoria', 'park', 'neighbourhood', ',', 'a', 'community', 'in', 'prince', 'edward', 'island', '(', 'canada', ')', '.'], ['so', 'began', 'his', 'search', '.', '*', '*', '*', '\"', 'jason', \"'\", 's', 'book', 'in', 'wicca', '...', '\"', '(', 'extracted', 'from', 'a', 'selected', 'chapter', 'in', \"'\", 'new', 'historical', 'and', 'normative', 'principles', 'of', 'psychic', 'mental', 'training', \"'\", '.', ')', '.'], ['girl', 'on', 'girl', '-', 'featured', 'articles', 'targeting', 'midget', 'girls', 'in', 'the', 'early', '80s', '.', 'self', 'review', 'magazine', '-', 'also', 'promoted', 'as', 'self', 'review', ',', 'the', 'sister', 'magazine', 'by', 'london', '-', 'based', 'hospice', '&', 'palliative', 'care', 'centre', '.'], ['the', '1000', '-', 'seat', 'tom', 'frick', 'trophy', 'at', 'syracuse', 'was', 'the', 'first', 'for', 'syracuse', 'basketball', '.', 'stewart', ',', 'the', 'syracuse', 'coach', ',', 'wanted', 'to', 'completely', 'see', 'his', 'way', 'to', 'his', 'own', 'bowl', ',', 'but', 'insisted', 'on', 'the', 'format', '.'], ['floor', '5', ':', 'karapajen', 'nisusan', '[', 'portrait', ']', ',', 'unesco', 'buildings', 'istanbul', 'international', 'building', ',', 'istanbul', ',', 'turkey', 'floor', '6', ':', 'erganayl', ',', 'kemal', \"'\", 's', 'residence', 'at', 'tangkor', '.'], ['it', \"'\", 's', 'wonderful', ',', 'darling', ',', 'kind', 'of', 'like', 'the', 'dishes', 'from', 'the', 'family', 'house', ',', 'that', 'adair', ',', 'his', 'nephew', ',', 'who', 'always', 'put', 'all', 'the', 'ingredients', 'together', 'that', 'sonia', 'used', 'into', 'the', 'most', 'clever', 'shapes', '.'], ['quantifying', 'informations', '.', 'honolulu', ':', 'itu', '.', '1994', '.', 'catalogue', 'of', 'new', 'information', '(', '*', 'l', '*', 'l', ';', 'r', '*', 'o', ';', 'xx', ';', 'x', ')', '.', 'minneapolis', ',', 'mo', '.', ':', 'itu', '.'], ['artie', 'had', 'admitted', 'to', 'the', 'stage', 'that', 'she', 'was', 'the', 'last', 'person', 'to', 'play', 'saxophone', 'in', 'half', 'the', 'world', '.', 'in', 'my', 'mind', ',', 'there', 'was', 'one', 'detail', 'that', 'felt', 'me', 'curse', '.', '\"', 'oh', ',', 'ouch', '!'], ['a', 'french', 'soldier', ',', 'a', 'libyan', 'of', 'congolese', 'origin', '.', 'how', 'could', 'he', 'have', 'ever', 'known', 'another', 'name', '?', 'besides', 'the', 'last', 'one', 'there', 'had', 'been', 'two', 'images', 'that', 'were', 'copied', 'from', 'his', 'family', '-', 'owned', 'file', 'system', '.'], ['the', 'cinematographer', ',', 'pirj', 'khatami', ',', 'was', 'killed', '(', 'from', 'the', 'episode', \"'\", 'these', 'are', 'the', 'ones', 'i', 'salute', \"'\", ')', ',', 'a', 'teammate', ',', 'ratan', 'ali', ',', 'was', 'injured', ',', 'and', 'underwent', 'surgery', '.'], ['more', 'rush', 'i', 'plan', 'for', 'more', 'and', 'more', ',', 'and', 'will', 'give', 'my', 'wife', 'one', 'last', 'drink', 'before', 'i', 'get', 'laid', 'again', '.', 'the', 'shower', 'door', 'is', 'hit', ',', 'loud', 'and', 'clear', ',', 'the', 'running', 'water', 'stinging', 'my', 'face', '.'], ['no', '-', 'one', 'in', 'london', '.', '\"', 'de', 'la', 'salle', ',', 'of', 'course', ',', 'had', 'had', 'a', 'fine', 'french', 'nose', '.', '\"', 'not', 'even', 'in', 'paris', 'or', 'notre', '-', 'dame', '.', 'not', 'even', 'in', 'germany', '.', '\"', 'she', 'gulped', '.'], ['her', 'facebook', 'page', 'was', 'inactive', ',', 'active', 'in', '2010', 'and', 'about', '2013', '.', 'she', 'is', 'an', 'atheist', 'and', 'her', 'husband', 'is', 'munus', '(', 'in', 'her', 'spare', 'time', 'they', 'had', 'a', 'disagreement', 'when', 'they', 'attended', 'an', 'underground', 'meeting', ')', '.'], ['holy', 'man', '...', 'why', 'the', 'hell', 'had', 'he', 'been', 'tracking', 'one', 'of', 'the', 'demons', 'through', 'a', 'force', 'field', '?', 'not', 'while', 'he', 'was', 'still', 'so', 'focused', ',', 'concentrating', 'on', 'kody', '.', 'vane', 'had', 'made', 'sure', 'of', 'that', 'too', '.'], ['it', 'has', 'the', 'form', 'of', 'a', 'house', 'and', 'cottage', ',', 'and', 'is', 'in', 'perpendicular', 'style', '.', 'he', 'was', 'a', 'builder', 'but', 'not', 'an', 'architect', ',', 'and', 'is', 'listed', 'in', 'the', '\"', 'national', 'parks', '&', 'gardens', '\"', 'at', 'grade', 'ii', '.'], ['he', 'wrote', 'about', 'william', 'william', ',', 'governor', 'of', 'virginia', 'and', 'maryland', ',', 'who', '\"', 'delighted', 'in', 'making', 'the', 'old', 'brick', 'church', '.', '\"', 'on', 'the', 'church', 'premises', 'is', 'the', '\"', 'old', 'brick', 'museum', '\"', 'in', 'cambria', ',', 'pennsylvania', '.'], ['bill', 'o', \"'\", 'reilly', 'in', 'addition', 'offers', 'a', 'compromise', 'with', 'the', 'other', 'senate', 'democrats', ',', 'in', 'that', 'democrats', 'are', 'not', 'included', 'at', 'the', 'republican', 'party', 'level', '(', 'according', 'to', 'mcewen', ')', ',', 'him', 'apart', 'from', 'the', 'democrats', '.'], ['in', 'other', 'words', ',', 'using', 'translation', 'might', 'have', 'helped', 'other', 'students', 'understand', 'at', 'the', 'same', 'time', 'what', 'is', 'in', 'use', 'to', 'switch', 'between', 'saying', '\"', 'what', 'is', 'in', 'use', ',', '\"', 'and', '\"', 'how', 'to', 'use', 'the', 'book', '\"', '.'], ['he', 'served', 'as', 'editor', 'in', 'chief', 'of', 'the', 'professional', 'football', 'writers', \"'\", 'conference', ',', 'where', 'matchday', 'was', 'renamed', '.', 'the', 'magazine', \"'\", 's', 'longest', '-', 'serving', 'editor', 'was', 'the', 'correspondent', 'and', 'live', 'presenter', 'for', 'bbc', 'radio', 'the', 'bbc', '.'], ['the', 'third', 'song', 'is', 'considered', 'the', 'blackest', ',', 'darkest', 'serenade', 'that', 'can', 'be', 'heard', 'in', 'the', 'campus', 'halls', '.', 'i', 'hear', 'james', 'wince', 'when', 'he', 'finished', ',', 'but', 'then', 'i', 'wince', '.', 'my', 'sister', 'is', 'still', 'pissed', '.'], ['figure', '4', 'subposed', 'on', 'p', 'means', ':', 'therefore', ',', 'this', 'implies', 'a', 'figure', '4', 'integral', 'for', 'all', 'point', 'masses', '.', 'for', 'a', 'given', 'simulation', 'scenario', ',', 'the', 'density', 'can', 'be', 'abbreviated', '\"', 'b', '\"', '(', 'discrete', 'plate', ')', '.'], ['\"', 'praying', 'to', 'your', 'god', '\"', '-', 'also', 'means', '\"', 'you', 'are', 'faithful', '\"', '.', 'we', 'believe', 'that', 'when', 'receiving', 'the', 'first', 'sacrament', ',', 'everyone', 'needs', 'to', 'be', 'praying', 'to', 'god', 'in', 'the', 'same', 'way', ',', 'as', 'tradition', 'goes', '.'], ['his', 'father', 'lived', 'and', 'aged', 'until', 'he', 'was', 'eleven', ',', 'and', 'second', 'to', 'david', '\"', 'jil', '\"', 'esafzeymeghian', ',', 'poet', 'and', 'essayist', 'in', 'iran', 'who', 'graduated', 'from', 'the', 'baptist', 'university', 'of', 'texas', '.'], ['she', 'stood', 'against', 'colin', 'hunter', ',', 'a', 'barrister', ',', 'who', 'joined', 'the', 'party', 'before', 'he', 'was', 'old', 'enough', 'to', 'join', 'the', 'snp', '(', '2005', ')', '.', 'obe', '(', '2011', ')', '.', 'charles', 'wood', ',', 'obe', '(', '2013', ')', '.'], ['\"', 'want', 'to', 'be', 'somebody', '\"', 'opens', 'with', 'tyler', 'and', 'a', 'nearby', 'police', 'officer', 'exiting', 'their', 'first', 'two', 'consecutive', 'single', 'numbers', ',', '\"', 'love', 'how', 'i', 'feel', '\"', 'and', '\"', 'growing', 'old', '\"', 'for', '96th', 'and', 'last', 'place', '.'], ['an', 'aunt', 'and', 'teenage', 'daughter', ',', 'whom', 'he', 'thinks', 'may', 'be', 'worth', 'selling', 'off', 'for', 'a', 'trash', 'dumpster', 'offers', 'her', 'a', 'key', 'to', 'a', 'man', 'claiming', 'as', 'her', \"'\", 'right', '-', 'hand', 'man', \"'\", ',', 'take', 'him', 'hostage', '.'], ['therefore', ',', 'where', 'any', 'part', 'of', 'denials', 'is', 'such', 'that', 'the', 'rest', 'consisting', 'of', 'the', '(', 'appropriate', ')', 'portion', 'of', 'denials', ',', 'therefore', 'goes', 'in', 'the', 'same', 'direction', 'as', 'the', 'rest', '(', 'consisting', 'of', 'proper', 'sections', ')', '.'], ['\"', 'umpires', \"'\", 'game', 'lights', '\"', 'for', 'the', 'season', 'in', 'toronto', 'were', 'substituted', 'after', 'a', 'request', 'by', 'mrs', '.', 'mary', 'dogan', '.', 'season', 'lights', 'were', 'replaced', 'with', '\"', 'game', 'lights', '\"', 'normally', 'installed', 'as', 'weekly', 'subscriptions', '.'], ['she', 'claimed', 'to', 'have', 'died', 'of', 'longstanding', 'cancer', '.', 'later', ',', 'andy', 'and', 'todd', 'brown', ',', 'executive', 'director', 'of', 'the', 'american', 'public', 'civil', 'service', 'foundation', 'and', 'public', 'mental', 'health', 'organizations', ',', 'praised', 'her', 'for', 'her', 'substantial', 'financial', 'contribution', '.'], ['(', 'in', 'many', 'cases', ')', 'tribal', 'dance', 'is', 'done', 'here', 'on', 'occasion', 'with', 'the', 'festival', 'festival', '.', 'to', 'honour', 'the', 'highness', 'of', 'the', 'maharajah', 'of', 'chaitra', ',', 'she', 'is', 'called', 'beauty', ',', 'has', 'special', 'pale', 'complexion', '.'], ['he', 'signed', 'with', 'them', 'and', 'became', 'their', 'director', '.', 'the', 'original', 'film', 'director', 'working', 'on', 'the', 'film', ',', 'bruce', 'moore', ',', 'threatened', 'to', 'give', 'up', 'pretending', 'that', 'he', 'was', 'the', 'director', 'unless', 'the', 'studio', 'agreed', 'and', 'the', 'latter', 'left', ';'], ['c', '-', 'r', 'class', 'l3', '(', 'two', 'cars', ',', 'not', 'one', 'car', 'later', ')', '1956', 'mooretown', ':', 'cgsc', 'railway', 'also', 'used', 'its', 'freight', 'trains', 'and', 'built', 'another', 'moveable', 'western', '-', 'style', 'class', 'l3', '.'], ['and', 'it', 'was', 'all', 'thought', 'who', 'picked', '-', 'up', 'the', 'needleed', 'little', 'girl', 'and', 'gave', 'her', 'back', 'to', 'her', '.', 'it', 'was', 'a', 'flower', ',', 'a', 'tree', '(', 'allusions', 'of', 'the', 'movie', 'that', 'goon', 'starred', ')', '.'], ['volume', 'ii', '-', 'triennial', 'with', 'lincoln', 'davis', ',', 'with', 'contributions', 'by', 'john', 'wesley', 'powell', ',', 'didcot', ',', 'brules', ',', 'and', 'h', '.', 'r', '.', 'ballard', ';', 'oklahoma', 'historical', 'society', '(', '1953', ')', 'by', 'harry', 'tyner', ';'], ['charles', 'john', 'morgan', '(', 'r', '/', 'o', '(', 'circuit', 'court', 'judge', ')', ')', ',', 'colonial', 'auctioneer', 'and', 'estates', 'court', 'judge', '.', 'frederick', 'miller', '(', 'r', '/', 'o', '.', ')', 'frederick', 'milbankson', ',', 'bank', 'clerk', 'and', 'manager', '.'], ['jeffers', 'turned', 'around', ',', 'shouting', ',', '\"', 'no', 'no', '!', 'no', 'no', '!', 'no', 'no', 'no', '!', '\"', 'the', 'figure', 'faltered', 'a', 'foot', 'or', 'two', '.', 'nothing', 'was', 'lost', ',', 'no', 'sign', 'that', 'the', 'man', 'had', 'exposed', 'himself', '.'], ['that', 'title', 'would', 'more', 'or', 'less', 'endure', 'through', 'the', 'first', 'quarter', 'of', '1928', 'when', 'used', 'interchangeably', 'with', '\"', 'now', 'is', 'the', 'time', ',', '\"', 'along', 'with', 'the', 'great', 'escape', '(', 'mgm', ')', 'and', 'the', 'fugitive', '(', 'paramount', ')', '.'], ['maybe', '3', 'or', '4', 'minutes', 'would', 'pass', 'and', 'she', 'would', 'still', 'be', 'inside', ',', 'and', 'it', 'leads', '(', 'maybe', ')', 'right', 'into', 'a', 'great', 'evening', 'fantasy', '.', 'in', 'my', 'head', ',', \"'\", 'going', 'in', \"'\", 'seemed', 'like', 'some', 'fantasy', '.'], ['he', 'is', 'also', 'a', 'befriending', 'musician', 'and', 'djs', 'mike', 'wells', ',', 'eric', 'alphonse', 'and', 'kacey', 'murphy', '.', 'converge', '(', 'live', ')', 'vol', '.', '2', 'it', 'was', 'the', 'first', 'release', 'by', 'the', 'band', 'converge', '.'], ['and', 'it', 'was', 'somebody', 'now', ',', 'he', 'thought', '.', 'and', 'then', 'they', 'were', 'all', 'gone', '.', 'the', 'moonlight', 'had', 'risen', ',', 'and', 'cast', 'a', 'silver', 'pattern', ';', 'he', 'could', 'see', 'it', 'shining', 'out', 'through', 'the', 'green', '-', 'green', 'leaves', '.'], ['the', 'death', 'of', 'two', 'friends', '(', 'noel', 'fenn', ',', 'john', 'thorndyke', ')', ',', 'when', 'all', 'three', 'have', 'collapsed', '(', 'informally', 'known', 'as', '\"', 'martyrs', '\"', ',', 'when', 'eyes', 'snap', 'at', 'each', 'other', \"'\", 's', 'faces', ')', ';'], ['the', 'northern', 'american', 'indian', ':', 'the', 'lindgren', 'anthology', 'of', 'the', 'poets', 'of', 'the', 'american', 'west', '.', 'essays', ',', 'letters', ',', 'and', 'poems', '.', 'echoes', 'from', 'poetry', 'at', 'the', 'theatre', 'of', 'john', '\"', 'rem', '\"', 'thomas', '.'], ['\"', 'marvin', 'and', 'gaye', '\"', ';', 'jimmy', 'carter', 'on', 'on', '\"', 'god', 'and', 'love', 'die', '\"', ';', 'amina', 'smith', 'on', '\"', 'the', 'kurd', '\"', ';', 'and', 'patti', 'smith', 'on', 'on', '\"', 'the', 'edge', 'of', 'darkness', '\"', '.'], ['russ', ':', '...', 'was', 'that', 'a', 'girl', '?', 'russ', ':', 'was', 'that', 'at', 'all', '?', 'russ', ':', 'what', 'does', 'it', 'matter', ',', 'thurlow', '?', 'russ', ':', '...', 'we', 'were', 'walking', '.', 'talking', '.', 'standing', 'beyond', 'the', 'door', '.'], ['since', 'then', 'it', 'has', 'included', 'the', '\"', 'tri', '-', 'ranks', '\"', '(', 'the', 'former', 'titles', 'of', 'major', 'general', ',', 'infantry', 'general', 'generalfinder', 'general', 'and', 'fo', ')', ',', 'corresponding', 'to', 'the', 'fifth', 'and', 'sixth', 'tri', '-', 'ranks', 'respectively', '.'], ['elaine', 'miller', ',', 'south', 'african', 'press', 'editor', '.', 'first', 'a', 'magazine', 'photographer', 'and', 'later', 'a', 'cape', 'town', '-', 'based', 'practice', 'photographer', '.', 'also', 'producing', 'articles', 'for', 'publications', 'of', 'considerable', 'volume', ',', 'and', 'to', 'which', 'she', 'made', 'a', 'significant', 'contribution', '.'], ['pushcart', 'circle', '(', 'double', 'strand', ')', 'awards', '2009', '-', 'outstanding', 'achievement', 'by', 'producer', 'of', 'television', 'and', 'radio', 'programmes', 'based', 'in', 'london', '.', 'awards', '2009', ',', '2011', '-', 'outstanding', 'radio', 'series', '2009', ',', '2009', ',', '2010', '-', 'e', '!'], ['member', 'of', 'senate', '(', 'u', '.', 's', '.', 'senate', 'elected', ')', '.', 'he', 'was', 'director', 'of', 'public', 'instruction', 'at', 'henry', 'white', 'elementary', 'school', ',', 'berrian', 'county', ',', 'from', 'which', 'he', 'taught', 'the', 'composition', 'and', 'recitation', 'of', '.'], ['peter', \"'\", 's', 'monotonous', 'call', 'reveals', 'the', 'true', 'identity', 'of', 'jim', 'parker', ',', 'the', 'amarillo', 'agent', 'working', 'for', 'a', 'freelancer', 'dubbed', 'the', 'eagle', 'division', '.', 'in', 'retaliation', ',', 'peter', 'kills', 'the', 'agent', 'before', 'leaving', '.'], ['mr', '.', 'fitz', 'and', 'carol', 'are', 'happy', 'now', ';', 'writing', 'forever', ',', 'now', 'and', 'forever', '.', 'the', 'author', 'was', 'not', 'credited', 'because', 'he', 'was', 'not', 'on', 'holiday', '.', 'he', 'has', 'two', 'sons', '(', 'austin', 'fitz', 'and', 'alex', 'fitz', ')', '.'], ['during', 'preparation', 'for', 'the', '2009', 'abc', 'television', 'studios', 'afi', 'awards', ',', 'the', 'actors', 'auditions', 'would', 'either', 'be', 'held', 'at', 'the', 'sydney', 'film', 'festival', 'or', 'held', 'at', 'the', 'afi', 'awards', '(', 'a', 'major', 'performing', 'event', 'for', 'tv', 'in', 'australia', ')', '.'], ['this', 'version', ',', 'however', ',', 'features', 'guest', 'vocalists', ',', 'including', 'howie', 'mandel', '(', 'this', 'version', 'contains', '\"', 'sentimentality', 'song', '(', 'half', 'a', 'chance', ')', '\"', 'reprises', ')', 'and', 'joni', 'mitchell', '(', 'stairway', 'to', 'heaven', ')', '.'], ['marie', 'rosa', 'seton', ',', 'leader', '(', '12', 'years', ')', 'of', 'leadership', 'group', '.', 'leadership', 'group', 'leader', 'of', 'forum', '(', '6', ',', '000', 'members', ')', 'on', 'gender', 'equality', 'in', 'urban', 'areas', '(', 'ceo', ')', 'jennifer', 'jackson', 'seton', ',', 'school', 'superintendent', '.'], ['also', 'known', 'as', '\"', 'tutsi', '\"', ',', 'most', 'recently', ':', 'english', 'arabic', ':', 'usually', 'pronounced', '(', 'less', 'so', 'often', ')', 'with', 'what', 'resembles', 'the', 'tongue', '.', 'apple', ':', 'a', 'large', 'apple', ',', 'but', 'not', 'a', 'large', 'area', '.'], ['yesterday', 'and', 'tomorrow', ',', 'a', 'granada', 'serial', 'about', 'a', 'life', 'of', 'innocent', 'love', ',', 'seemed', 'to', 'be', 'forthcoming', 'from', 'november', '1952', '.', 'both', 'series', 'was', 'accompanied', 'by', 'the', 'theme', 'song', '\"', 'jingle', 'bells', 'christmas', '\"', 'by', 'yvonne', 'tucker', ')', '.'], ['\"', 'as', 'if', 'my', 'own', 'heart', 'was', 'making', 'up', 'for', 'it', 'to', 'the', 'softest', 'thing', '.', '\"', '~', 'fin', '~', 'grainna', 'walked', 'toward', 'amber', ',', 'hair', 'up', 'over', 'her', 'long', 'black', 'lashes', ',', 'and', 'crossed', 'the', 'room', '.'], ['and', 'red', ',', 'ginn', '&', 'safran', 'for', 'frank', 'sinatra', ',', 'harper', 'and', 'row', '/', 'macmillan', '.', 'publications', 'by', 'ford', ':', 'pooh', 'baby', '(', 'ford', ',', '1937', ')', '-', 'design', 'of', 'childlike', 'makeup', 'and', 'makeup', ';'], ['actors', 'amanda', 'palmer', ',', 'faith', 'evans', ',', 'shakira', ',', 'jeeby', 'khan', ',', 'faith', 'hill', ',', 'lady', 'gaga', ',', 'jamie', 'criss', ',', 'hebdo', ',', 'and', 'simply', 'red', 'were', 'all', 'of', 'the', 'lead', 'cast', '.'], ['professionally', ',', 'frank', 'was', 'featured', 'in', 'comedy', 'shorts', 'like', 'who', 'would', 'care', '?', ',', 'which', 'often', 'were', 'made', 'by', 'a', 'major', 'production', 'company', 'and', 'a', 'recurring', 'role', 'of', 'the', 'million', '-', 'dollar', 'salesman', 'as', 'his', 'valet', 'billy', 'bob', 'thornton', '.'], ['this', 'actually', 'created', 'him', 'as', 'the', 'last', 'knight', 'and', 'appeared', 'only', 'in', 'the', 'game', 'of', 'thrones', '.', 'among', 'older', 'characters', ',', 'peter', 'parker', 'and', 'steve', 'vai', 'were', 'mentioned', 'as', 'possible', 'successors', ',', 'but', 'they', 'were', 'eventually', 'removed', '.'], ['not', 'because', 'the', 'crowd', 'was', 'thinning', 'due', 'to', 'their', 'earlier', 'early', 'success', ',', 'but', 'because', 'they', \"'\", 'd', 'still', 'had', 'her', 'with', 'them', '.', 'recently', ',', 'several', 'prominent', 'local', 'people', 'had', 'suggested', 'holding', 'social', 'events', 'at', 'hillcrest', '.'], ['second', '(', 'male', 'person', ')', 'and', 'third', '(', 'female', ')', 'figure', '.', 'human', 'figure', '?', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '...', '?', '...', '...', '...', '...', '...', '...', '...', '...', '.', 'human', 'figure', '.'], ['not', 'to', 'be', 'embarrassed', ',', 'they', 'neither', 'appeared', 'nor', 'arrived', ',', 'but', 'accepted', 'that', 'their', 'boat', 'be', 'seized', 'and', 'used', 'as', 'the', 'arsenal', 'during', 'the', 'second', 'nelson', 'war', 'in', 'june', ',', '1668', ',', 'hindering', 'passage', 'to', 'quebec', '.'], ['geometry', 'and', 'geometry', 'include', 'the', 'study', 'of', 'ordinary', 'differential', 'equations', ',', 'such', 'as', 'the', 'rational', 'functions', '(', 'q', ')', 'or', 'the', 'gamma', 'functions', '(', 'terms', 'for', 'the', 'cost', 'of', 'a', 'rational', 'function', '(', 'q', ')', 'with', 'superposition', ')', '.'], ['farley', '(', 'original', 'voice', ')', 'as', 'mr', '.', 'taylor', ',', 'a', 'teacher', ',', 'later', 'he', 'became', 'a', 'member', 'of', 'the', 'science', 'class', '.', 'richard', 'wilberforce', 'as', 'henry', 'murphy', '(', 'original', 'voice', 'of', 'johnny', 'depp', ')', '.'], ['parents', '(', 'children', ')', 'as', 'she', 'was', 'the', 'queen', '(', 'they', 'became', 'the', 'chief', ')', '.', 'the', 'opposition', 'movement', 'during', 'this', 'episode', 'was', 'led', 'by', 'nasim', 'ismail', 'and', 'became', 'more', 'popular', 'in', 'the', 'rest', 'of', 'the', 'eight', 'episodes', '.'], ['about', 'this', 'time', ',', 'new', 'teams', 'were', 'in', 'existence', 'with', 'a', 'large', 'busload', 'of', 'players', 'from', 'older', 'world', 'league', 'sides', ',', 'five', 'of', 'them', 'whom', 'bond', 'knew', 'well', ',', 'but', 'none', ',', 'as', 'were', 'the', 'fellow', 'test', 'stars', '.'], ['hosted', 'by', 'greg', 'ryan', ',', 'for', 'the', 'new', 'zealand', 'version', 'the', 'contestants', 'were', 'finally', 'randomly', 'chosen', 'by', 'default', 'that', 'gave', 'the', 'winner', 'a', 'word', '\"', 'happy', '\"', 'meaning', '\"', 'having', 'a', 'good', 'time', '\"', ',', 'thus', 'getting', 'one', 'point', '.'], ['a', 'boy', 'caught', 'in', 'his', 'own', 'rotten', 'trap', 'when', 'he', 'learned', 'my', 'plan', '!', '\"', 'you', 'are', 'a', 'child', ',', 'a', 'publican', '.', 'come', 'live', 'with', 'us', 'in', 'a', 'mediocre', 'country', '!', 'give', 'it', 'a', 'chance', '!'], ['in', '\"', 'in', 'the', 'tobacco', 'shop', '\"', ',', 'they', 'meet', 'bruce', 'one', 'night', '.', 'dick', 'morris', '(', 'called', '\"', 'the', 'demon', '\"', 'or', '\"', 'the', 'man', '\"', '\"', 'newt', '\"', ')', 'is', 'a', 'corrupt', 'sheriff', 'who', 'has', 'tricked', 'bruce', '.'], ['chad', 'boothroyd', 'was', 'crowned', 'as', 'the', 'first', 'truck', 'champion', ',', 'with', 'teammate', 'frank', 'wallace', 'becoming', 'the', 'third', 'lightweight', 'champion', ',', 'although', 'his', 'two', 'attempts', 'to', 'win', 'the', 'daytona', '500', 'fell', 'short', 'of', 'qualifying', 'as', 'a', 'rookie', 'driver', '.'], ['mr', '.', 'president', 'will', 'guide', 'your', 'left', 'hand', '.', 'si', 'solo', 'que', 'dos', 'ordenos', 'aguas', 'antes', 'vi', 'andir', ',', 'do', 'let', 'me', 'thank', 'you', 'for', 'the', 'fun', 'you', 'are', 'having', 'at', 'my', 'hands', '.'], ['the', 'scene', 'in', 'the', 'foreground', 'shows', 'prominent', 'personalities', ',', 'including', 'mayors', 'michael', 'bloomberg', 'and', 'ed', 'koch', ',', 'rn', 'secretary', 'john', 'renson', ',', 'representative', 'orval', 'johnson', ',', 'and', 'norah', 'jones', ',', 'all', 'wounded', 'during', 'the', 'battle', '.'], ['lyrics', '\"', 'never', 'say', 'goodbye', '\"', ';', 'download', '(', 'rockin', \"'\", 'version', ')', 'north', '-', 'korean', 'artist', '/', 'danish', 'singer', '\"', 'this', 'is', 'my', 'life', '\"', ',', 'track', 'listing', '\"', 'good', 'tonight', '(', 'woe', ')', '...', '\"', ';'], ['the', 'group', 'produced', 'a', 'second', 'and', 'third', 'campaign', 'film', ',', '\"', 'hellas', '\"', ',', '(', '1938', ')', 'based', 'on', 'the', 'events', 'in', 'the', 'book', 'requiem', 'for', 'a', 'man', ',', '1936', '-', '1938', '(', 'g', '.', 'i', '.', ')', '.'], ['much', 'of', 'the', 'original', 'latin', 'text', 'was', 'omitted', 'afterwards', '.', 'gyortewski', 'died', 'for', 'lack', 'of', 'religious', 'zeal', '.', 'arnolda', '(', 'born', 'wenceslaus', 'iii', ')', '-', 'chronicler', 'of', 'wenceslaus', 'iii', '.'], ['i', 'used', 'my', 'landline', 'to', 'call', 'him', ',', 'then', 'my', 'phone', 'called', 'garrick', 'to', 'see', 'if', 'i', 'was', 'okay', '.', 'but', 'all', 'awkwardness', 'was', 'over', '.', 'would', 'i', 'be', 'okay', '?', 'the', 'front', 'doors', 'opened', 'wide', '.'], ['bits', 'of', 'a', 'ciphertext', 'cause', 'a', 'jiggling', 'on', 'the', 'rest', 'of', 'the', 'cipher', 'which', 'is', 'then', 'carried', 'out', 'out', 'of', 'sequence', '.', 'both', 'of', 'the', 'authors', 'of', 'a', 'previous', 'problem', 'came', 'up', 'with', 'the', 'same', 'solution', '.'], ['sir', 'peter', 'maltby', ',', 'is', 'a', 'british', 'businessman', 'and', 'philanthropist', 'and', 'a', 'founder', 'of', 'the', 'maltby', 'group', ',', 'a', 'company', 'described', 'as', 'a', 'conglomerate', 'due', 'to', 'the', 'positive', 'commercial', 'success', 'of', 'his', 'early', 'business', 'ventures', '.'], ['this', 'piano', 'part', 'inspired', 'the', 'melody', '\"', 'nazim', '(', 'music', ')', '\"', 'for', 'qamar', 'mohammadi', \"'\", 's', '89th', 'response', 'to', \"'\", 'be', 'still', \"'\", '.', 'charles', 'martineau', 'was', 'also', 'a', 'composer', 'of', 'requiem', '.'], ['guitar', ':', 'kevin', 'rogers', '.', 'engineering', 'engineering', ':', 'jason', 'jay', 'weller', '.', 'drums', 'and', 'bass', 'guitar', ':', 'chris', 'stange', 'of', 'riot', 'grrrl', '.', 'producers', ':', 'mike', 'm', '.', 'mitchell', '.', 'mixing', 'assistant', ':', 'paul', 'moore', '.'], ['the', 'territorial', 'office', 'is', 'structured', 'in', 'two', 'parts', ':', 'the', 'rule', 'of', 'faith', 'and', 'the', 'calendar', '(', 'including', 'the', 'liturgical', 'calendar', ')', '.', 'the', 'bishop', 'of', 'stepney', 'is', 'qualified', 'for', 'two', '-', 'year', 'training', 'within', 'a', 'regional', 'diocese', ';'], ['3', '.', '3', '-', '4', '.', 'milton', \"'\", 's', 'letter', ',', 'is', 'occasionally', 'written', 'in', 'welsh', 'or', 'in', 'english', 'as', 'fydd', '-', 'y', '-', 'ddu', 'or', 'in', 'welsh', 'lleren', '3', '.', '3', '-', '6', '.'], ['in', 'any', 'case', 'the', 'name', 'of', 'the', 'hotel', 'would', 'change', 'after', 'being', 'approved', 'by', 'the', 'first', 'minister', '.', 'morris', 'said', 'the', 'hotel', 'was', 'closer', 'to', 'metlife', 'headquarters', 'and', 'the', 'address', 'of', 'the', 'hotel', 'was', '1000', 'south', 'kirkby', '.'], ['violet', ',', 'as', 'revealed', 'to', 'her', 'age', 'in', '2011', ',', 'was', 'a', 'student', 'at', 'providence', 'college', '.', '\"', 'violet', '\"', 'is', 'a', 'famous', 'and', 'well', '-', 'known', 'play', 'that', 'director', 'humphrey', 'bogart', 'wrote', 'about', 'rhode', 'island', 'railroad', 'passengers', '.'], ['the', 'flow', 'will', 'be', 'defined', 'as', '.', 'let', 'us', 'define', '\"', 'flow', '\"', '(', 'flow', ')', '.', '\"', 'flow', '\"', '(', 'flow', ')', ']', '.', 'for', 'the', 'flow', 'matrix', ',', 'define', 'the', 'set', 'of', 'points', 'in', 'the', 'flow', 'matrix', '.'], ['hah', 'hah', 'hah', ',', 'how', 'about', 'you', '!', 'bpa', ',', 'page', '3', 'bpa', ':', 'children', \"'\", 's', 'age', ':', '6', '-', '10', 'years', ',', 'female', '11', '-', '17', 'sex', ':', '0', '.', '9', '%', '.'], ['bolin', 'later', 'founded', 'video', 'production', 'company', 'mrf', 'and', 'produced', 'videos', 'for', 'bobbi', 'griff', ',', 'janessa', ',', 'pop', 'singer', 'michael', 'arriaga', ',', 'travis', '\"', 'teddy', 'and', 'jerry', '\"', 'davis', 'and', 'michel', 'barbieri', '.'], ['also', 'in', 'the', 'picture', 'are', '(', 'some', ')', 'commercial', 'cartoons', ',', 'where', 'boats', 'deliver', 'a', 'garbage', 'pail', 'for', 'karps', 'bay', '(', 'the', 'harbour', ')', 'and', 'to', 'the', 'port', 'for', 'the', 'same', 'reason', 'as', 'the', 'earlier', 'examples', '.'], ['the', 'international', 'aquatic', 'center', 'housing', 'fishermen', 'from', 'port', 'of', 'delaware', 'and', 'the', 'port', 'of', 'brooklyn', 'hosts', 'a', 'pajama', 'boat', 'shop', 'for', 'boats', 'along', 'the', 'new', 'york', 'harbor', 'shoreline', ',', 'including', 'the', 'washington', '-', 'delmarva', 'ferry', '.'], ['along', 'with', 'these', ',', 'matha', ',', 'eac', 'christian', 'church', ',', 'evangelical', 'church', 'of', 'toronto', ',', 'community', 'of', 'god', 'evangelical', 'christian', 'church', ',', 'church', 'of', 'our', 'lady', 'and', 'the', 'church', 'of', 'our', 'lord', 'prayer', 'are', 'the', 'principal', 'churches', '.'], ['\"', 'actually', 'it', 'is', '!', '\"', '[', 'gossew', 'describes', ']', 'the', 'original', 'version', 'as', '\"', 'over', 'fifty', 'years', 'old', ',', '\"', '[', 'etc', '.', ']', '\"', 'charming', '.', '\"', 'the', 'basic', 'hook', 'and', 'ending', 'are', 'also', 'discussed', '.'], ['yes', ',', 'but', 'the', 'question', 'remains', '-', '...', 'how', 'come', 'a', 'young', 'woman', 'was', 'not', 'involved', 'in', 'the', 'conflict', 'and', 'had', 'received', 'treatment', 'at', 'a', 'mental', 'health', 'treatment', 'center', 'which', '[', 'she', ']', 'subsequently', 'did', 'not', 'latch', 'on', 'to', '?'], ['they', 'were', 'the', 'only', 'monsters', 'out', 'there', ':', 'the', 'kind', 'either', 'living', 'dead', 'or', 'dying', 'of', 'fright', '.', 'she', 'thought', ',', 'stay', 'calm', ',', 'stay', 'calm', ',', 'stay', 'calm', ',', 'remember', ',', 'remember', 'this', ',', 'how', 'queensland', 'had', 'died', '.'], ['later', 'in', 'the', 'same', 'year', 'it', 'was', 'nominated', 'again', 'for', \"'\", 'ananda', 'literary', 'award', \"'\", '.', 'the', 'new', 'delhi', 'edition', 'is', 'owned', 'by', 'ananda', 'publications', 'limited', '(', 'apart', 'from', 'earlier', 'editions', 'owned', 'by', 'ananda', 'publications', ')', '.'], ['(', 'also', 'known', 'as', 'tactical', 'observer', '.', 'airborne', '.', ')', 'the', 'rf', '-', '525t', \"'\", 's', 'designation', 'is', 'automatically', 'renounced', 'as', 'airborne', '-', 'control', 'equipment', ',', 'either', 'by', 'the', 'manufacturer', 'or', 'through', 'the', 'manufacturer', 'itself', '.'], ['both', 'in', 'variety', 'and', 'on', 'center', 'stage', 'in', '1933', 'and', '1934', '.', 'down', 'the', 'street', 'was', 'money', ',', 'inc', '.', 'and', 'the', 'supper', 'club', '(', 'performing', 'arts', 'association', '#', '51', 'art', 'show', ')', 'is', 'an', 'example', 'of', 'this', 'work', ';'], ['\"', 'not', 'bad', '.', '\"', 'later', ',', 'in', 'the', 'hallway', ',', 'burbank', 'says', '\"', 'goody', '\"', 'and', 'points', 'out', ',', '\"', 'goody', 'is', 'nearly', 'finished', '.', 'he', 'can', 'see', 'where', '[', 'it', ']', 'is', '\"', '.'], ['she', 'had', 'to', 'find', 'a', 'job', '.', 'she', 'had', 'to', 'call', 'job', 'and', 'tell', 'him', 'they', 'weren', \"'\", 't', 'doing', 'absolutely', 'nothing', '!', \"'\", 'come', 'on', 'mom', '!', '\"', '(', 'jake', ')', 'carter', '!', 'enough', 'of', 'this', 'girl', 'shit', '.'], ['while', 'in', 'high', 'school', ',', 'brown', 'appeared', 'in', 'several', 'comics', 'including', 'the', 'statue', 'of', 'liberty', ',', 'the', 'war', 'beast', ',', 'in', 'real', 'life', 'the', 'quest', 'for', 'liberty', ',', 'my', 'wild', 'days', ',', 'nightmare', '#', '8', ',', 'and', 'star', 'wars', '.'], ['in', 'the', 'anime', '(', '2', 'characters', ')', ',', 'the', 'model', 'is', 'a', 'very', 'good', 'model', 'for', 'another', 'adventure', 'point', 'where', 'she', 'happily', 'fights', 'shiva', '(', 'due', 'mainly', 'to', 'fear', ')', 'and', 'helps', 'him', 'make', 'the', 'so', 'called', 'new', 'alien', '.'], ['(', 'chapter', 'iii', ':', 'scientific', 'expeditions', 'to', 'europe', ',', 'africa', 'and', 'the', 'united', 'states', ';', 'in', 'mexico', ',', 'brazil', ',', 'and', 'texas', ')', 'these', 'expeditions', 'were', 'carried', 'out', ',', 'from', '1862', 'to', '1864', ',', 'the', '\"', 'scientific', 'expedition', '.', '\"'], ['nacho', ',', 'his', 'brothers', 'mariana', 'and', 'agustin', ',', 'are', 'playing', 'mosqueira', 'with', 'them', 'in', 'el', 'palomino', '.', 'the', 'team', 'will', 'have', '\"', 'juanes', '\"', 'to', 'aunt', 'martha', ',', 'mary', ',', 'as', 'mariana', 'marries', 'agustin', '.'], ['she', 'is', 'a', 'strong', 'girl', 'like', 'the', 'sisters', 'of', 'man', '-', 'zero', 'and', 'she', 'meets', 'jack', '.', 'teddy', 'mcguire', ':', 'the', 'teenage', 'son', 'and', 'daughter', 'in', 'foster', 'care', ',', 'who', 'attends', 'jacob', 'academy', '.', 'charlie', 'mcguire', ':', 'charlie', 'mcguire', '.'], ['\"', 'although', 'perhaps', 'she', 'was', 'not', 'an', 'actress', ',', 'she', 'was', 'a', 'figment', 'of', 'english', 'society', ',', 'an', 'actual', 'performer', '.', '\"', 'oh', ',', 'with', 'anthony', 'and', 'his', 'men', '-', 'bailiff', ',', 'gibbon', ',', 'and', 'others', '.'], ['the', 'forest', 'is', 'used', 'as', 'a', 'nesting', 'spot', 'for', 'birds', 'on', 'the', 'main', 'island', '.', 'it', 'was', 'also', 'used', 'as', 'a', 'breeding', 'site', 'by', 'the', 'maltese', 'government', 'in', '2015', '.', 'the', 'forest', 'was', 'declared', 'a', 'terra', 'biosphere', 'reserve', '.'], ['minimeg', 'introducing', 'the', 'measures', 'were', 'tri', '-', 'state', '(', 'civiii', ')', '(', 'proposals', 'to', 'oversee', 'development', ')', ',', 'college', ',', 'county', '(', 'prior', 'to', 'the', 'initial', 'state', '-', 'owned', '\"', 'home', '\"', ')', 'and', 'city', '.'], ['the', 'philosophical', 'research', 'society', '.', 'k', '.', 'e', '.', 'm', '.', 'ar', '.', ',', 'vol', '12', ',', 'no', '3', '(', '1999', ')', ':', '5', '-', '16', '.', '\"', 'new', 'testament', 'of', 'our', 'professor', 'michael', 'd', '.', 'hewes', '\"', '.'], ['i', 'made', 'him', 'pay', 'for', 'being', 'foolish', '.', '\"', 'rhein', 'had', 'come', 'to', 'his', 'senses', 'now', '.', 'he', 'resigned', 'his', 'position', 'to', 'become', 'an', 'illustrator', 'for', 'economy', 'magazine', '.', 'these', 'memories', 'were', 'of', 'love', 'and', 'friendship', '.'], ['however', ',', 'he', 'criticized', 'the', 'title', 'as', 'containing', 'poor', 'gameplay', ':', '\"', 'essentially', 'nothing', 'had', 'gone', 'wrong', '.', 'previous', 'entries', 'had', 'been', 'a', 'lot', 'more', 'expensive', ',', 'but', 'they', 'seemed', 'the', 'same', 'for', '[', '...', '3', ']', 'reasons', '\"', '.'], ['william', 'and', 'agnes', 'flees', 'scotland', ',', 'leaving', 'margaret', 'and', 'james', 'orphans', ':', 'elizabeth', 'stewart', 'of', 'ballaloe', 'and', 'newry', ',', 'possibly', 'with', 'her', 'husband', ',', 'the', '14th', 'earl', 'of', 'tuam', 'bodhran', 'and', 'loch', 'ness', '.'], ['\"', 'buffalo', 'bill', '\"', '(', 'from', 'the', 'movie', 'rawkus', ')', ',', 'from', 'the', 'long', '-', 'running', 'hit', 'television', 'show', '.', 'christopher', 'franklin', '(', 'toby', ')', 'daggett', ';', 'mit', 'professor', 'dr', '.', 'gregory', 'dewitt', ',', 'editor', '.'], ['source', ':', '(', 'as', 'a', '\"', 'reserve', '\"', 'side', ')', 'note', ':', 'both', 'the', 'founding', 'chairman', 'and', 'the', 'past', 'board', 'chairman', ',', 'ian', 'anderson', ',', 'played', 'for', 'castlemahon', '.', 'source', ':', '(', 'c', ')', 'champions', 'in', 'bold', '.'], ['§', '2', '-', '4', '(', 'williams', '1987', ')', '\"', 'detailed', 'survey', 'map', 'of', 'the', 'american', 'country', 'in', '1825', 'and', 'of', 'fort', 'hood', 'in', '1829', '(', 'fort', 'hood', ',', 'arkansas', '1978', ')', '\"', 'american', 'historical', 'review', 'vol', '2', ',', 'no', '.'], ['am', 'antilhete', 'athenie', '(', ',', 'the', 'glory', 'of', 'her', ')', '.', '.', '.', '.', '.', '(', ',', 'the', 'life', 'prepared', 'for', 'her', ')', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.'], ['archetype', '(', 'quests', ')', ':', 'without', 'self', '-', 'knowledge', ',', 'thinking', 'is', 'regarded', 'as', 'thinking', ',', 'whereas', 'memory', 'is', 'unconscious', '.', 'memory', ':', 'the', 'part', 'of', 'the', 'body', 'which', 'produces', 'the', 'magic', 'which', 'leads', 'light', 'on', '.'], ['person', ',', 'friend', '(', 'x', '-', 'y', ')', ';', 'relationship', ',', 'familiar', '(', '-', 'x', ')', ';', 'second', 'mate', '(', '-', 'y', ')', ';', 'esp', '.', ',', 'couple', ',', 'friend', 'with', 'partner', ';', 'instance', ',', 'one', 'between', 'friends', ';'], ['and', 'now', 'he', 'beckoned', 'me', '.', '\"', '(', 'part', '3', ')', 'riff', 'raff', 'sang', 'the', 'final', ',', '\"', 'on', 'the', 'train', '\"', 'savage', 'lord', ',', 'savage', 'lady', ',', 'savage', 'lover', ',', '\"', 'but', 'his', 'voice', 'was', 'dry', '.'], ['[UNK]', '[UNK]', '220', 'm', '/', '[UNK]', '~', '[UNK]', '/', '[UNK]', '[UNK]', '[UNK]', '220', 'm', ';', '[UNK]', 'and', '[UNK]', 'have', 'misrepresented', 't', '-', '3', '.', '0', 'and', 't', '-', '3', '.', '9', '.', '0', 'for', 'the', 'lyrics', '.'], ['nominated', 'for', '\"', 'thicker', 'than', 'water', '\"', 'soundtrack', '.', 'soundtrack', 'single', ':', '\"', 'leave', 'out', 'of', 'the', 'room', '\"', 'performed', 'by', 'richard', 'marx', 'and', 'the', 'ventures', '.', 'additional', 'nominations', ':', 'outstanding', 'jazz', 'vocal', 'choir', 'for', 'this', 'is', 'your', 'life', '.'], ['daniel', 'l', '.', 'herrera', '.', 'editors', '2003', 'and', '2004', '.', 'ninos', 'sagrados', '(', 'ninos', ')', 'sunday', ',', 'roman', 'catholic', 'archdiocese', 'of', 'pangasinan', '-', 'san', 'fernando', ',', 'gracia', 'gutierrez', '-', 'san', 'juan', '.'], ['2013', ',', 'bizipdocspyrx', '.', '2013', ',', '\"', 'twista', 'twista', 'style', '\"', 'makazie', '.', '2013', ',', '\"', 'diy', 'diy', 'you', 'diy', '\"', 'featuring', 'absolom', '.'], ['tim', 'graham', ':', 'mayor', 'of', 'tyler', ',', 'and', 'an', 'executive', 'director', 'tyler', 'chamber', 'of', 'commerce', '.', 'chris', 'smith', '(', 'caruana', ',', 'ca', ')', ':', 'indie', 'punk', 'musician', 'who', 'runs', 'his', 'private', 'beach', 'resort', ',', 'la', 'galalga', '.'], ['she', 'is', 'not', 'just', 'beautiful', ',', 'but', 'she', 'has', 'some', 'kind', 'of', 'aura', ',', 'because', 'not', 'with', 'even', 'her', 'eyes', '.', 'even', 'in', 'my', 'mind', 'i', 'just', 'think', 'of', 'people', ',', 'of', 'her', ',', 'of', 'ministers', 'being', 'looked', 'over', '.'], ['david', 'jones', 'composed', 'the', 'music', 'for', 'walkinshaw', 'and', 'the', 'duchess', 'at', 'the', 'royal', 'national', 'theatre', ',', 'conducted', 'by', 'christopher', 'fry', '.', 'he', 'was', 'the', 'seaman', '/', 'photographer', ':', 'national', 'geographic', 'explorer', ',', 'played', 'by', 'george', 'mraz', '.'], ['next', 'there', 'is', 'another', 'antiphon', 'where', 'an', 'alto', 'saxophonist', 'and', 'a', 'flutist', ',', 'juan', 'carlos', 'garcia', '(', 'arr', '.', ')', ',', 'play', 'the', 'first', 'bandidos', 'of', 'el', 'pilar', 'y', 'el', 'kultura', '.'], ['for', '1', 'is', 'not', 'one', 'of', '(', 'non', '-', 'zero', 'n', ')', 'then', 'there', 'is', 'no', 't', 'such', 'that', 'n', 'is', 'always', 'still', '0', '.', 'now', ',', 'now', 'let', 'be', 'any', 'number', 'which', 'is', 'not', 'always', '\"', '1', '\"', '.'], ['the', 'bodies', 'of', 'maggie', 'and', 'matt', 'appear', 'in', 'these', 'images', 'of', 'maggie', 'and', 'matt', 'tacitally', ',', 'figuratively', '.', 'both', 'were', 'strongly', 'influenced', 'by', 'mount', 'diablo', ',', 'the', 'resting', 'place', 'of', 'the', 'star', ',', 'and', 'raine', '.'], ['a', 'special', 'service', 'on', 'saturdays', 'was', 'arranged', 'by', 'bch', ',', 'from', 'where', 'some', 'drivers', '(', 'over', 'the', 'previous', 'two', 'years', ',', 'including', 'passengers', \"'\", ')', 'persuaded', 'this', 'to', 'share', 'the', 'rennie', 'island', 'branch', 'track', 'of', 'other', 'services', '.'], ['the', 'originals', 'are', 'a', 'mysterious', 'team', 'of', 'police', 'that', 'take', 'on', 'an', 'anubis', '-', 'like', 'vixen', 'in', 'the', 'mighty', 'chum', 'form', 'of', 'spider', '-', 'man', 'and', 'on', 'the', 'emerging', 'crisis', 'of', 'retribution', 'for', 'doom', '.'], ['below', 'them', ',', 'the', 'demons', 'sang', 'every', 'station', ',', 'mission', '...', 'missions', ',', 'like', 'styx', 'and', 'brock', 'and', 'brock', 'and', 'kade', ',', 'like', 'alex', 'and', 'tavia', 'and', 'sebastian', '.', 'who', 'knew', ',', 'centuries', 'after', 'that', ',', 'fires', 'burned', '.'], [\"'\", 'there', 'will', 'never', 'be', 'anyone', 'i', 'want', 'to', 'commit', 'to', '.', \"'\", \"'\", 'other', 'plans', 'pick', 'up', ',', \"'\", 'he', 'said', 'kindly', '.', \"'\", 'thank', 'you', '[', 'again', ']', '.', \"'\", 'he', 'hung', 'up', 'again', 'with', 'a', 'sigh', '.'], ['simon', 'tipton', '-', 'new', 'reporter', ',', 'a', 'special', 'consultant', 'on', 'the', 'programme', 'who', 'worked', 'for', 'tipton', '.', 'at', 'the', 'bottom', 'of', 'the', 'hole', '(', 'tv', 'serial', ')', '-', 'sam', 'cara', 'had', 'reporting', 'to', 'channel', 'four', 'to', 'report', '.'], ['the', 'term', 'was', 'coined', 'through', 'twitter', 'by', 'both', 'bob', 'marley', 'and', 'his', 'father', ',', 'michael', 'marley', 'who', 'called', 'krangl', 'mackey', 'the', '\"', 'forefather', ',', 'guru', ',', 'or', 'spiritual', 'fawn', 'she', 'has', 'encountered', '\"', '.'], ['new', 'zealand', 'actor', 'gavin', 'mapes', 'is', 'a', 'producer', ',', 'and', 'mapes', 'had', 'worked', 'as', 'a', 'producer', 'for', 'the', 'team', 'in', 'this', 'earliest', 'picture', ',', 'though', 'two', 'australian', 'actors', 'had', 'contributed', 'the', 'first', ';', 'portrayed', 'by', 'martin', 'phillips', '.'], ['she', 'was', 'the', 'one', 'who', 'ignored', 'all', 'the', 'tests', 'luke', 'gave', 'her', '(', 'in', 'denial', 'and', 'truth', '-', 'to', '-', 'date', ')', ',', 'even', 'though', 'she', 'and', 'mac', 'back', 'had', 'always', 'been', 'known', 'for', 'their', 'beer', '-', 'raising', 'schemes', '.'], ['1907', '.', \"'\", 'the', 'hypodermic', 'needle', \"'\", 'magazine', '33', '(', 'journal', 'on', 'surgery', 'and', 'medical', 'physics', ')', ':', '1', '-', '16', 'eastlake', ',', 'w', '.', 't', '.', '.', '1904', '.', '\"', 'aspects', 'of', 'surgery', '\"', '.'], ['the', 'declaration', 'of', 'unity', 'was', 'opposed', 'by', 'the', 'liberal', 'community', 'and', 'by', 'those', 'who', 'wrote', 'or', 'illegally', 'sent', 'pamphlets', 'to', 'other', 'nations', '.', 'national', 'liberal', 'party', 'deputy', 'leader', 'and', 'senator', 'william', 'gomms', 'johnson', 'maintained', 'nuc', 'favour', '.'], ['he', 'played', 'as', 'goalkeeper', 'in', 'six', 'clubs', 'in', 'costa', 'rica', 'in', 'four', 'years', ':', 'costa', 'rica', 'universidad', 'de', 'la', 'cordova', ',', 'usa', '(', 'usa', ')', ',', 'numero', ',', 'fondo', 'nacional', 'and', 'costa', 'rica', 'tilde', '.'], ['each', 'tender', 'had', 'a', 'cooling', 'system', '.', 'the', 'tenders', 'were', 'steam', '-', 'powered', '/', 'diesel', '-', 'powered', 'and', 'ran', 'as', 'stock', 'trains', '.', 'each', 'locomotive', 'was', 'held', 'in', 'a', 'pit', 'containing', 'almost', '10', 'million', 'tonnes', 'of', 'iron', 'ore', '.'], ['below', ',', 'above', ';', '(', 'above', ',', 'in', 'hebrew', ',', 'meaning', '\"', 'i', 'must', 'be', 'there', '\"', ')', 'or', '\"', 'to', 'be', 'off', ';', 'o', 'brother', '.', '\"', 'rabbinical', 'compound', 'jerusalem', '/', 'teferria', 'in', 'los', 'angeles', ';'], ['(', 'irc', ')', '.', 'mike', 'distler', 'as', 'jake', 'ryan', '(', 'week', '6', ')', 'mike', 'heseltine', 'as', 'paul', 'wilson', '(', 'football', 'hall', 'of', 'fame', ')', '.', '.', 'jim', 'foley', '(', 'week', '7', ')', '.', '.', '.'], ['as', 'a', 'consequence', ',', 'the', 'malaysia', 'case', 'introduced', 'six', 'years', 'earlier', 'also', 'found', 'that', 'differences', 'in', 'policy', 'and', 'practice', 'weighed', 'on', 'one', 'another', 'from', 'the', 'introduction', 'of', 'the', 'quota', 'system', 'in', 'schools', ',', 'as', 'did', 'the', 'new', 'zealand', 'case', '.'], ['ani', 'martin', 'is', 'hired', 'at', 'her', 'website', 'imdb', 'for', 'pet', 'after', 'adoption', ',', 'and', 'also', 'at', 'her', 'website', 'to', 'house', 'pictures', '.', 'for', 'example', ',', 'other', 'notable', 'fans', 'for', 'pet', 'after', 'adoption', 'include', 'robin', 'williams', 'or', 'pet', '!'], ['cissy', 'and', 'mike', 'run', 'as', 'manuel', 'repairs', 'his', 'broken', 'leg', ',', 'before', 'rushing', 'down', 'the', 'stairway', 'and', 'catching', 'a', 'fallen', 'manuel', '.', 'there', ',', 'they', 'find', 'david', 'fighting', 'manuel', 'with', 'everything', 'he', 'has', ',', 'while', 'manuel', 'attacks', 'manuel', '.'], ['digital', 'drawing', 'robots', '(', 'dms', ')', 'mechanical', 'drawing', 'robots', ',', 'intended', 'for', 'automatic', 'drawing', ',', 'robotic', 'animations', ',', 'backgrounds', 'or', 'textures', ',', 'first', 'intended', 'for', 'rendering', '.', 'they', 'have', 'additional', 'features', 'multiscale', 'and', 'dft', '.'], ['is', '15', '-', '15', 'and', 'finished', 'in', '7th', 'place', '.', 'sam', 'davis', '-', 'released', 'from', 'minnesota', 'and', 'wftda', 'member', 'eligibility', 'in', '1977', '.', 'peter', 'collins', '-', 'headed', 'the', 'big', 'ten', 'in', '1976', 'and', 'north', 'america', 'in', '1975', ';'], ['while', 'at', 'mutv', 'she', 'was', 'an', 'executive', 'creative', 'associate', 'on', 'two', 'seasonal', 'episodes', ',', '\"', 'who', 'is', 'in', 'the', 'futon', '?', '\"', 'and', '\"', 'swapping', 'identities', ':', 'the', 'complete', 'ghost', 'story', 'for', 'the', 'first', 'time', '\"', '.'], ['\"', 'they', 'have', 'the', 'undead', 'dead', 'in', 'the', 'police', 'compound', ',', '\"', 'said', 'caleb', ',', 'but', 'he', 'could', 'not', 'read', 'the', 'words', 'from', 'caleb', \"'\", 's', 'mouth', '.', 'maybe', 'they', 'could', 'find', 'out', 'more', 'about', 'grish', 'on', 'abby', '.'], ['the', 'show', 'featured', 'hollywood', 'celebrities', 'and', 'artists', 'singing', 'songs', 'such', 'as', 'america', 'and', 'europe', '(', 'from', 'all', 'good', 'and', 'this', 'is', 'god', ')', ',', 'which', 'included', 'chavo', 'performing', 'the', 'beatles', \"'\", '\"', 'hey', 'boy', ',', 'hey', 'boy', '\"', '.'], ['33', ',', '(', '[UNK]', ';', 'ed', '.', 'yeong', '-', 'soon', 'lee', ',', '2008', ')', '(', '[UNK]', ';', 'ed', '.', 'gordon', '&', 'castle', ',', '2012', ')', 'wheel', 'of', 'gold', ',', '(', 'pantheon', 'books', ')', '(', '1992', ')', ',', 'p', '.'], ['m', '(', 'n', ')', '=', '1', ',', '(', 'p', '=', '1', ')', 'p', '(', 'n', ')', ',', '(', 'p', 'is', '1', ')', ';', 'thus', 'a', 'complex', 'number', 'p', 'has', 'an', 'inverse', 'when', 'he', 'means', '(', '1', ')', '=', '0', ';'], ['[', 'rev', '.', 'alexander', 'james', 'scott', 'robinson', ',', '1856', '-', '26', 'november', '1877', ']', 'assists', ',', 'supervises', 'and', 'directs', 'education', 'for', 'the', 'inland', 'empire', '.', 'this', 'college', 'is', 'roman', 'catholic', 'and', 'has', 'been', 'officially', 'named', 'after', 'general', 'robinson', '.'], ['bowman', ',', 'h', '.', ';', 'hill', ',', 'h', '.', ';', '&', 'hutchison', ',', 'james', 'r', '.', 'c', '.', ',', 'who', 'worked', 'on', 'the', 'sacred', 'heart', 'branch', 'railway', '.', 'bowman', ',', 'george', 'h', ';', 'c', '.', 'aydara', ';'], ['it', 'dedicates', 'the', '\"', 'high', '-', 'jazz', 'american', 'movement', '\"', 'by', 'combining', 'jazz', 'fusion', 'and', 'mimicry', ',', 'as', 'showcased', 'in', 'the', 'post', '-', 'punk', 'rocker', 'michael', 'jackson', 'and', 'the', 'jazzy', 'pop', 'music', 'of', 'the', '1970s', '.'], ['gsm', 'vision', 'and', 'transcendent', 'vision', ':', 'for', 'devices', 'like', 'the', 'smartphone', '.', 'indoor', 'and', 'outdoor', 'vision', ':', 'for', 'broadband', ',', 'local', 'area', 'network', ',', 'ip', 'line', ',', 'home', 'cellular', 'and', 'other', 'gsm', 'devices', 'are', 'available', '.'], ['the', 'monsters', 'ate', 'a', 'bluder', 'to', 'cover', 'their', 'dirty', 'pale', 'pale', 'faces', '.', 'short', 'swords', 'like', 'the', 'tip', 'of', 'a', 'spittoon', 'ate', 'the', 'dust', 'of', 'battle', ',', 'but', 'there', 'were', 'none', 'under', 'the', 'stone', 'wall', 'above', '.'], ['rabbit', ',', 'the', 'headless', 'snake', 'wearing', 'a', 'crooked', 'sunroof', 'pole', '.', 'buster', ',', 'the', 'forked', 'snake', 'at', 'his', 'head', '.', '...', 'dad', ',', 'an', 'asthmatic', '.', '...', 'they', 'picked', 'up', 'markie', 'bull', 'moose', ';'], ['they', 'were', 'probably', 'about', 'fifty', 'yards', 'away', 'and', 'very', 'close', '.', 'surely', 'the', \"'\", 'parting', 'shot', \"'\", 'took', 'them', 'far', ',', 'but', 'when', 'shots', 'were', 'fired', ',', 'it', 'was', 'like', 'someone', 'prodding', 'them', 'every', 'time', 'their', 'partner', 'shot', '.'], ['on', 'occasion', 'he', 'will', 'adopt', 'lucy', 'as', 'his', 'pet', ',', 'although', 'lucy', 'could', 'be', 'very', 'angry', 'she', 'has', 'either', 'of', 'the', 'form', 'of', 'an', 'angel', '.', 'he', 'is', 'also', 'often', 'arrogant', 'and', 'rude', '.', 'lucy', 'is', 'warm', 'and', 'friendly', ';'], ['the', 'initial', 'study', 'cost', 'about', '$', '50', 'million', '.', 'the', 'list', 'of', 'projects', 'referred', 'by', 'the', 'epa', 'has', 'included', 'requirements', 'for', 'oil', 'and', 'gas', 'in', 'underground', 'wells', '(', 'nicknamed', '\"', 'streams', '\"', ')', 'as', 'well', 'as', 'natural', 'resources', 'use', '.'], ['why', '?', '\"', '\"', 'the', 'gurkha', '.', 'there', 'are', 'three', 'or', 'four', 'simple', 'means', 'of', 'splitting', 'the', 'animals', 'up', '.', 'the', 'total', 'size', 'of', 'a', 'single', 'forest', 'is', 'to', 'average', 'right', 'next', 'to', 'the', 'creatures', 'in', 'force', '.'], ['there', 'is', 'only', 'one', 'character', '.', 'she', 'is', 'a', 'carefree', 'girl', 'played', 'by', 'anna', 'manyanska', '.', 'the', 'young', 'cast', 'is', 'still', 'from', 'all', 'the', 'days', 'of', 'revolution', 'and', 'everyone', 'is', 'decimated', 'by', 'the', 'other', 'half', '.'], ['timothy', 'close', '-', 'son', ',', 'william', 'close', '-', 'son', ';', 'dianna', 'close', ';', 'stephen', 'close', ';', 'complete', 'list', ',', 'includes', ':', 'cornelius', 'close', '-', 'nephew', ';', 'bing', 'close', '-', 'brother', ';', 'complete', 'list', ',', 'includes', ':', 'charles', 'close', ';'], ['he', 'was', 'also', 'the', 'oldest', 'of', 'the', 'african', '-', 'american', 'commanders', 'in', 'command', ',', 'who', 'landed', 'at', 'camp', 'pima', 'as', 'part', 'of', 'the', 'amphibious', 'operations', 'of', 'force', 'west', '4', '/', '5', 'and', 'airborne', 'in', 'august', 'to', 'capture', 'japan', '.'], ['see', 'cers', 'chemistry', '.', '(', 'see', 'mohs', 'chemistry', 'for', 'illustration', '.', ')', 'cerium', '(', 'iii', ')', '(', 'cers', ')', 'forms', 'from', 'k70b₆', '(', 'k70a₆', ')', 'as', 'a', 'light', 'intermediate', '.'], ['teachers', 'participate', 'in', '(', 'standardized', ')', 'test', 'testing', 'including', 'math', ',', 'reading', ',', 'and', 'writing', 'labs', ',', 'which', 'focuses', 'on', 'identifying', 'high', 'school', 'students', 'in', 'school', 'as', 'being', 'eight', 'years', '(', 'often', ')', '\"', 'when', 'they', 'were', 'ten', '\"', '.'], ['\"', 'openness', 'to', 'emotions', 'and', 'friendliness', ',', 'a', 'film', '-', 'sharper', 'place', 'in', 'the', 'history', 'of', 'film', 'and', 'television', ',', '\"', 'yale', 'history', 'review', '.', '1999', '.', '\"', 'who', \"'\", 's', 'artie', '?', '\"', 'ed', '.'], ['based', 'on', 'rough', 'cuts', 'of', 'version', 'for', 'tv', ',', 'with', 'on', '-', 'set', 'footage', 'recorded', 'by', 'harrison', 'along', 'with', 'the', 'costume', 'and', 'voice', 'changes', ',', 'it', 'was', 'interlaced', 'to', '\"', 'the', 'knack', '\"', 'by', 'barry', 'allen', '.'], ['b', '.', 's', '.', 'smith', 'and', 'davis', 'caught', 'fire', 'and', 'closed', 'in', ',', 'claiming', 'the', 'victory', ',', 'but', 'gave', 'out', 'two', 'penalty', 'minutes', 'later', '(', 'may', 'admitted', 'guilt', ')', ',', 'davis', '\"', 'really', 'deranged', \"'\", 'again', '\"', '.'], ['3', '(', '20', ')', ',', 'retrieved', 'july', '18', ',', '2017', '.', '\"', 'davis', '-', 'mendocino', 'football', 'game', ',', 'pottsville', '2017', ',', '\"', 'memorial', 'bowl', ',', 'the', 'international', 'herald', 'tribune', ',', '2017', '.', 'campbell', ',', 'colin', ';'], ['while', 'sheriff', 'hamilton', 'attended', 'pru', ',', 'he', 'produced', '\"', 'a', 'great', 'monologue', 'that', 'often', 'jumps', 'his', 'speed', '\"', 'with', '\"', 'critical', 'assessments', 'of', 'the', 'world', '\"', 'which', '\"', 'ahead', 'him', 'on', 'a', 'question', 'of', 'the', 'economy', '\"', '.'], ['|', ':', 'et', 'mucium', '(', '|', ')', '|', 'ad', '(', '?', '-', '1st', 'century', 'bce', ')', '|', '|', '|', '(', 'neamt', 'inscriptions', ',', 'c', '.', 'n', '.', ',', '12th', 'century', '-', '1501', 'ce', ')', '?'], ['the', 'first', 'scene', 'is', 'about', '\"', 'zhao', 'shu', 'and', 'lin', 'starts', 'to', '[', 'write', ']', 'their', 'own', 'songs', '\"', '(', 'and', 'lin', 'claimed', 'that', 'this', 'process', '\"', 'worked', 'like', 'a', 'star', '\"', '-', 'it', 'was', 'in', 'full', 'effect', ')', '.'], ['as', 'to', 'how', 'important', 'the', 'question', 'had', 'become', ',', 'he', 'had', 'completed', 'three', 'questions', 'and', 'another', 'five', 'questions', ',', 'each', 'given', 'q', '&', 'a', 'with', 'the', 'topic', ':', '-', 'the', 'psychology', 'of', 'psychology', 'including', 'politics', ',', 'science', 'and', 'philosophy', '.'], ['the', 'ancestors', 'of', 'the', 'dynasty', 'were', 'alfonso', 'v', ',', 'queen', 'isabella', 'of', 'aragon', ',', 'alfonso', 'vi', ',', 'or', 'perhaps', 'neighboring', 'aragon', 'and', 'perhaps', 'possibly', 'portugal', '.', 'spain', 'is', 'an', 'andalusian', 'state', 'with', 'its', 'own', 'sovereign', 'government', '.'], ['but', 'still', '...', 'it', 'felt', 'like', 'a', 'gift', '.', 'maybe', 'a', 'big', 'potato', 'salad', 'and', 'a', 'piece', 'of', 'apple', 'pie', '...', 'milly', 'glanced', 'up', 'again', 'at', 'the', 'cool', ',', 'stainless', 'steel', 'monitor', 'in', 'the', 'rusk', 'entertainment', 'room', '.'], ['america', 'and', 'the', 'caribbean', 'are', 'the', 'most', 'expensive', 'and', 'important', 'countries', 'in', 'latin', 'america', ',', 'and', 'as', 'you', 'pointed', 'out', ',', 'the', 'hispanic', 'middle', 'class', 'is', 'further', 'down', 'in', 'latin', 'america', '.', \"'\", \"'\", 'and', 'you', 'really', 'believe', 'so', '?'], ['2005', ':', 'forgetmooting', '.', 'the', 'editions', 'le', 'cime', 'of', 'brussels', '2006', ':', 'soriano', 'le', 'capo', 'of', 'naples', '.', '(', 'monographs', ',', 'own', 'series', ')', '.', '2011', ':', '\"', 'destude', '\"', '.'], ['\"', 'first', 'off', ',', 'i', 'nearly', 'blew', 'out', 'the', 'subaltern', '.', '\"', '\"', 'there', 'might', 'still', 'be', 'fire', ',', '\"', 'said', 'ivar', '.', '\"', 'a', 'small', 'explosion', ',', '\"', 'i', 'reported', ',', 'and', 'handed', 'it', 'over', '.'], ['the', '\"', 'locker', 'room', 'champions', '\"', 'included', ':', 'muhammad', 'ali', ',', 'richard', 'burton', '(', 'although', 'burton', 'was', 'only', '4', '′', '15', '-', 'feet', 'tall', 'or', 'was', 'ranked', '11', 'by', 'the', 'early', '1980s', ')', ',', 'jr', '.', ',', 'and', 'themselves', '.'], ['he', 'drank', 'it', 'down', ',', 'swallowed', 'it', '.', 'without', 'a', 'word', ',', 'he', 'took', 'his', 'lemonade', 'with', 'him', '.', 'laughter', 'laced', 'with', 'a', 'warm', 'peace', 'lit', 'the', 'very', 'air', '.', 'aly', 'and', 'i', 'were', 'both', 'enjoying', 'ourselves', '.'], ['\"', 'i', 'was', 'upwind', ',', 'out', 'of', 'their', 'reach', '.', '\"', '\"', 'as', 'you', 'would', '.', 'there', 'were', 'no', 'other', 'humans', '.', 'do', 'your', 'harder', 'work', ',', 'and', 'do', 'what', 'i', 'could', '.', 'do', 'what', 'i', 'could', '.', '\"'], ['weltzer', ',', '(', '2018', '-', '14', ',', '2016', ')', '-', 'drums', 'orkop', ',', '(', '2013', ')', '-', 'guitar', ',', 'lead', 'vocals', 'demos', 'post', '-', 'sessions', 'with', 'rick', 'hall', 'and', 'corey', 'crawford', '(', '?', '?', ')', ';'], ['now', 'playing', 'blues', 'guitar', 'in', 'the', 'changing', 'times', '.', 'pete', 'townshend', ':', 'dia', 'addict', '.', 'mark', 'barker', ':', 'business', 'consulting', ',', 'london', 'spa', '(', 'formerly', 'barclay', \"'\", 's', ')', '.', 'gerard', 'cronin', 'novelist', 'and', 'film', 'director', '.'], ['or', \"'\", 'the', 'son', 'of', 'the', 'king', 'comes', '(', 'again', ')', 'soon', '.', \"'\", 'some', 'names', 'could', 'be', '\"', 'the', 'son', 'of', 'the', 'king', 'too', '\"', ',', '\"', 'man', '\"', ',', '\"', 'god', '\"', ',', 'or', '\"', 'day', '\"', '.'], ['the', 'west', ',', 'back', 'to', 'snelkraut', ':', 'sporting', 'events', 'and', 'tourism', 'in', 'the', 'west', '(', 'pug', ')', '.', 'travel', 'international', '.', 'pacific', 'institute', ',', 'international', 'publication', 'centre', '.', '2005', '\"', 'us', '-', 'china', 'year', '\"', '.'], ['following', 'his', 'failed', 'effort', 'from', 'the', 'vatican', 'to', 'appoint', 'a', 'new', 'bishop', 'as', 'aulacus', 'minor', '(', 'an', 'orthodox', 'bishop', ')', ',', 'critics', 'and', 'supporters', 'like', 'philpot', 'and', 'southall', 'had', 'their', 'funerals', 'held', 'for', 'him', '.'], ['county', '(', 's', ')', 'are', 'uneated', ',', 'including', 'most', 'counties', 'in', 'southern', 'california', 'on', 'hr', ',', 'viewers', 'question', 'why', 'counties', 'are', 'not', 'completely', 'defined', 'and', 'to', 'address', 'the', 'question', 'will', 'have', 'a', 'speech', 'for', 'the', 'first', 'time', 'ever', '.'], [\"'\", '8000h', '...', \"'\", \"'\", 'yeah', '.', \"'\", \"'\", 'he', 'puts', 'the', 'tablet', 'down', 'and', 'points', 'to', 'the', 'screen', '.', \"'\", 'yeah', ',', 'i', 'am', '.', \"'\", \"'\", 'five', '.', \"'\", \"'\", 'six', '?', \"'\", \"'\", 'five', '?'], ['the', 'soundtrack', 'version', 'also', 'inserts', 'two', 'electro', '-', 'dancetronic', 'songs', ',', '\"', 'flash', '\"', 'and', '\"', 'longshot', '\"', 'from', 'the', '18th', '-', 'century', 'tie', '-', 'in', 'series', 'south', 'park', 'season', '1', ':', 'spy', 'radio', 'incident', '.'], ['-', 'robert', 'taylor', 'in', 'a', 'modern', 'world', 'tyranny', 'chapter', '18', ',', 'the', '\"', 'second', 'book', '\"', 'begins', 'with', 'taylor', \"'\", 's', 'cover', 'artwork', 'prior', 'to', 'the', 'event', 'as', 'well', 'as', 'short', '\"', 'gubbit', '\"', 'segments', '.'], ['the', 'main', 'character', 'pierroto', 'venancio', 'tells', 'the', 'story', 'of', 'travellers', 'traversing', 'the', 'city', 'of', 'rome', 'in', 'northern', 'italy', '.', 'the', 'narrator', 'finds', 'out', 'that', ',', 'indeed', ',', 'he', 'is', 'saad', 'from', 'adno', '.'], ['\"', 'every', 'day', '\"', '(', 'words', 'by', 'david', 'hawking', 'and', 'music', 'by', 'annadne', 'hornsby', ')', 'written', 'by', 'joe', 'bazin', '\"', 'the', 'journey', 'to', 'africa', '\"', '(', 'rhapsody', 'in', 'rhythm', 'and', 'blues', 'medley', ')', 'feat', '.'], ['she', 'and', 'ashli', 'watched', 'the', 'white', 'family', 'members', 'in', 'white', 'court', 'gowns', 'look', 'at', 'the', 'cargo', 'hold', '.', '\"', 'briony', 'loves', 'her', 'children', ',', '\"', 'ashli', 'said', ',', 'after', 'she', 'and', 'ken', 'had', 'vacated', '.'], ['in', '2016', ',', 'schweizer', 'and', 'okada', 'also', 'announced', 'in', 'kamen', 'rider', 'v', 'a', 'kind', 'of', 'sequel', 'called', 'kamen', 'rider', 'forever', 'to', 'be', 'with', 'you', '.', 'kamen', 'rider', 'chase', 'has', 'a', 'notable', 'link', 'with', 'benjamin', 'franklin', '.'], ['shakespeare', 'bridge', 'award', '(', 'in', 'the', 'best', 'drama', 'category', 'of', 'the', '2007', '-', '2008', 'akasaka', 'theatre', 'festival', ';', '2007', '-', '2008', ')', '2003', '(', 'hot', 'dog', ',', 'a', 'succulent', 'wafer', ',', '2011', ')', ':', 'winner', ';'], ['in', 'jimmy', 'kelly', ':', 'the', 'complete', 'biography', ',', 'elizabeth', 'j', '.', 'murphy', 'concedes', 'this', 'is', 'an', '\"', 'account', 'of', 'his', 'early', 'years', 'in', 'great', 'britain', ',', 'where', 'he', 'was', 'named', 'jimmy', 'kelly', 'following', 'an', 'aviation', 'accident', '\"', '.'], ['the', 'dundurn', 'jazz', 'festival', 'is', 'held', 'annually', 'from', 'july', '2018', '.', 'patrick', 'henry', 'and', 'the', 'lakers', 'attend', 'this', 'festival', 'and', 'exclusively', 'welcomes', 'the', 'musicians', 'who', 'come', 'to', 'the', 'performing', 'venue', '(', 'especially', 'the', 'recording', 'equipment', ')', '.'], ['at', 'that', 'moment', ',', 'pacific', 'inc', '.', 'well', '...', 'have', 'you', '?', 'and', 'bette', 'davis', 'suggested', 'we', 'teach', 'you', 'proper', 'manners', '.', 'the', 'guy', 'who', 'kneads', 'you', 'is', 'a', 'barista', '...', 'nick', ',', 'back', 'off', '!'], ['one', 'review', 'describes', 'the', 'led', 'zeppelin', 'sound', 'as', 'straightforward', ',', 'while', 'another', 'describes', '\"', 'little', 'things', '\"', 'as', 'a', 'kind', 'of', 'song', 'of', 'the', 'choosy', 'style', ',', 'which', 'removes', 'the', 'dregs', 'of', '\"', 'alive', '\"', '.'], ['frank', 'sinatra', ',', 'nicolas', 'cage', ',', 'james', 'l', '.', 'mitchell', ',', 'sammy', 'cahn', '(', 'james', 'l', '.', 'mitchell', 'iii', ')', ',', 'brannon', 'lincoln', '-', 'nichols', 'and', 'frank', 'sinatra', 'created', 'the', 'idea', 'of', '\"', 'the', 'sinatra', 'story', '\"', '.'], ['you', 'can', 'crash', 'off', 'the', 'side', 'at', 'the', 'stoplight', ',', 'like', 'a', 'stoplight', '.', '\"', 'he', 'turns', 'back', 'around', 'and', 'looks', 'back', 'up', 'at', 'the', 'last', 'locked', 'gate', '.', 'this', 'one', 'is', 'reset', 'as', 'the', 'fourth', 'gate', '.'], ['players', 'go', 'through', 'a', 'number', 'of', 'levels', 'called', 'by', 'their', 'choice', 'and', 'choose', 'to', 'focus', 'on', 'or', 'focus', 'on', 'one', ':', 'silent', '.', 'the', 'student', 'kills', 'silent', ',', 'is', 'a', 'game', 'which', 'can', 'be', 'silent', 'but', 'can', 'be', 'silent', '.'], ['i', 'am', 'sure', 'she', 'is', 'having', 'a', 'poppet', 'in', 'the', 'nursery', '.', 'we', 'are', 'just', 'worried', 'about', 'her', '.', '\"', '\"', 'so', 'in', 'your', 'opinion', ',', 'we', 'are', 'fighting', 'a', 'correlative', 'solution', '?', 'just', 'awkward', 'language', '.'], ['and', 'she', 'really', 'liked', 'them', '.', '\"', 'do', 'you', 'like', 'your', 'barbies', '?', '\"', 'she', 'was', 'thirsty', ',', 'and', 'starving', '.', '\"', 'because', 'i', 'do', 'like', 'all', 'my', 'barbies', ',', '\"', 'she', 'said', ',', 'very', 'much', 'afraid', '.'], ['dorothy', 'and', 'harry', ',', '(', '1937', ')', ',', 'are', 'married', '(', 'under', 'the', 'light', 'of', 'the', 'setting', 'sun', ')', 'and', 'never', 'meet', 'again', 'in', 'a', 'rented', 'deserted', 'motel', '.', 'after', 'a', 'few', 'rings', ',', 'cecily', 'and', 'bernard', 'reunite', '.'], ['black', 'x', 'color', 'hidden', 'mode', '.', 'red', 'x', 'a', 'rectangle', 'of', 'screen', 'simulating', 'hidden', 'mode', '(', 'kombi', 'mode', ')', ',', 'adding', 'more', 'depth', 'to', 'the', 'game', 'while', 'providing', 'more', 'depth', 'over', 'other', 'modes', 'in', 'chess', '.'], ['oh', 'yes', ',', 'we', 'are', 'providing', 'prompt', 'answers', 'to', 'questions', 'of', 'what', '\"', '(', 'see', 'below', ')', '...', '\"', '=', '.', 'oh', 'yes', ',', 'we', 'have', ',', 'for', 'example', ',', 'not', 'been', 'deprived', 'of', 'a', 'right', ':', '?', '_', '.'], ['darrel', 'gives', 'to', 'jessica', 'her', 'telephone', 'number', 'to', 'add', 'to', 'his', 'facebook', 'page', '.', 'springfield', 'springfield', 'springfield', 'springfield', 'in', 'springfield', 'jessica', 'responds', 'by', 'handing', 'him', 'a', 'four', '-', 'digit', 'number', 'from', 'her', 'wallet', 'and', 'telling', 'him', 'about', 'it', '.'], ['he', ',', 'too', ',', 'however', ',', 'had', 'an', 'energy', 'that', 'could', 'only', 'bring', 'death', ',', 'like', 'pulling', 'embers', 'entered', 'the', 'jugular', 'or', 'pouring', 'blood', 'through', 'a', 'miniature', 'jug', '.', 'eadric', 'had', 'it', 'now', ',', 'too', '!'], ['seika', 'na', 'hero', ',', 'ghost', 'of', 'love', '(', '[UNK]', ')', '(', 'japanese', ':', '[UNK]', ',', 'suginamika', 'na', 'hero', ')', '(', '[UNK]', ',', 'aka', '(', 'lonely', 'soul', 'poet', ')', 'song', ':', 'alone', ',', 'alone', 'alone', 'alone', ';'], ['its', 'layout', 'was', 'polygonal', ',', 'and', '50', 'persons', 'were', 'holding', 'weapons', 'hence', 'the', 'nickname', ',', 'the', 'centre', '.', 'the', 'centre', 'was', 'also', 'home', 'to', 'one', 'of', 'the', 'largest', 'post', 'apartheid', 'protest', 'rooms', 'of', 'the', '\"', 'apartheid', '\"', 'era', '.'], ['she', 'was', 'the', 'only', 'slaver', 'there', '.', 'their', 'eyes', 'were', 'always', 'fixed', 'on', 'him', '.', 'for', 'the', 'boys', ',', 'he', 'taught', 'them', '.', 'as', 'for', 'antha', ',', 'he', 'taught', 'lady', 'juliana', '-', 'a', 'fair', '-', 'haired', 'girl', '.'], ['the', 'anchor', '(', 'or', 'anchor', ')', 'crew', 'and', 'portsmouth', 'prize', 'tender', 'were', 'often', 'commanded', 'either', 'by', 'a', 'john', 'keith', ',', 'a', 'james', 'wynne', '(', 'captain', 'from', '1577', '-', '1620', ')', 'or', 'the', 'prize', 'ship', 'any', 'adair', '.'], ['source', ':', 'cfa', 'the', 'league', 'is', 'structured', 'around', 'the', 'following', 'clubs', ':', 'ballarat', 'r', '&', 't', 'fc', 'afc', 'lauderdale', 'r', '&', 't', 'fc', 'the', 'league', 'features', '18', 'teams', '.', 'these', 'teams', 'have', 'also', 'entered', 'into', 'various', 'knockout', 'rounds', '.'], ['is', 'everything', 'okay', '?', 'but', 'how', 'else', 'can', 'it', 'ever', 'end', '?', 'by', 'the', 'time', 'laylen', 'was', 'still', 'moving', ',', 'he', 'knew', 'in', 'time', 'that', 'they', 'were', 'gone', '.', 'not', 'in', 'their', 'apartment', 'or', 'home', ',', 'but', 'gone', '.'], ['i', 'made', 'a', 'mistake', '.', '\"', 'the', 'exchange', 'of', 'teeth', 'makes', 'the', 'man', 'groan', 'and', 'roll', 'over', ',', 'for', 'he', 'clearly', 'is', 'a', 'wolf', ',', 'bigger', 'yet', 'not', 'adhering', 'to', 'any', 'statement', 'about', 'the', 'plenus', 'movement', '.'], ['house', 'is', 'down', 'and', 'anything', 'that', 'lasts', 'forever', 'was', 'two', 'singles', 'released', 'from', 'house', 'is', 'down', 'on', '3', 'march', '1987', '(', 'video', 'version', ')', '(', 'madonna', ',', 'kate', 'bush', ',', 'amy', 'winehouse', ',', 'gayle', 'darbyshire', ')', '.'], ['becoming', 'a', 'king', '-', 'becoming', 'a', 'king', 'was', 'much', 'easier', 'when', 'it', 'told', 'me', 'they', 'couldn', \"'\", 't', 'carry', 'a', 'spear', 'or', 'shoot', 'arrows', 'through', 'their', 'stomachs', '.', 'but', 'how', 'did', 'talking', 'with', 'silver', 'chopsticks', 'help', '?'], ['the', 'quontius', 'was', 'led', 'by', 'william', 'baptist', 'and', 'followed', 'by', 'prophets', 'samuel', 'johnson', ',', 'adam', 'smith', ',', 'and', 'nathan', 'english', ',', 'believing', 'that', 'humankind', \"'\", 'has', '(', 'and', 'met', ')', 'many', 'of', 'the', 'old', 'laws', \"'\", '.'], ['dutton', ',', '2002', '.', 'jeanette', 'cumming', 'and', 'the', 'woman', 'who', 'walked', '.', 'newark', ',', 'nj', ':', 'nps', ',', '1989', ',', 'volume', 'ssp', '44230', '.', 'baker', ',', 'thomas', 'r', '.', 'l', '.', 'bonin', '.'], ['the', 'arrangement', 'is', 'self', '-', 'described', 'in', 'principle', '.', 'merchant', 'vessels', 'like', 'those', 'trading', 'in', 'harbours', 'can', 'receive', 'rs', '100000', 'if', 'they', 'meet', 'the', 'acca', 'standards', '.', 'them', 'trading', 'on', 'jetties', 'receive', 'rs', '100000', '.'], ['suddenly', ',', 'i', 'felt', 'like', 'we', 'belonged', 'back', 'and', 'i', 'was', 'whole', 'again', 'again', '.', 'conner', 'and', 'me', 'were', 'taken', 'away', 'and', 'the', 'other', 'men', 'ran', 'around', 'looking', 'for', 'us', '.', 'finally', 'they', 'came', 'back', 'and', 'we', 'were', 'over', '.'], ['at', 'that', 'we', 'all', 'saw', 'it', '.', 'taking', 'our', 'lives', 'away', ',', 'acting', 'like', 'husband', 'and', 'wife', ',', 'and', 'ways', 'of', 'living', 'together', 'in', 'spite', 'of', 'our', 'own', 'needs', ',', 'caring', 'for', 'others', ',', 'ways', 'of', 'being', 'resourceful', '.'], ['mannerleft', 'and', 'the', 'kvorgedsalendje', 'd', '.', 'staffel', ',', 'which', 'have', 'two', 'floors', ',', 'hang', 'out', 'in', 'either', 'direction', '.', 'on', 'the', 'ninth', 'floor', ',', 'there', 'are', 'the', 'rooms', 'the', 'regular', 'hotel', ';'], ['noah', 'mailer', '(', 'norman', 'mailer', ')', 'tells', 'her', 'the', 'truth', ',', 'about', 'his', 'best', 'friend', ',', 'lala', 'deleo', ',', 'a', 'young', 'woman', 'who', 'escaped', 'a', 'mysterious', 'rabbit', 'hole', 'that', 'was', 'hidden', 'under', 'awnings', '.'], ['the', 'document', 'was', 'from', 'goldfinger', '.', 'and', 'it', 'was', 'his', 'handwriting', '.', 'not', '-', '\"', 'please', ',', 'your', 'majesty', '.', 'get', 'rid', 'of', 'her', 'immediately', '.', '\"', 'ah', ',', 'sad', 'that', 'this', 'visit', 'gave', 'me', 'the', 'creeps', '.'], ['the', 'election', 'machine', 'counts', 'the', 'winning', 'candidates', ',', 'allowing', 'the', 'incumbent', 'robert', 'lee', 'and', 'other', 'candidates', 'to', 'be', 'up', 'for', 're', '-', 'election', 'with', 'only', 'five', 'candidates', '.', 'the', 'machine', 'is', 'operated', 'by', 'over', 'four', 'million', 'people', 'throughout', 'thailand', '.'], ['matt', '!', '\"', 'said', 'matt', 'murmuringly', 'as', 'he', 'looked', 'around', 'to', 'see', 'how', 'things', 'were', 'being', 'handled', 'to', 'matt', ',', 'especially', 'the', 'fight', 'between', 'annie', 'and', 'chris', '.', 'annie', 'and', 'chris', 'opened', 'the', 'door', 'looking', 'around', 'for', 'matt', '.'], ['it', 'was', 'the', 'pipes', 'they', 'burned', 'so', 'long', 'over', 'and', 'over', 'again', 'that', 'i', 'did', 'think', 'gregory', 'should', 'read', 'this', 'somewhere', '.', 'the', 'complete', 'tale', 'says', 'that', 'the', 'morgoths', 'are', 'rulers', 'of', 'arria', 'and', 'dacia', '.'], ['sculptures', '(', '2001', ')', 'dagier', 'focuses', 'on', 'human', 'figures', '.', 'in', 'this', 'one', 'he', 'revisited', 'the', 'subject', 'of', 'human', 'design', 'and', 'colour', 'perception', '.', 'a', 'wide', 'variety', 'of', 'his', 'sculptures', 'are', 'mounted', 'which', 'are', 'shown', 'in', 'digital', '.'], ['there', 'remains', 'training', 'and', 'activity', 'focused', 'on', 'leadership', 'in', 'global', 'organizations', 'reported', 'below', ',', 'where', 'leadership', 'and', 'the', 'global', 'environment', 'become', 'central', 'to', 'their', 'training', '.', 'whipple', ',', 'bsb', ',', 'holds', 'the', 'rank', 'of', 'union', 'beta', 'pac', '.'], ['jose', 'and', 'barbara', 'gonzales', '(', 'born', '1938', ')', '.', 'in', '1962', ',', 'they', 'were', 'placed', 'as', 'a', 'couple', '.', 'barbara', 'lee', 'gonzales', ',', 'an', 'active', 'dominican', 'veteran', 'who', 'lived', 'in', 'gainesville', ',', 'florida', ',', 'was', 'eventually', 'admitted', 'there', ';'], ['this', 'list', 'includes', 'critique', 'of', 'the', '(', '\"', 'elite', '\"', ')', 'late', '19th', '/', 'early', '20th', 'century', 'fine', 'arts', 'figures', 'of', 'the', 'period', ',', 'citations', 'and', '(', 'more', ')', 'memoirs', 'made', 'by', 'other', 'authors', 'from', 'different', ',', 'scholarly', 'sources', '.'], ['he', '(', 'starring', 'kevin', 'mccarthy', 'and', 'kelly', 'fraser', ')', 'was', 'produced', 'by', 'co', '-', 'star', 'joan', 'crawford', ',', 'and', '(', 'later', ')', 'stars', 'as', 'a', 'reporter', 'and', 'the', 'new', 'york', 'news', 'agent', 'finds', 'a', 'missing', 'man', 'in', 'the', 'street', '.'], ['partnership', 'with', 'arab', 'american', 'bank', 'the', 'bank', 'established', 'saad', 'ash', '-', 'mazzani', 'in', '2008', ',', 'enhancing', 'its', 'business', 'and', 'banking', 'services', 'through', 'an', 'all', '-', 'in', '-', 'one', 'approach', 'of', 'co', '-', 'operating', 'arab', 'american', 'branches', ';'], ['three', 'weeks', 'later', '.', 'paid', 'my', 'unofficial', 'duty', 'three', 'times', ':', 'the', 'day', 'before', 'sundown', ',', 'the', 'day', 'or', 'night', 'when', 'mrs', '.', 'gramm', 'took', 'over', 'the', 'ranch', 'swing', ';', 'when', 'she', 'took', 'me', 'to', 'the', 'cemetery', '.'], ['and', 'then', 'roland', '-', 'and', 'his', 'children', '-', 'were', 'still', 'alive', '.', 'jake', 'and', 'susannah', 'knew', 'they', 'were', 'still', 'crying', ',', 'and', 'roland', 'knew', 'that', 'the', 'children', 'were', 'dead', '.', 'somewhere', 'nearby', ',', 'jake', 'and', 'susannah', 'were', 'talking', 'away', '.'], ['the', 'old', 'hatters', 'of', 'a', 'peak', 'is', 'a', '1933', 'play', 'developed', 'from', 'the', 'brian', 'carteret', 'novel', 'three', 'specks', 'for', 'the', 'lord', 'and', 'was', 'followed', 'by', 'the', 'garro', 'swifts', 'and', 'their', 'revenge', '(', '1962', ')', '.'], ['2010', '.', 'open', '-', 'air', 'sharks', 'quiz', ':', 'australian', 'sharks', 'and', 'the', 'zoo', '.', 'brooks', ',', 'michael', '.', '\"', 'sharks', 'in', 'australia', '\"', '(', 'pdf', ')', '.', 'journal', 'of', 'the', 'australian', 'veterinary', 'association', 'davis', ',', 'a', '.', 'bengt', '.'], ['julie', 'hall', 'and', 'emily', 'morris', '(', 'hired', 'by', 'msx', 'licensing', 'when', 'they', 'bought', 'pixies', 'television', 'franchise', ')', 'have', 'been', 'captured', 'in', 'a', 'time', 'sequence', 'on', 'emaxs', '.', 'com', 'using', 'a', 'slightly', 'pirated', 'version', '.'], ['won', ':', '1954', 'british', 'comedy', 'awards', 'nomination', ':', '1955', 'the', 'dog', 'was', 'great', '-', 'half', '-', 'hour', 'stand', '-', 'up', 'sketch', 'comedy', '(', 'second', 'original', 'series', ')', '-', 'barbara', 'egerton', 'did', 'pretty', 'much', 'what', 'she', 'normally', 'did', '.'], ['they', 'all', 'looked', 'more', 'like', 'exactly', 'two', 'people', 'to', 'be', 'looking', 'at', '.', 'caillen', 'wore', 'a', 'black', 'tank', 'top', ',', 'black', 'pants', ',', 'a', 'long', '-', 'sleeve', 'shirt', ',', 'and', 'a', 'brooch', 'on', 'his', 'left', 'cheekbone', '.'], ['his', 'gaze', 'panned', 'over', 'an', 'endless', 'expanse', 'of', 'farms', 'and', 'farmhouses', ',', 'each', 'one', 'seating', 'thousands', '.', 'some', 'were', 'the', 'occasional', 'blur', ',', 'their', 'eyes', 'open', 'to', 'some', 'magical', 'mirror', 'image', 'of', 'the', 'population', 'lived', 'out', 'there', '.'], ['terrin', 'lewis', '(', 'adopted', 'by', 'battus', \"'\", 'family', 'though', 'a', 'friend', 'of', 'hers', ')', '-', 'composer', ',', 'performer', '&', 'producer', '.', '(', 'executive', 'producer', '&', 'writer', ')', '-', 'musicians', 'and', 'you', '(', 'band', ')', '-', 'backing', 'vocalist', '.'], ['~', '~', 'oscar', 'rolf', 'harris', 'oscar', 'rolf', 'harris', 'translations', 'for', 'my', 'youngest', 'brother', 'andrew', 'were', 'translated', 'by', 'someone', 'who', 'said', 'that', 'the', 'stars', 'were', 'of', 'one', 'earth', 'which', 'created', 'another', 'and', 'the', 'other', 'part', 'was', 'taken', 'from', 'harold', 'richards', '.'], ['find', 'a', 'way', 'to', 'protect', 'us', 'all', '.', 'that', 'only', 'meant', 'knowing', 'that', ',', 'if', 'needed', ',', 'someone', 'might', 'teach', '.', 'if', 'playing', 'this', 'game', ',', 'if', 'not', 'on', 'this', 'road', ',', 'we', 'would', 'teach', 'the', 'world', 'a', 'lesson', '.'], ['later', ',', 'in', 'india', ',', 'she', 'earned', 'a', 'second', 'place', 'in', 'the', 'clothier', 'junior', 'badminton', 'league', '.', 'then', 'she', 'switched', 'both', 'her', 'allegiance', 'to', 'england', ',', 'and', 'went', 'on', 'to', 'win', 'the', 'english', 'under', '-', '19', 'badminton', 'championships', '.'], ['cyanna', ',', 'with', 'great', 'care', ',', 'disappeared', 'down', 'the', 'hallway', ',', 'frowning', '.', 'she', 'stared', 'out', 'through', 'the', 'open', 'doorway', 'into', 'the', 'hallway', 'as', 'she', 'looked', 'in', ',', 'noting', 'the', 'familiar', 'but', 'unfamiliar', 'foreign', 'language', '-', 'spanish', 'portuguese', '.'], ['neither', 'athlete', 'reached', 'the', '30', '-', '30', '.', 'all', 'lady', 'fighting', 'irish', 'programs', 'finished', '29th', 'in', 'ncaa', 'division', 'ii', '.', '(', 'g', ')', 'points', '(', 'g', ')', 'goals', 'against', 'total', 'games', 'ended', '(', 't', ')', 'tied', '(', 't', ')', '.'], ['and', 'that', 'bad', 'memory', 'was', 'there', '.', 'scarlet', 'smiled', 'and', 'shook', 'her', 'head', '.', '\"', 'no', ',', '\"', 'maax', 'said', '.', 'keeping', 'her', 'secret', 'from', 'his', 'big', 'sister', '.', 'the', 'one', 'who', ',', 'therefore', ',', 'murdered', 'her', 'daughter', '.'], ['pierre', '.', 'pierre', 'frederic', '.', 'antoinette', 'frederic', '(', 'with', 'her', 'husband', 'emile', 'du', 'cote', 'and', 'wife', ')', '.', 'jean', 'frederic', '(', 'with', 'emile', 'du', 'cote', ')', '.', 'pierre', 'frederic', '.', 'jeanne', 'elucidaire', '.', 'yvonne', 'joye', '.'], ['jason', 'jason', 'bateman', 'as', 'jack', '.', 'above', 'the', 'lines', 'english', 'language', 'version', '(', 'mr', '.', 'bee', '-', 'song', 'for', 'the', 'french', 'composer', 'claude', 'monet', ',', 'performed', 'by', 'miss', 'lou', 'ann', 'hofgen', ')', 'on', 'sony', 'pictures', '.'], ['beardmore', '(', 'sackville', ')', 'machine', 'gun', 'regiment', '364', '/', 'cpl', 'raf', 'lt', '-', 'colonel', 'duncan', '.', 'danny', 'work', 'appears', 'as', 'lt', '-', 'colonel', 'duncan', \"'\", 's', 'nephew', '.', 'several', 'other', 'british', 'army', 'soldiers', 'also', 'appear', '.'], ['god', 'damn', 'them', '!', 'they', 'were', 'everywhere', ',', 'made', 'out', 'of', 'nothing', ',', 'they', 'had', 'come', 'from', 'far', 'away', ',', 'laughing', ',', 'muttering', 'things', 'like', 'that', '!', '!', '!', '!', '!', '!', 'he', 'had', 'not', 'known', 'such', 'creatures', 'before', '.'], ['also', ',', 'another', 'song', 'in', 'the', 'film', ',', '\"', 'lots', 'of', 'sunshine', ',', 'never', 'much', 'traffic', 'in', '\"', ',', 'highlights', '\"', 'famous', 'men', 'making', 'comebacks', ',', '\"', 'as', 'the', 'lyrics', 'created', 'the', 'alter', 'egos', 'of', 'dudley', 'dudley', '.'], ['soon', 'there', 'would', 'be', 'more', '.', 'i', 'did', 'slowly', ',', 'slowly', 'undressing', 'each', 'time', '.', 'i', 'looked', 'around', ',', 'feeling', 'slightly', 'uncomfortable', ',', 'but', 'then', 'i', 'saw', 'that', 'the', 'guardians', 'entered', 'the', 'room', 'to', 'watch', 'over', 'me', '.'], ['2013', ':', 'tandem', ':', 'tandem', ':', 'joy', 'of', 'rescue', 'animals', 'is', 'closed', 'the', 'morgue', 'due', 'to', 'loss', 'of', 'spleen', 'after', 'surgery', 'with', 'pulmonary', 'complications', '.', '2014', ':', 'e', '-', 'e', 'horse', 'trainers', 'are', 'suspended', 'for', 'one', 'year', '.'], ['the', 'main', 'monster', 'was', 'a', 'large', 'fairy', 'with', 'blue', '-', 'black', 'ears', '.', 'the', 'other', 'three', 'monsters', 'were', 'the', 'fairies', ',', 'the', 'green', 'book', 'fairy', 'and', 'the', 'dragons', 'with', 'great', 'golden', 'claws', 'called', '\"', 'dragon', '-', 'slayers', '\"', '.'], ['and', 'karl', 'schenker', '(', 'the', 'rockformer', 'with', 'dramatization', ':', 'paul', 'nielsen', 'and', 'kopjes', ')', '.', 'promoted', 'from', 'division', '1', ':', 'felix', 'hellmeyer', ';', 'shane', 'dyson', ';', 'andrew', 'hill', ';'], ['he', 'has', 'difficulty', 'exploring', 'the', 'early', 'beginnings', 'of', 'a', 'contemporary', 'composer', 'and', 'artist', 'in', 'the', 'community', ',', 'although', 'his', 'conversation', 'with', 'van', 'de', 'kamp', '(', 'october', '2014', ')', 'has', 'apparently', 'addressed', 'concerns', 'over', 'funding', 'for', 'the', 'fellowship', 'program', '.'], ['there', 'had', 'truly', 'been', 'same', 'men', ',', 'the', 'very', 'same', 'questors', ',', 'inspecting', 'me', 'as', 'if', 'my', 'physical', 'appearance', 'was', 'nothing', 'special', '.', '\"', 'oh', ',', 'now', 'it', 'is', 'all', 'mine', '.', 'oh', ',', 'now', 'it', 'is', 'yours', '.'], ['the', 'park', 'is', 'featured', 'on', 'the', '1966', 'album', 'the', 'rolling', 'stones', 'live', '.', 'a', '1967', 'time', 'magazine', 'article', 'about', 'the', 'project', 'was', 'about', 'the', 'continued', 'use', 'of', 'iberville', 'park', 'by', 'the', 'colonial', 'school', 'children', 'during', 'this', 'period', '.'], ['\"', 'rain', '\"', 'from', 'the', 'ep', 'was', 'complementing', 'the', 'next', 'single', 'on', 'refugio', ',', 'then', 'a', 'collaboration', 'with', 'the', 'former', 'resulting', 'in', 'a', 'third', 'sound', 'stage', 'set', 'in', '2006', ',', 'with', 'ciara', 'providing', 'backing', 'vocals', '.'], ['he', 'claimed', 'to', 'partake', 'of', 'the', 'brand', 'through', 'a', 'mortgage', 'payment', 'system', 'incorporated', 'through', 'shanks', 'electronics', 'plc', '.', 'previously', 'providing', 'on', '-', 'premises', 'access', 'to', 'start', '-', 'up', 'and', 'online', 'payments', 'services', 'the', 'business', 'is', 'now', 'closed', '.'], ['\"', 'it', 'will', 'be', 'all', 'right', ',', '\"', 'the', 'clatter', 'of', 'a', 'ringing', 'cell', 'phone', 'comes', 'from', 'beyond', 'christian', 'grey', ',', 'hands', 'gripping', 'the', 'dashboard', ',', 'marring', 'his', 'handsome', 'face', 'as', 'i', 'anxiously', 'squeeze', 'against', 'the', 'panic', '.'], ['global', 'sustainable', 'development', 'guide', '{', 'sustainable', 'development', 'guide', '}', '{', 'lulu', '.', 'au', '.', 'com', '.', 'au', '/', '/', 'htc', '.', 'ndu', '.', 'edu', '/', 'lulu', '.', 'au', '/', ',', 'gba', '_', 'planet', 'international', '.'], ['\"', 'individual', ',', 'experience', '\"', ',', 'appearing', 'in', 'the', 'new', 'york', 'times', ',', '31', '(', '4', ')', 'march', '1503', '\"', 'what', 'is', 'the', 'word', \"'\", 'more', '-', \"'\", '?', '\"', '\"', 'life', '\"', ',', '27', 'may', ',', 'p', '.'], ['\"', 'the', 'three', '\"', 'the', 'three', 'brothers', 'are', 'attending', '\"', 'the', 'soa', '\"', '(', 'more', 'commonly', 'known', 'as', '\"', 'a', '\"', ')', 'level', '.', 'cissy', 'has', 'grey', 'hair', ',', 'a', 'white', 'mustache', ',', 'and', 'an', 'earring', '.'], ['i', \"'\", 'll', 'just', 'put', 'that', 'one', 'more', 'thing', 'out', 'of', 'her', 'mind', '.', '\"', 'the', 'others', 'start', 'making', 'little', 'statements', ',', 'especially', 'gren', 'and', 'kory', '.', 'they', \"'\", 're', 'cramming', 'as', 'she', 'is', 'now', '.'], ['a', 'commission', 'office', 'for', 'the', 'transfer', 'of', 'the', 'nas', 'chateauguay', 'to', 'alaska', '.', '1944', ':', 'a', 'commission', 'office', 'for', 'transfer', 'of', 'anchorage', ',', 'seattle', ',', 'and', 'tempe', 'to', 'the', 'coast', 'guard', 'of', 'washington', '.', '1942', ';'], ['this', 'is', 'a', 'series', 'spin', '-', 'off', ',', 'not', 'a', 'series', '.', 'the', 'five', 'main', 'characters', 'are', 'steve', ',', 'bob', ',', 'tony', ',', 'fred', 'and', 'strozzi', '(', 'respectively', ')', '.', 'tony', 'the', 'dead', 'man', '!', '!', '!', '!'], [\"'\", 'touch', 'daddy', \"'\", 'features', 'ray', 'brown', 'on', 'bass', 'and', 'vocals', '.', 'eddie', 'ray', 'r', '.', 'fairley', 'plays', 'keyboards', '.', 'ben', 'macdonald', 'was', 'featured', 'on', 'sean', 'wicks', \"'\", 'co', 'single', '\"', 'gay', '/', 'lesbian', 'electronic', 'song', '\"', '.'], ['it', 'consists', 'of', 'stories', 'with', 'music', 'by', 'trippi', '\"', 'coco', '\"', 'clark', '.', 'a', 'notable', 'short', 'in', 'the', 'gwendolyn', 'science', 'fiction', 'series', 'is', 'a', 'passage', 'in', 'time', 'written', 'by', 'terry', 'butler', 'and', 'illustrated', 'by', 'michael', 'patterson', '.'], ['shortly', 'after', 'its', 'completion', ',', 'it', 'was', 'completely', 'demolished', 'and', 'is', 'now', 'occupied', 'by', 'the', 'modern', '-', 'day', 'lake', 'village', '.', 'blue', 'devil', '(', '1987', ')', 'by', 'james', 'r', '.', 'langhorne', ';', 'music', 'score', 'by', 'david', 'allen', ';'], ['complete', 'poems', 'is', 'the', 'second', 'verse', 'collection', 'of', 'william', 'shakespeare', ',', 'a', 'play', 'produced', 'initially', 'as', 'woodcut', 'that', 'year', 'by', 'henry', 'william', 'shakespeare', ',', 'and', 'performed', 'later', 'as', 'a', 'series', 'of', 'farces', 'before', 'edward', 'iii', 'of', 'england', '.'], ['jose', '(', 'antonio', ')', 'de', 'la', 'merza', '(', 'sang', 'the', 'english', 'version', 'of', '\"', 'don', 'sandro', '\"', ')', ';', 'gloria', '(', 'originally', 'for', 'venne', 'franklin', ',', 'and', 'kay', 'marshall', 'doubling', 'as', '\"', 'miss', 'houston', '\"', ')', ';'], ['screaming', 'her', 'name', '.', 'after', 'that', ',', 'he', 'would', 'haunt', 'her', '-', 'every', 'night', '-', 'hard', 'and', 'fast', '-', 'hard', 'and', 'fast', 'for', 'ages', '.', 'he', 'did', 'it', 'every', 'time', 'in', 'between', '.', 'every', 'single', 'time', 'inside', 'and', 'out', '.'], ['right', 'and', 'left', ':', 'how', 'to', 'do', 'identification', ':', 'select', 'only', 'a', 'few', 'numbers', 'and', 'create', 'their', 'input', '.', 'numbers', 'can', 'also', 'be', 'used', 'to', 'determine', 'the', 'distance', 'you', 'use', 'and', 'process', ',', 'the', 'tool', 'used', 'and', 'the', 'symbol', '.'], ['that', 'was', 'it', ',', '[', 'and', 'the', ']', 'best', 'way', 'of', '\"', 'aryanization', '\"', '(', '\"', 'slaughter', '\"', ')', ',', 'to', 'free', 'them', 'from', 'the', 'meddling', 'of', 'nazism', 'or', 'martyrdom', 'at', 'the', 'hands', 'of', 'nazis', '.'], ['the', 'second', 'soviet', 'missile', 'strike', 'hit', 'the', 'city', 'in', '1945', 'and', 'he', 'consulted', 'with', 'vasily', ',', 'the', 'vase', 'maker', '.', 'he', 'also', 'built', 'chairs', 'for', 'tables', '-', 'in', 'his', 'case', ',', 'for', 'horse', '-', 'weighing', 'around', '7', 'kilograms', '.'], ['kai', 'valsgaard', 'auditioned', 'for', 'bass', ',', 'breaking', 'up', 'a', 'flexible', 'speedwo', 'mariner', 'played', 'by', 'dre', 'green', 'from', 'his', 'drum', 'kit', 'and', 'persuaded', 'him', 'to', 'become', 'a', 'free', '-', 'lance', 'model', 'of', 'double', 'bass', '.'], ['sci', '-', 'fi', 'central', 'featured', 'nba', 'live', ',', 'and', 'nba', 'live', 'series', 'd', 'titled', 'nba', 'tournament', 'live', '.', 'martin', 'and', 'shahin', 'encounter', 'players', 'from', 'every', 'major', 'league', 'except', 'boston', 'celtics', ',', 'toronto', 'raptors', '&', 'tears', 'for', 'fears', '.'], ['his', 'daughters', 'elizabeth', 'alice', 'hopkins', 'and', 'kathleen', 'hopkins', 'worked', 'for', 'both', 'charles', 'frederick', 'harvey', 'and', 'william', 'gorler', '.', 'of', 'his', 'six', 'children', ',', 'there', 'was', 'the', 'brothers', 'frank', 'lorimer', 'and', 'john', 'geoffrey', 'hopkins', ',', 'shipping', 'merchants', ';'], ['in', '2000', 'this', 'project', 'received', 'official', 'recognition', 'by', 'isco', 'and', 'some', 'forty', 'european', 'companies', 'and', 'institutions', 'received', 'it', ',', 'including', ':', 'network', 'blueprint', 'project', 'phase', 'iii', ':', 'a', 'european', 'strategy', 'for', 'integrating', 'software', 'in', 'network', 'peripherals', ';'], ['sakai', 'soon', 'finds', 'ryu', \"'\", 's', 'story', 'about', 'his', 'war', 'service', ',', 'and', 'they', 'meet', 'again', ',', 'but', 'not', 'before', 'at', 'the', 'korean', 'sino', '-', 'japanese', 'border', ',', 'where', 'their', 'friendship', '(', 'which', 'they', 'never', 'share', ')', 'begins', '.'], ['the', 'process', 'that', 'went', 'ahead', 'was', 'to', 'create', 'several', 'new', 'courts', ',', 'but', 'this', 'quickly', 'proved', 'a', 'failure', 'when', 'downside', 'shopping', '(', 'his', 'old', ')', '(', 'constructed', '1993', ')', 'was', 'created', 'as', 'a', 'new', 'shopping', 'centre', 'and', 'hospital', '.'], ['with', 'kanye', 'west', 'on', 'tour', 'and', 'columbia', 'records', 'making', 'a', '$', '4', 'shift', 'in', 'march', '2006', 'towards', 'fund', 'raising', ',', 'quincy', 'jones', 'alongside', 'alison', 'krauss', 'launched', 'a', 'new', 'australian', 'themed', 'channel', 'late', 'night', 'music', '(', 'australia', ')', '.'], ['after', 'experiencing', 'moderate', 'success', ',', 'gao', 'daqing', 'completed', 'huayu', ',', 'a', 'collection', 'of', 'poems', 'and', 'plays', 'by', 'prominent', 'chinese', 'writers', '(', '[UNK]', ';', 'yang', 'zhiping', 'daqing', 'xian', ')', '(', 'which', 'included', 'characters', ')', '.'], ['\"', 'rectangles', ',', 'dad', '.', 'tell', 'us', 'what', 'you', 'think', 'dad', '-', 'that', 'whatever', 'this', 'was', ',', 'whatever', 'is', ',', 'that', 'is', 'happening', 'here', '.', '\"', '\"', 'i', 'was', 'joking', ',', '\"', 'dad', 'had', 'said', ',', 'nodding', '.'], ['(', 'colin', 'wray', '.', ')', 'the', 'election', 'list', 'comprised', ':', 'councillor', 'k', 'm', 'de', 'oliveira', ',', 'an', 'mfnp', 'candidate', ',', 'councillor', 'j', 'c', 'w', 'price', ',', 'councillor', 'a', 'm', 'gray', 'and', 'adjoining', 'councillor', 'p', 'j', 'white', '.'], ['virgin', 'rugby', 'is', 'an', 'english', 'rugby', 'union', 'club', 'owned', 'by', 'virgin', '.', 'the', 'club', 'operates', 'pro', 'rugby', 'in', 'uk', ',', 'using', 'the', 'stevenage', 'arena', 'of', 'building', 'at', 'the', 'same', 'time', 'as', 'rugby', 'league', 'team', 'liverpool', '&', 'stevenage', '.'], ['if', '(', 'c', '(', 'b', ')', ')', 'is', 'on', 'b', ',', 'then', 'b', 'holds', 'the', 'shortest', 'path', 'on', 'a', 'and', 'c', 'the', 'shortest', 'length', 'on', 'a', '.', 'this', 'means', 'that', 'a', 'pass', 'on', 'a', 'does', 'not', 'hold', 'for', 'itself', ';'], ['\"', 'i', 'think', 'so', ',', 'actually', '.', 'i', 'was', 'on', 'a', 'three', '-', 'day', 'camping', 'trip', '.', 'i', 'had', 'such', 'an', 'amazing', 'time', 'going', 'there', '.', '\"', 'there', 'it', 'was', 'again', '.', 'a', 'small', 'stone', ',', 'a', 'bramble', '.'], ['the', 'architects', 'involved', 'in', 'those', 'trials', 'were', 'to', 'be', 'compensated', 'for', 'these', 'divergences', 'by', 'x', '-', 'rays', 'on', 'the', 'venetian', 'facade', 'of', 'the', 'famous', 'clock', 'duomo', '(', 'by', 'the', 'michelangelo', ')', 'at', 'the', 'palazzo', 'barberini', '.'], ['it', 'then', 'organised', 'a', 'regional', 'political', 'consultative', 'meeting', 'with', 'sir', 'nigel', 'heath', 'as', 'the', 'chairman', '(', 'born', 'c', '.', '1920', '-', 'died', '1996', ')', ',', 'sir', 'bernard', 'rawlinson', 'as', 'deputy', 'chairman', 'and', 'sir', 'hugh', 'robertson', 'as', 'deputy', 'chairman', '.'], ['i', 'had', 'no', 'celebrity', 'friends', 'or', 'tycoons', '.', '\"', '\"', 'wisely', ',', 'yours', 'is', 'one', 'hundred', 'seventy', 'thousand', 'of', 'ours', '?', '\"', '\"', 'katya', 'rakoczy', '.', 'influential', ',', 'brilliant', ',', 'brilliant', 'woman', '.'], ['1968', '-', '1970', ':', 'the', 'sound', 'and', 'the', 'rock', 'music', '1970', '-', '1977', ':', 'horror', 'movies', ',', 'commercial', 'films', ',', 'gay', 'movies', ',', 'early', '/', 'latter', 'horror', 'movies', ',', 'and', 'films', 'popularized', 'by', 'hakkooohee', 'luther', '.'], ['i', 'took', 'him', ',', 'tore', 'him', '.', 'what', 'was', 'his', 'blood', 'destiny', '?', 'no', ',', 'please', ',', 'please', 'please', ',', 'never', 'please', ',', 'never', 'please', '!', 'he', 'roared', 'with', 'his', 'long', 'fangs', ',', 'nails', ',', 'mouth', 'curved', 'in', 'hate', '.'], ['and', 'a', 'man', 'who', 'was', ',', 'literally', ',', 'just', 'plain', 'plain', 'old', ',', 'killed', 'with', 'bats', 'and', 'hemlockes', 'and', 'shit', '.', 'the', 'cartel', 'had', 'murdered', 'dozens', '-', '-', 'europeans', ',', 'americans', ',', 'and', 'whites', '-', '-', 'what', '?'], ['griffin', ',', 'sarah', 'hylton', ';', 'dorothy', 'brown', ',', 'and', 'ira', 'gershwin', '.', '\"', 'she', 'apparently', 'spoke', 'little', 'latin', 'in', 'her', 'little', 'time', '\"', '.', 'pitzer', ',', 'richard', 'm', ';', 'lewis', ',', 'richard', 'l', '(', '2004', ')', '.'], ['series', 'creator', 'josh', 'smith', 'reviewed', 'the', 'background', 'of', 'sam', ',', 'saying', 'that', 'he', 'believed', 'the', 'character', 'would', 'represent', 'a', 'world', 'as', '\"', 'a', 'fully', 'living', 'version', 'of', 'the', 'outside', 'world', '\"', 'and', '\"', 'it', 'has', 'grayish', 'focus', '\"', '.'], ['classes', 'carry', 'the', 'symbol', \"'\", 'h', \"'\", '.', 'in', 'northern', 'england', ',', 'british', 'schools', 'are', 'regularly', 'forced', 'to', 'flaunt', 'the', 'symbol', '(', 'h', ')', 'for', 'daily', 'use', '(', 'sati', ')', 'when', 'travelling', 'for', 'work', 'or', 'holidays', '.'], ['estimates', 'of', 'the', 'numbers', 'kept', 'by', 'the', 'japanese', 'were', 'no', 'more', 'than', '60', '.', 'gladys', 'holmes', ',', 'the', 'woman', 'who', 'escaped', ',', 'along', 'with', 'hibbert', 'were', 'arrested', 'and', 'the', 'prisoners', 'were', 'subsequently', 'held', 'in', 'an', 'underground', 'chamber', '.'], ['one', 'of', 'the', 'original', 'cast', 'members', ',', 'world', '-', 'famous', 'cabaret', 'performer', ',', 'will', 'perform', 'adrienne', '-', 'inspired', 'innovative', 'cabarets', 'influenced', 'by', 'hamburg', '-', 'based', 'große', 'scene', \"'\", 's', '\"', 'one', 'man', 'show', '\"', '.'], ['the', 'movie', 'became', 'a', 'blockbuster', 'film', 'running', 'at', '573', 'days', '.', '20', 'dong', 'guan', '(', '[UNK]', ')', 'nan', '(', '[UNK]', ')', 'dong', 'guan', 'nan', 'is', 'xinyu', \"'\", 's', 'classmate', 'and', 'works', 'with', 'wang', 'hong', '.'], ['1', 'august', '2013', '.', 'grocott', ',', 'w', '.', '(', 'september', '2017', ')', '.', '(', 'london', ':', 'brown', 'and', 'company', 'group', ')', '\"', 'free', 'school', 'learning', ':', 'integrating', 'theoretical', 'teaching', 'results', 'with', 'clinical', 'knowledge', 'for', 'young', 'adults', '\"', '.'], ['john', 'and', 'catherine', '.', 'allusions', 'to', 'a', 'play', 'by', 'thomas', 'paine', '.', 'british', 'library', '.', 'john', 'and', 'catherine', '.', 'mgr', '.', 'p', '121', 'priestley', ':', '(', 'july', '1911', ')', '.', 'your', 'place', 'in', 'the', 'palace', '.'], ['like', 'the', 'other', 'actors', '(', 'buckminster', 'fuller', ',', 'and', 'stanley', 'blackmon', ')', ',', 'this', 'act', 'earns', 'him', 'a', 'legendary', 'black', 'mask', 'as', 'yolande', 'buchner', ',', 'the', 'nazi', '\"', 'god', 'of', 'the', 'beast', '\"', '.'], ['no', ',', 'there', 'were', 'never', 'any', 'witnesses', 'and', 'i', 'had', 'to', 'do', 'a', 'perusal', '.', 'that', 'is', 'why', 'the', 'murders', 'happened', 'in', 'person', 'and', 'in', 'person', '.', '\"', 'at', 'the', 'car', ',', 'someone', 'was', 'reenergizing', '.'], ['production', ':', 'tomti', 'or', 'the', 'dragonfly', '(', 'owned', 'by', 'a', 'company', 'called', '\"', 'magic', '\"', ')', '.', 'casting', 'credit', ':', 'gerhard', 'joachim', 'beck', '(', 'gestress', ')', '.', 'production', 'credit', ':', 'gerhard', 'joachim', 'beck', '(', ')', '.'], ['he', 'founded', 'a', 'self', '-', 'service', 'store', ',', 'brown', 'sugars', ',', 'getty', 'city', '.', 'he', 'also', 'participated', 'in', 'public', 'talks', 'and', 'in', 'conversations', 'for', 'a', 'short', 'time', 'with', 'the', 'young', 'american', 'baptist', 'preacher', ',', 'john', 'shenk', '.'], ['these', 'companies', 'are', 'to', 'be', 'divided', 'into', 'three', 'groups', 'and', 'expand', 'their', 'corporate', 'area', '.', 'gotz', 'grattan', 'was', 'named', 'as', 'ceo', 'initial', 'initially', 'once', 'work', 'resumed', ',', 'however', ',', 'us', 'attorney', 'general', ',', 's', '.', 'j', '.'], ['she', 'felt', 'damn', 'good', '.', 'when', 'we', 'finished', 'we', 'jumped', 'from', 'the', 'roof', 'and', 'then', 'climbed', 'into', 'the', 'shade', 'under', 'the', 'tall', 'trees', 'that', 'linked', 'to', 'the', 'ocean', '.', 'the', 'moonlight', 'was', 'streaming', 'in', 'through', 'the', 'deeply', 'tinted', 'windows', '.'], ['interview', 'with', 'ahmed', 'abdel', 'aziz', '-', 'bechanel', '2001', '-', '07', '.', 'interview', 'with', 'farouk', '-', 'jafar', ':', '40', 'years', 'of', 'traditional', 'recordings', ']', '(', 'dvd', '.', 'uk', ')', '(', 'jogo', '.', 'uk', ')', '.'], ['(', '20', 'additional', 'credits', ')', 'with', 'a', 'specialization', 'in', 'chemistry', ',', 'marine', 'physics', 'and', 'mathematics', '.', 'about', '20', 'postgraduate', 'credits', 'and', '20', 'relevant', 'programs', '(', 'from', '1998', 'to', 'present', ')', '2018', '-', 'phd', '.', '2017', '-', 'quantum', 'quanta', '.'], ['yes', '.', \"'\", \"'\", 'good', '.', 'now', 'that', 'i', 'have', 'some', 'friends', 'up', 'here', ',', 'i', 'should', 'have', 'liked', 'to', 'go', 'outside', 'and', 'consider', 'what', 'happened', '.', \"'\", 'the', 'death', 'of', 'the', 'cab', '-', 'driver', 'seemed', 'a', 'week', 'ago', '.'], ['\"', '60', '%', 'failure', 'rate', 'of', 'parallel', 'computers', '.', '\"', 'networked', 'computing', ';', 'pitzer', ',', 'steven', 'a', '.', ';', 'myers', ',', 'eric', 'v', '.', 'w', '.', ',', '\"', 'cryptography', 'and', 'conditional', 'probability', '(', '2006', ')', '\"', '.'], ['\"', 'would', 'you', 'ladies', 'mind', 'coming', 'in', 'this', 'evening', '?', '\"', '\"', 'you', \"'\", 're', 'welcome', 'too', ',', 'jack', ',', '\"', 'he', 'said', ',', 'as', 'if', 'snatching', 'their', 'coats', 'to', 'put', 'them', 'in', 'the', 'living', 'room', 'right', 'away', '.'], ['production', '(', 'mixing', '/', 'mastering', ')', ':', 'mewan', 'hamblech', '[', 'bass', ']', '&', 'gary', 'johnson', '(', 'gary', 'johnson', ')', 'album', 'in', 'modern', 'hands', ',', 'also', 'known', 'as', 'hartford', 'courant', 'made', 'available', 'at', 'oast', '.'], ['the', 'mystique', 'sur', 'la', 'vie', ',', 'or', 'common', 'morality', ',', 'is', '(', 'as', 'the', 'first', 'was', ')', 'defined', 'by', 'the', 'author', 'in', 'a', 'long', 'monologue', 'tyrannitically', 'chastising', 'audacity', 'alone', '.'], ['kate', 'oldgart', ',', 'mary', 'mather', 'from', 'manchester', ',', 'and', 'sybel', 'watson', 'from', 'edinburgh', '.', 'it', 'gives', 'readings', 'by', 'british', 'poets', 'catherine', 'tate', 'and', 'peter', 'owen', 'and', 'is', 'free', 'to', 'staff', 'and', 'director', '-', 'in', '-', 'creative', '.'], ['later', 'part', 'of', 'southdown', 'west', 'and', 'gourock', '.', 'jennie', 'carnock', ',', 'married', 'charles', 'foster', ',', 'architect', 'and', 'cousin', 'to', 'mrs', 'foster', ',', 'who', 'owned', 'it', ';', 'agnes', 'ann', 'thomas', ',', 'married', 'charles', 'william', 'hoare', ';'], ['as', 'you', 'can', 'see', ',', 'the', 'transcripts', 'are', 'of', 'her', 'dance', 'lessons', '.', 'would', 'this', 'be', 'her', 'last', '?', '\"', '(', '?', '?', ')', '\"', 'when', 'interview', 'with', 'our', 'math', 'teacher', 'we', 'gave', 'her', 'address', 'as', 'girl', 'friends', '.'], ['someone', 'she', 'liked', 'could', 'only', 'be', 'her', 'first', '.', 'she', 'glared', 'at', 'the', 'pre', '-', 'rendered', 'blip', 'on', 'her', 'tiny', 'handheld', 'computer', 'screen', 'that', 'would', 'only', 'communicate', 'with', 'several', '(', ')', 'temporary', 'recipients', 'during', 'no', 'time', 'in', 'time', '.'], ['notable', 'features', 'include', 'sideshow', 'alley', '\"', 'dr', 'spoon', 'soup', '\"', 'involving', 'his', 'life', 'in', 'his', 'kitchen', ',', '\"', 'spin', 'it', 'up', '\"', ',', 'and', 'interactive', 'intermarications', 'with', 'youtube', 'videos', 'and', 'paraphernalia', 'displays', '.'], ['ayesha', 'was', 'told', 'that', 'she', 'has', 'been', 'found', 'in', 'southern', 'india', ',', 'parts', 'of', 'the', 'country', 'located', 'in', 'nepal', '.', 'a', 'blast', 'kills', 'her', '.', 'it', 'infects', 'janaki', 'and', 'leaves', 'the', 'two', 'of', 'them', 'alone', '.'], ['then', ',', \"'\", 'who', 'do', 'you', 'think', 'you', 'are', '?', \"'\", 'stuart', 'was', 'smiling', 'slightly', '.', 'stuart', 'had', 'asked', 'himself', 'how', 'well', 'worth', 'money', 'peachy', \"'\", 'd', 'taken', 'since', 'she', 'had', 'first', 'served', 'him', 'with', 'a', 'red', 'light', '.'], ['they', 'are', 'led', 'by', 'three', 'emi', 'vice', '-', 'presidents', ',', 'mark', 'warlow', ',', 'sir', 'hama', 'campbell', 'and', 'first', 'lady', 'katherine', 'cameron', ',', 'with', 'whom', 'the', 'institute', 'has', 'partnered', 'with', 'many', 'leading', 'universities', 'in', 'the', 'united', '-', 'kingdom', '.'], ['at', 'the', 'end', ',', 'the', 'player', 'runs', 'up', 'another', 'place', '(', 'as', 'in', 'the', 'sprite', ')', 'to', 'find', 'starting', 'line', '(', 's', ')', 'and', 'then', 'goes', 'up', 'line', '(', 's', ')', 'to', 'find', 'the', '\"', 'finish', 'line', '\"', '.'], ['*', '*', '*', '*', '*', '*', '*', 'chapter', 'eighteen', ':', 'is', 'it', 'bad', 'book', '?', 'fifteen', ':', 'josh', ',', 'billy', ',', 'billy', ',', 'donny', ',', 'and', 'mrs', '.', 'warren', 'all', 'meet', 'mrs', '.', 'warren', 'in', 'the', 'living', 'room', '.'], ['sh', '-', 'ez', '(', '\"', 'naughty', 'dog', '\"', ')', 'in', 'ds9', ':', 'a', 'player', 'character', 'in', 'the', 'new', 'bitsy', '-', 'stoned', '(', 'who', 'has', 'a', 'knack', 'for', 'tickling', 'her', 'neighbours', 'with', 'bits', ')', '.'], ['after', 'the', 'battle', 'of', 'alfreton', 'the', 'fields', 'were', 'gradually', 'levelled', 'and', 'ploughfields', 'cleared', ',', 'and', 'they', 'remain', 'near', 'st', 'mary', \"'\", 's', 'scatteryards', 'just', 'off', 'the', 'edge', 'of', 'the', 'town', 'centre', '.'], ['folk', 'magazine', 'and', 'atlantic', 'woman', \"'\", 's', 'band', '!', 'now', '\"', '[', 's', ']', '50', 'great', 'female', 'singers', '/', 'entertainers', 'and', '50', 'great', 'female', 'singers', 'of', '13', 'plus', '10', '\"', ',', 'as', 'a', 'tribute', 'to', 'w', '.', 'w', '.'], ['gothic', 'revival', ':', 'sometimes', 'said', 'to', 'be', 'simply', 'a', 'gothic', 'revival', 'church', '.', 'air', 'ice', 'age', '(', '500', 'million', 'years', ')', 'advanced', '.', 'land', 'ice', 'age', ':', 'there', 'is', 'evidence', 'of', 'intense', 'glacial', 'activity', '.', 'glacial', 'activity', 'is', 'extensive', '.'], ['chapter', 'seven', 'somehow', 'i', 'knew', 'that', 'when', 'kiyo', 'moved', 'he', 'always', 'paid', 'attention', '.', 'his', 'mind', 'could', 'stop', 'thinking', 'and', 'feeling', ',', 'but', 'would', 'focus', 'his', 'attention', 'elsewhere', '.', 'd', \"'\", 'artois', 'was', 'the', 'council', 'of', 'elders', '.'], ['\"', 'all', 'right', ',', 'simon', ',', 'this', 'ancient', 'web', 'of', 'vampires', 'is', 'asking', 'for', 'a', 'detailed', 'summation', 'of', 'your', 'humble', 'ability', 'to', 'be', 'a', 'vampire', 'and', 'piece', 'these', 'pages', 'together', '.', '\"', 'lucien', 'curled', 'me', 'tighter', 'against', 'him', '.'], ['\"', 'peril', '\"', '.', ';', ';', '\"', 'the', 'hatchets', 'of', 'the', 'hobbit', '.', '.', '.', '.', '\"', ';', '\"', 'who', '[', 's', ']', '?', '\"', ';', '\"', 'the', 'chaperones', '.', '.', '.', '\"', ';'], ['(', 'first', 'series', ')', ':', 'yeah', 'come', 'on', '(', 'makeover', ')', ':', 'michelle', 'jeffers', '(', 'adaptation', ')', ':', 'dorothy', 'beswick', '(', 'dan', ')', 'awry', ')', ':', 'tillie', 'the', 'couple', 'had', 'divorced', 'at', 'some', 'point', '.'], ['1982', 'saw', 'construction', 'start', 'on', 'the', 'new', 'centre', 'for', 'the', 'community', 'of', 'louw', ',', 'initially', 'to', 'include', 'cravans', 'and', 'while', 'cheyney', 'received', 'an', 'extension', 'the', 'name', 'of', 'harbour', 'and', 'promontory', 'were', 'changed', '.'], ['before', 'the', 'monster', 'says', 'anything', ',', 'both', 'my', 'body', 'and', 'my', 'breathing', 'suddenly', 'merge', ',', 'pushing', 'and', 'pushing', 'and', 'pushing', ',', 'to', 'push', 'back', ',', 'to', 'make', 'me', 'move', ',', 'to', 'push', 'back', '...', 'the', 'monster', 'pushes', 'back', 'slowly', '.'], ['yeah', ',', 'boswell', 'thinks', 'with', 'a', 'look', 'on', 'his', 'face', '.', '1', '-', 'fritz', '2', '-', 'fritz', 'sucks', ',', 'greg', 'kirke', ',', 'fritz', 'sucks', ',', 'fritz', 'sucks', ',', 'greg', 'kirke', ',', 'i', '.', 'a', '.', 'n', '.'], ['another', 'project', 'that', 'year', ',', 'the', 'ma', 'shi', 'ling', 'project', ',', 'was', 'a', 'failure', ',', 'due', 'to', 'its', 'poor', 'sales', ',', 'most', 'notably', 'the', 'console', 'version', 'for', 'the', 'playstation', '4', 'and', 'lite', 'in', 'the', 'playstation', '2', 'console', 'version', '.'], ['(', 'brownlee', ',', 'h', '.', 'p', '.', ';', 'ciarra', ',', 'a', '.', ',', '[', '1985', ']', ',', 'eds', '.', ')', 'peckholt', ',', 'marriage', 'and', 'equality', ',', 'v', ',', 'and', 'beckwith', ',', 'marcela', 'a', '.'], ['i', 'had', 'seen', 'this', 'woman', 'once', 'or', 'twice', ',', 'and', 'i', 'knew', ',', 'more', 'or', 'less', ',', 'that', 'she', 'was', 'a', 'woman', ',', 'and', 'i', 'did', 'not', 'know', 'much', '.', 'but', 'there', 'was', 'a', 'man', 'among', 'the', 'living', 'things', '.'], ['(', '\"', 'been', 'a', 'little', 'sore', '\"', 'was', 'also', 'performed', 'during', 'this', 'tour', 'by', 'paul', 'f', '.', 'walker', 'and', 'dan', 'hartman', '.', ')', 'lorrie', 'smith', ',', 'former', 'lead', 'singer', 'for', 'broken', 'hearts', ',', 'now', 'appears', 'solo', 'as', 'well', '.'], ['\"', 'coniston', 'bridge', '\"', ',', 'in', 'south', 'cumbria', 'east', ',', 'was', 'designed', 'by', 'milliert', 'vale', 'arts', 'and', 'design', 'group', '.', 'the', 'bridge', 'was', 'to', 'contain', 'a', 'park', ',', 'secondary', 'school', ',', 'primary', 'school', 'and', 'sports', 'centre', '.'], ['...', 'you', 'have', 'been', 'there', '.', 'i', 'am', 'there', 'now', ',', 'carrying', 'you', ',', 'and', 'her', 'gold', 'flower', 'around', 'you', '-', 'and', 'you', 'love', 'her', ',', 'or', 'daniel', ',', 'or', 'christ', ',', 'or', 'any', 'of', 'us', ',', 'and', 'ye', '.'], ['inholly', '?', '\"', '\"', 'hillsides', 'even', 'have', 'black', 'splat', 'logs', 'and', 'more', '.', 'some', 'are', 'high', 'high', 'enough', 'to', 'roll', 'down', 'to', 'the', 'snow', 'and', 'pick', 'up', '.', 'all', 'i', 'see', 'is', 'an', 'asara', '.'], ['another', 'was', 'the', 'capture', 'of', 'several', 'men', 'from', 'militia', '(', 'wisconsin', 'militiars', ')', '.', 'lieutenant', 'colonel', 'joseph', '(', 'later', ',', 'major', 'general', ')', 'klamp', 'continued', 'his', 'duty', 'in', 'the', 'milwaukee', ',', 'wisconsin', 'telephone', 'and', 'telegraph', 'company', '.'], ['6', '.', 'international', 'geographical', 'association', '.', '5', ':', '278', '-', '275', '.', 'guide', 'to', 'species', 'in', 'the', 'seas', '.', 'biological', 'survey', 'paper', 'with', 'a', 'look', 'at', 'intercoastal', 'and', 'water', '-', 'borne', 'animal', 'cougar', 'communities', '.'], ['anthony', 'scott', '-', 'johnson', ',', 'plo', 'bass', ',', 'pedal', 'steel', 'bass', 'john', 'yeats', ',', 'drums', 'john', 'higgins', \"'\", 'breakthrough', 'albums', 'live', 'album', ',', 'released', 'on', 'cd', '(', 'backing', 'vocals', ')', 'vanity', 'fair', '(', '1999', ')', 'band', 'together', '!'], ['july', ':', 'blacky', 'jackson', 'was', 'released', '.', 'july', '12', ':', 'construction', 'worker', 'joe', 'winton', 'is', 'arrested', 'in', 'flandria', 'by', 'police', 'lieutenant', 'joe', 'winton', 'after', 'filling', 'out', 'excuses', 'for', 'a', 'bus', 'ride', 'on', 'the', 'highway', '.'], ['for', 'devon', ',', 'the', 'room', 'was', 'very', 'warm', '.', 'he', 'remembered', 'the', 'wild', 'ways', 'devon', 'had', 'led', 'him', 'to', 'the', 'sofa', ',', 'the', 'bed', 'on', 'which', 'he', 'sat', 'comfortably', ',', 'and', 'saw', 'the', 'pat', 'downs', 'between', 'devon', 'and', 'dante', '.'], ['(', 'lyrics', 'by', 'rex', 'scott', ')', '.', 'he', 'enrolled', 'at', 'notting', 'hill', 'theatre', 'then', 'known', 'as', 'drury', 'lane', ',', 'and', 'he', 'wrote', 'music', 'for', 'shows', 'like', 'caprice', ',', 'rapier', 'and', 'most', 'notably', 'his', 'hit', 'encores', '!'], ['a', 'sister', 'katerina', 'stipa', '(', 'nee', 'cikov', ')', ',', 'originally', 'from', 'montenegro', ';', 'and', 'a', 'brother', ',', 'josefa', '(', '[', 'josefa', ']', ')', 'who', 'is', 'named', 'after', 'her', 'mother', 'josepha', 'stepova', ';'], ['-', 'indian', 'society', 'for', 'anti', '-', 'cancer', ',', 'society', 'for', 'heart', 'and', 'cancer', 'transplantation', '(', 'sst', ')', '(', 'india', ')', 'collectibles', '(', 'cdd', ')', ',', 'association', '.', 'this', 'bag', 'contains', ':', 'cancer', 'cures', ';'], ['she', 'pulled', 'up', 'the', 'file', '.', 'it', 'probably', 'looked', 'like', 'an', 'accident', '.', 'working', 'carefully', ',', 'she', 'opened', 'it', 'and', 'read', 'another', 'section', '.', 'its', 'only', 'flaw', 'was', 'that', 'a', 'picture', 'could', 'be', 'taken', 'from', 'someone', 'outside', 'the', 'site', '.'], ['mars', 'marsalis', ',', 'catriona', 'pickard', ',', 'marilyn', 'brown', ',', 'and', 'background', 'singers', '.', 'william', 'michael', 'devitt', ',', 'veteran', 'of', 'regular', '(', 'episodes', 'of', 'last', 'tweet', ')', 'programming', '.', 'portrayed', 'lichinsky', '.'], ['i', 'am', 'now', 'responsible', 'for', 'all', 'of', 'your', 'decisions', '-', 'your', 'actions', ',', 'your', 'thoughts', ',', 'your', 'thoughts', '.', 'my', 'every', 'desire', 'is', 'that', 'spray', '-', 'painted', 'runt', 'of', 'yours', 'sitting', 'in', 'front', 'of', 'me', ',', 'no', 'question', '.'], ['i', 'was', 'hoping', '...', '\"', '\"', 'that', \"'\", 's', 'not', 'gonna', 'go', 'well', 'here', '.', '\"', 'detective', 'hastings', 'looked', 'up', 'and', 'heard', 'a', 'slight', 'note', 'of', 'desperation', 'in', 'my', 'voice', '.', '\"', 'goodbye', ',', 'detective', '.', '\"', 'he', 'smiled', '.'], ['i', 'believe', 'in', '-', 'i', 'believe', '-', 'the', 'theory', 'about', 'which', 'we', 'speak', 'most', 'strongly', 'in', 'the', 'framework', 'of', 'the', 'green', 'dog', 'code', 'and', 'the', 'work', 'of', 'social', 'scientists', '.', '\"', '\"', 'i', 'shall', 'take', 'care', 'of', 'it', 'then', '.'], ['these', 'fortifications', 'are', 'made', 'from', 'earth', 'and', 'concrete', 'blocks', 'and', 'used', 'for', 'circular', 'bastions', '(', 'see', 'list', 'of', 'japan', \"'\", 's', 'fortifications', ')', 'with', 'sokka', '(', 'pinnacled', 'towers', ')', 'built', 'around', 'them', 'within', 'traditional', 'bastions', '.'], ['the', 'cooperating', 'aspects', 'of', 'the', 'composition', 'result', 'from', 'the', 'interaction', 'between', 'the', 'composition', 'and', 'audience', '.', 'the', 'interpersonal', 'issues', 'include', 'the', 'nature', 'of', 'the', 'public', 'process', 'in', 'which', 'the', 'composer', 'participates', 'with', 'others', 'and', 'the', 'production', 'process', '.'], ['it', 'had', 'obviously', 'taken', 'some', 'of', 'both', 'but', 'the', 'coast', 'was', 'clear', '.', '\"', 'she', \"'\", 's', 'worth', 'more', 'time', 'now', ',', 'thank', 'you', ',', 'dickie', '.', '\"', 'chapter', 'four', 'that', 'night', ',', 'it', 'was', 'dickie', 'home', '.'], ['the', 'other', 'four', 'dropped', 'off', 'it', 'and', 'gave', 'way', 'to', 'another', 'dot', '.', 'the', 'noise', 'was', 'almost', 'deafening', '.', 'that', 'was', 'it', ',', 'and', 'there', 'was', 'a', 'long', 'straight', 'line', 'ahead', '.', '16', '³⁄₄', '?', 'reacher', 'had', 'guessed', '.'], ['biology', 'and', 'human', 'evolution', ',', 'book', '4', ',', '(', '1975', ')', '.', 'the', 'big', 'picture', ':', 'how', 'does', 'humanity', 'decide', 'who', 'we', 'are', ',', 'make', 'new', 'things', 'for', 'ourselves', '?', 'how', 'many', 'choices', 'need', 'to', 'be', 'made', 'by', 'nature', '?'], ['\"', 'no', 'more', 'beatings', 'today', '.', '\"', 'he', 'needed', 'rest', '.', 'in', 'the', 'chamber', 'were', 'just', 'a', 'few', 'top', '-less', 'people', ',', 'but', 'he', 'took', 'out', 'a', 'pair', 'of', 'pistols', 'and', 'placed', 'them', 'at', 'his', 'wounded', 'hand', '.'], ['her', 'other', 'aunts', '-', 'tiny', 'stuck', '-', 'up', 'chicks', 'who', 'dreamed', 'about', 'having', ',', 'or', 'not', ',', 'having', '...', 'children', '-', 'livy', 'was', 'with', 'her', 'family', '-', '-', 'in', 'fact', ',', 'they', 'were', 'making', 'it', 'for', 'themselves', '.'], ['[', '\"', 'toronto', '\"', 'on', '-', 'cd', 'release', ',', 'rx12', '.', ']', 'sanderson', ',', 'roger', '(', '1997', ')', '.', '\"', 'on', 'speaking', 'to', 'the', 'province', 'of', 'nova', 'scotia', '\"', '.', '\"', 'on', 'upstate', 'young', 'people', '\"', '.'], ['guitarist', 'john', 'coltrane', '(', 'thomas', 'joseph', 'chamberlain', ')', ',', 'who', 'discovers', 'haines', 'with', 'him', ',', 'is', 'busy', 'with', 'hundicrantch', 'star', 'magic', 'and', 'he', 'instructs', 'the', 'singer', 'to', 'take', 'his', 'attention', 'elsewhere', '.'], ['(', '2008', ')', 'and', '\"', 'james', 'grenadier', '\"', '(', '2013', ')', '.', 'tv', 'movie', '-', 'in', '1971', ',', 'joan', 'crawford', '(', 'as', '\"', 'joan', '\"', ')', 'won', 'the', 'best', 'writer', 'at', '\"', 'new', 'york', 'city', 'writers', '\"', '.'], ['mile', '(', 's', ')', '(', 'tr', ')', '.', 'us', '52', 'hwy', '52', 'was', 'the', 'final', 'designation', 'for', 'the', 'ramp', 'from', 'branson', 'tri', '-', 'county', 'airport', 'to', 'boeing', 'field', ',', 'h', 'bts', ',', 'cats', ',', 'and', 'others', '.'], ['following', 'the', 'same', 'techniques', 'as', 'traditional', 'artists', ',', 'taylor', 'spray', '-', 'painted', 'in', 'either', 'painting', 'or', 'window', '.', 'taylor', 'created', 'landscape', 'and', 'home', 'design', 'for', 'international', 'and', 'local', 'arts', 'and', 'crafts', 'people', ',', 'usually', 'from', 'the', 'doveton', 'area', '.'], ['the', 'initial', 'e', 'was', 'written', 'after', 'the', 'initial', 'g', ',', 'and', 'before', 'the', 'initial', 'a', '.', 'the', 'final', 'e', 'was', 'downsized', 'to', 'denote', 'a', 'middle', 'e', '(', 'or', 'pitch', 'indicating', 'a', 'final', 'd', ')', 'viable', 'in', 'english', 'poetry', '.'], ['or', 'maybe', 'gas', 'lights', '?', 'even', 'half', '-', 'dark', ',', 'or', 'so', 'dark', 'that', 'they', 'showed', 'loose', 'patches', 'of', 'asphalt', '.', 'where', 'the', 'driver', 'swore', 'under', 'his', 'breath', ',', 'the', 'engine', 'slowed', 'down', 'and', 'the', 'driver', 'lit', 'a', 'cigarette', '.'], ['and', 'in', 'fact', ',', 'even', 'though', 'she', 'did', ',', 'she', 'was', 'beautiful', '.', 'but', 'then', 'my', 'bff', 'ran', 'her', 'clothes', 'hanger', '.', 'she', 'used', 'it', 'to', 'break', 'to', 'my', 'studio', 'and', 'look', 'at', 'the', 'doll', 'i', 'owned', '.'], ['this', 'same', 'local', 'association', 'association', 'was', 'in', 'development', '.', 'no', 'other', 'nrhp', '-', 'listed', 'local', 'organizations', 'like', 'the', 'bay', 'area', 'initiative', 'for', 'healthy', 'bay', 'area', 'or', 'the', 'madoc', 'triangle', 'neighborhood', 'association', 'had', 'become', 'independent', 'in', '1974', '.', 'approx', '.'], ['sylvia', 'glover', '-', '-', '-', 'social', 'worker', ',', 'nurse', ',', 'sociologist', ',', 'and', 'psychiatric', 'doctor', '.', 'ronald', 'glover', '-', '-', '-', '-', 'psychopath', ',', 'psychologist', 'and', 'sociopath', 'frank', 'glover', 'and', 'catherine', 'glover', ',', 'maternal', ',', 'paternal', 'cousins', '.'], ['peerage', 'and', 'peerage', 'series', ',', 'volume', '1', ',', '1971', '.', 'the', 'loyalty', 'of', 'prince', 'charles', 'in', 'the', 'civil', 'war', ':', 'a', 'comparison', 'of', 'the', 'french', 'civil', 'war', 'with', 'the', 'english', 'revolution', '.', 'in', 'william', 'thomson', '.', 'into', 'political', 'history', '.'], ['the', 'eicks', 'lived', 'in', 'a', 'brick', 'dwelling', 'with', 'a', 'wooden', 'sign', 'which', 'read', '\"', 'dogs', 'of', 'south', 'walk', '\"', '.', 'they', 'lived', 'in', 'part', 'of', 'north', 'forks', 'where', 'it', 'is', 'called', 'kiamichi', 'on', 'the', 'signs', '.'], ['he', 'moved', 'over', 'it', 'and', '(', 'several', 'feet', 'short', 'of', 'the', 'peak', 'of', 'the', 'cavern', 'floor', ')', 'glided', 'to', 'the', 'farthest', 'end', '.', '\"', 'hmmm', '.', 'look', 'at', 'it', '.', '\"', '*', '*', '*', 'it', 'was', 'beautiful', '.'], ['when', 'i', 'graduated', ',', 'my', 'family', 'continued', 'to', 'hover', 'over', 'me', ',', 'feeling', 'like', 'i', 'never', 'spent', 'as', 'much', 'time', 'with', 'them', 'rather', 'far', 'from', 'home', '.', 'they', 'would', 'tell', 'me', 'their', 'mother', 'loved', 'and', 'needed', 'them', 'most', '.'], ['\"', '(', '21', '-', '22', ')', 'brother', 'and', 'sister', ',', 'there', 'are', 'different', 'groups', 'of', 'people', 'like', 'us', '.', 'be', 'careful', '-', 'you', 'hear', 'sad', 'songs', 'and', 'old', 'ballads', 'with', 'their', 'rethellations', ',', 'and', 'then', 'listen', '.'], ['then', 'the', 'men', 'started', 'to', 'sweat', '.', \"'\", 'father', '!', \"'\", '*', '*', \"'\", 'father', '!', \"'\", '*', '*', \"'\", 'all', 'that', 'i', 'saw', 'inside', ',', 'the', 'fire', 'and', 'the', 'shield', '...', 'all', 'i', \"'\", 've', 'got', 'eros', '.'], ['however', ',', 'the', 'garden', 'was', 'never', 'flooded', 'and', 'drunkenness', 'was', 'common', 'and', 'well', 'known', 'to', 'tourists', '.', 'they', 'also', 'learned', 'that', 'people', 'had', 'worked', 'harder', 'and', 'harder', 'in', 'europe', ',', 'and', 'windows', 'were', 'often', 'accidentally', 'set', 'ablaze', '.'], ['albums', ':', '\"', 'goody', 'for', 'flesh', '\"', 'and', '\"', 'the', 'ways', 'we', 'would', 'learn', '\"', '.', 'for', 'the', 'library', 'publishing', 'company', ',', 'london', '.', 'british', 'and', 'foreign', 'publishing', 'company', ':', '130', 'tracks', '.', 'various', 'manuscripts', 'of', 'the', 'bible', '.'], ['yes', ',', 'i', 'am', ',', 'just', 'as', 'your', 'grandfather', 'and', 'i', 'should', 'have', 'already', 'done', '.', 'here', 'is', 'the', 'possibility', 'of', 'a', 'world', 'war', 'i', 'will', 'be', 'going', '.', 'all', 'knows', 'we', 'must', 'not', 'wait', 'for', 'the', 'next', 'action', '.'], ['\"', 'we', 'still', 'hardly', 'liked', 'each', 'other', ',', 'even', 'being', 'together', '.', 'why', 'did', 'you', 'say', 'that', 'before', '?', '\"', '\"', 'well', ',', 'if', 'who', 'wanted', '...', '\"', '.', 'and', 'then', 'a', 'shrill', \"'\", 'shout', \"'\", 'circled', 'the', 'bar', '.'], ['george', 'bunt', '.', 'in', '2012', '\"', 'my', 'years', '\"', '(', '8', 'years', 'of', '7th', 'grade', 'activities', 'for', 'teachers', 'and', 'students', ')', '\"', 'house', 'where', 'george', 'lived', '\"', '.', 'digital', ':', 'e', '-', 'book', ':', '(', 'british', ')', 'library', '.'], ['in', 'new', 'york', ',', 'sesame', 'street', 'hired', 'producer', 'bob', 'harris', 'from', 'the', 'simpsons', 'film', 'company', 'to', 'portray', 'scotty', 'in', 'sideshow', 'druids', ':', 'the', 'simpsons', ':', 'the', 'program', 'about', 'the', 'family', ':', 'mini', '-', 'ghost', 'show', '.'], ['all', 'of', 'it', 'eludes', 'me', ',', 'because', 'it', 'was', 'always', 'that', 'way', 'until', '...', 'i', 'stayed', 'away', 'when', 'i', 'moved', 'in', 'with', 'dom', 'and', 'paul', '.', 'i', 'thought', 'of', 'paul', 'right', 'about', 'now', ',', 'almost', 'a', 'hundred', 'years', '.'], ['mother', 'and', 'child', 'reunite', 'and', 'jonathan', 'takes', 'the', 'mother', 'and', 'child', 'to', 'them', '.', 'jonathan', 'and', 'sarah', 'try', 'to', 'find', 'the', 'mother', 'but', 'they', 'do', 'nothing', '.', 'madini', 'says', 'she', 'will', 'let', 'the', 'children', 'go', 'hand', 'in', 'hand', '.'], ['the', 'problem', 'is', ':', 'they', 'go', 'home', ',', 'they', 'leave', 'their', 'families', ',', 'they', 'come', 'from', 'one', 'family', 'to', 'the', 'other', '(', 'valentino', 'said', '\"', 'jesus', 'would', 'be', 'happy', 'with', 'the', 'last', 'one', 'in', 'his', 'family', '\"', ')', '.'], ['known', 'as', 'the', 'diadem', 'of', '\"', 'the', 'shining', '\"', 'by', 'bo', 'diddley', ',', 'and', '\"', 'look', 'out', 'there', '\"', 'by', 'maxwell', '.', 'life', 'by', 'the', 'day', '(', '1969', ',', 'thunderbirds', ')', '1975', '(', 'uk', ')', '.'], ['on', 'a', 'warm', 'day', ':', 'more', 'meat', ',', 'fruits', ',', 'and', 'chilli', '(', 'still', 'no', 'further', 'than', 'middle', 'of', 'january', ')', '.', 'on', 'another', ',', 'cooler', 'and', 'technically', 'shadier', 'day', ':', 'more', 'ripe', 'fruits', 'than', 'coarse', 'grain', '.'], ['.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'under', 'god', \"'\", 's', 'ultimate', 'command', '.', '.', '.', '.', '.', '.', '\"', '25', ',', 'no', '.', '1209', '(', '1925', ')', '.', 'the', 'malt', '.'], ['pedro', 'banibaribe', 'octavio', 'paz', 'daniel', 'wright', 'dan', 'jackson', 'aileen', 'palmer', 'mark', 'collins', 'david', 'jones', 'robert', 'taylor', 'fad', 'al', 'khakob', 'in', '1979', ',', 'harvest', 'poetry', 'organized', 'three', '\"', 'harvest', 'poets', '\"', 'festivals', '.'], ['for', 'metroplex', 'media', ',', 'she', 'became', 'involved', 'with', 'ghosts', ',', 'which', 'are', 'real', '-', 'life', 'ghosts', ',', 'on', 'the', 'infinity', 'gauntlet', 'and', 'the', 'pompeon', 'episode', '\"', 'let', 'the', 'kids', 'get', 'a', 'brand', 'new', 'ride', '\"', '.'], ['duns', '&', 'kurzen', '2016', ':', 'ep', '.', '59', '-', '61', '(', '9086rpm', ')', '2017', ':', \"'\", '(', '59', '-', '57', ')', \"'\", '(', 'featuring', 'kymone', 'lees', ')', 'duns', 'ep', '.'], ['my', 'only', 'consolation', 'is', 'how', 'far', 'you', 'can', 'get', 'to', 'do', 'so', 'later', 'in', 'the', 'day', '.', 'they', 'can', 'try', 'to', 'kill', 'a', 'vampire', '.', 'it', \"'\", 's', 'a', 'whole', 'weird', 'and', 'twisted', 'idea', ',', 'but', 'they', 'can', 'try', '.'], ['carter', 'is', 'also', 'a', 'highly', 'attended', 'speaker', 'who', '\"', 'rarely', 'delays', '\"', 'and', 'has', 'supported', 'victims', 'of', 'sexual', 'assault', \"'\", '\"', 'the', 'prodicate', 'for', 'abuse', 'of', 'speech', '\"', 'carter', 'claims', 'she', 'was', 'influenced', 'by', 'the', 'gay', 'rights', 'movement', '.'], ['2016', '-', 'jeff', 'berlin', 'portrayed', 'wright', 'in', 'atlanta', '.', '2014', '-', 'jeff', 'berlin', 'portrayed', '\"', 'crash', '\"', 'wright', 'in', 'the', 'drive', 'in', 'episode', '\"', 'the', 'lost', 'family', '\"', '(', 'with', 'gags', 'and', 'interactions', 'via', 'e', '-', 'mail', ')', '.'], ['\"', 'rock', 'steady', '\"', '(', 'back', 'to', 'back', 'version', ')', 'james', '\"', 'bob', '\"', 'hall', '(', 'voice', 'over', ')', 'robert', 'mcalpine', ',', 'jr', '.', '\"', 'the', 'bridge', '\"', '(', 'master', 'of', 'ceremonies', ')', '\"', 'southpaw', '\"'], ['the', 'original', 'filmmakers', 'insisted', 'that', 'the', 'original', 'cover', 'story', 'be', 'a', 'fake', 'account', ',', 'instead', 'of', 'what', 'was', 'meant', 'for', 'the', 'character', 'bounty', 'that', 'was', 'intended', 'for', 'his', 'brother', 'richard', 'cullum', '(', 'played', 'by', 'al', 'winans', ')', '.'], ['the', 'dallas', 'art', 'association', '(', 'dallas', 'fac', ')', ';', 'the', 'broome', 'art', 'museum', 'in', 'plano', 'vista', ';', 'museo', 'del', 'mexico', 'alcorona', ';', 'the', 'dallas', 'historical', 'society', 'and', 'the', 'historic', 'rantoun', 'historical', 'society', ';'], ['gorin', 'libramovic', 'in', 'the', 'movie', '\"', 'a', 'serbian', 'proverb', '\"', ';', 'vincent', 'staccato', 'in', 'the', 'film', ',', 'television', 'series', ',', 'and', 'movie', '(', '\"', 'the', 'kate', 'and', 'nick', 'story', '\"', ')', ';'], ['i', 'scratch', 'my', 'hangover', '.', 'that', 'might', 'work', 'for', 'me', ',', 'but', 'my', 'secret', 'life', 'is', 'probably', 'the', 'driving', 'force', 'of', 'my', 'little', 'brother', ',', 'josh', '.', 'the', 'rest', 'of', 'the', 'girls', 'go', 'on', ',', 'looking', 'insecure', '.'], ['(', 'toronto', ',', 'spring', '1988', ')', '.', 'selected', 'poems', ',', 'published', 'by', 'the', 'toronto', 'mercury', '.', 'gloria', 'perez', 'cardellino', 'as', 'patronado', 'del', 'mama', 'argentina', '.', 'collection', 'connery', ',', 't', '.', ';', 'h', '.', 't', '.', ';'], ['cherie', 'brown', ':', 'model', 'samantha', 'allen', ':', 'model', ';', 'co', '-', 'head', 'coach', 'of', 'gcw', '2008', ';', 'champion', 'gaby', 'brown', ':', 'model', '2008', ';', 'champion', 'sarah', 'moore', ':', 'model', ';', 'finalist', 'emma', 'scott', ':', 'model', ';'], ['according', 'to', 'her', ',', 'when', 'mattio', \"'\", 's', 'parents', 'asked', 'belmonte', 'where', 'they', 'were', ',', '\"', 'the', 'belmonte', 'family', '(', 'mattio', ')', 'is', 'from', 'lucca', ',', 'which', 'is', 'in', 'northern', 'italy', 'once', 'again', '\"', '.'], ['20', '(', '1921', ')', '.', '\"', 'signs', 'of', 'a', 'more', 'prosperous', 'time', '[', 'avec', 'arnaud', 'de', 'thomas', ']', '\"', ',', 'avant', '-', 'gardes', 'de', 'l', \"'\", 'eveque', ',', '1860', '-', '1918', ',', '1922', ',', 'p', '.'], ['\"', 'what', ',', 'and', 'are', 'you', '\"', 'represents', ',', 'by', 'the', 'chorus', ',', '\"', 'they', 'do', 'what', 'they', 'shouldn', \"'\", 't', '\"', '-', 'in', '\"', 'their', 'questions', 'cannot', 'be', 'answered', '\"', ',', 'characters', 'in', 'the', 'movie', 'leprosy', '.'], ['in', '1968', ',', 'charlie', 'clark', 'was', 'named', 'team', 'manager', 'after', 'both', 'jack', 'nicklaus', 'and', 'dave', 'gardner', 'stepped', 'down', ',', 'but', 'pat', 'ryan', 'and', 'dr', '.', 'geri', 'aylmer', 'replaced', 'him', 'in', 'a', 'co', '-', 'general', 'position', '.'], ['modern', 'versions', '(', 'masculine', 'and', 'feminine', 'emblems', ')', 'of', 'the', 'spanish', 'and', 'portuguese', 'versions', 'of', 'national', 'symbols', 'of', 'spain', 'and', 'portugal', ',', 'manufactured', 'by', 'museu', 'teatre', 'de', 'gijon', ',', 'and', 'made', 'by', 'nispor', '.'], ['for', 'this', 'year', \"'\", 's', 'tapes', 'were', 'mixed', ',', 'edited', 'and', 'produced', 'primarily', 'by', 'john', 'hughes', ',', 'and', 'frequent', 'collaborators', 'guy', 'maxwell', ',', 'in', 'which', 'the', 'dance', '-', 'pop', 'artists', 'also', 'used', 'the', 'samples', 'they', 'provided', 'in', 'their', 'releases', '.'], ['take', 'me', 'back', ',', 'may', 'god', 'get', 'over', 'here', 'now', ',', 'ainsley', 'pleaded', '.', 'she', 'was', 'born', 'there', ',', 'and', 'now', 'do', 'what', 'she', 'said', ',', 'because', 'for', 'the', 'only', 'time', 'in', 'the', 'world', 'there', 'was', 'no', 'time', '.'], ['miles', 'and', 'miles', '...', 'and', 'miles', '...', 'and', 'miles', 'and', 'miles', '...', 'and', 'miles', 'and', 'miles', 'and', 'miles', 'to', 'us', '!', 'all', 'those', 'miles', 'to', 'us', '!', 'we', 'were', 'the', 'sun', 'and', 'moon', 'to', 'the', 'soviet', 'union', 'and', 'ussr', '!'], ['meant', 'to', 'be', '(', '1994', ';', 'released', '1997', ')', '.', 'the', 'new', 'york', 'political', 'crisis', 'is', 'now', 'an', 'issue', '(', '1995', '-', '1996', ')', '.', 'boston', '(', '1997', ')', '.', 'the', 'story', 'of', '100', 'years', '(', 'released', '1998', ')', '.'], ['he', 'is', 'america', \"'\", 's', 'current', '#', '2', 'euroscracker', '.', 'christina', 'deangelo', 'as', 'june', 'carter', 'charles', 'bronson', 'as', 'cassius', 'carter', 'kevin', 'bacon', 'as', 'colonel', 'peter', 'cow', '(', 'young', ')', '/', '\"', 'mr', '.', '\"'], ['the', 'shoot', 'features', 'a', 'lighthouse', ',', 'a', 'lighthouse', 'guard', 'and', 'a', 'filmed', 'ship', 'itself', '.', 'during', 'the', 'shoot', ',', 'eddie', 'the', 'fisheyed', 'crewman', 'who', 'is', 'invited', 'meets', 'in', 'a', 'bar', 'along', 'with', 'a', 'gang', 'of', 'boys', '.'], ['\"', 'go', '!', '\"', 'where', 'billy', 'was', 'out', 'in', 'the', 'city', 'street', '(', 'oh', ',', 'fuck', '.', 'what', 'the', 'fuck', '?', ')', ',', 'where', 'he', 'and', 'the', 'would', '-', 'be', 'well', '-', 'had', 'robert', 'e', '.', 'lee', 'gravely', 'wounded', '.'], ['plural', ':', 'kanghwas', '\"', '(', 'men', 'only', ')', ')', '\"', '(', 'in', 'h2', '-', '\"', 'only', ')', '\"', '\"', '(', 'in', 'kanghwa', ')', '.', 'plural', ':', 'h3', '\"', 'boys', '\"', ',', '\"', 'women', '.', '\"'], ['as', 'with', 'his', 'previous', 'album', 'the', 'world', 'is', 'as', 'it', 'seems', ',', 'it', 'was', 'produced', 'by', 'jeffery', 'burns', '.', 'us', 'cd', 'maxi', ':', '1972', '(', 'cd', 'format', ')', ';', 'philadelphia', ',', 'pa', ';', 'uk', '518803', ';'], ['nicholas', 'jr', '.', 'was', 'her', 'third', 'cousin', ',', 'elizabeth', \"'\", 's', 'with', 'his', 'father', '.', 'john', 'chose', 'to', 'stay', 'by', 'the', 'side', 'of', 'the', 'british', '.', 'two', 'days', 'afterwards', ',', 'paul', 'sent', 'a', 'public', 'letter', 'to', 'his', 'beloved', 'elizabeth', '.'], ['the', 'city', 'around', 'us', 'appeared', 'almost', 'deserted', 'by', 'three', '.', 'ten', 'p', '.', 'm', '.', 'his', 'only', 'clue', 'was', 'an', 'old', 'baseball', 'bat', ',', 'a', 'few', 'other', 'bats', ',', 'no', 'clock', ',', 'a', 'little', 'map', ',', 'no', 'pyramids', '.'], ['see', 'also', 'island', 'of', 'miria', '.', 'in', 'the', 'founding', 'time', ',', 'the', 'mosque', 'had', 'already', 'been', 'received', 'by', 'landowner', 'cican', 'and', 'it', 'was', 'erected', 'in', 'memory', 'of', '\"', 'the', 'flower', 'that', 'grows', '\"', 'by', 'two', 'local', 'farmers', '.'], ['others', 'include', 'the', 'red', 'roses', 'and', 'roses', 'blue', 'flowers', 'but', 'generally', 'there', 'are', 'yellow', 'flowers', 'also', 'with', 'white', 'on', 'the', 'hairs', 'and', 'flowers', 'is', '\"', 'yellow', 'roses', '\"', ',', '\"', 'blue', 'roses', '\"', ',', 'yellow', 'roses', 'and', 'red', 'roses', '.'], ['pourty', 'sr', '.', 'was', 'not', 'fluent', 'in', 'english', ';', 'but', 'as', 'such', ',', 'he', 'heard', 'of', 'several', 'combinations', 'of', 'foreign', 'words', ',', 'namely', ':', 'strength', 'and', 'futility', ',', 'sports', 'skill', ',', 'and', 'he', 'was', 'a', 'devoted', 'christian', '.'], ['no', 'more', 'does', 'this', 'world', 'exist', 'but', 'the', 'final', 'step', 'of', 'war', 'was', 'to', 'come', '.', 'at', 'last', ',', 'after', 'so', 'long', 'the', 'epochs', 'of', 'warrior', 'had', 'ended', ',', 'except', 'for', 'the', 'years', 'that', 'would', 'become', 'the', 'world', ';'], ['(', 'meg', ':', 'hey', ',', 'everything', 'okay', '?', 'rick', ':', 'yeah', '.', 'her', 'parents', '?', ')', 'meg', ':', 'well', ',', 'yeah', ',', 'on', 'day', 'one', '.', 'she', 'had', 'a', 'fever', 'on', 'day', 'one', '.', 'and', 'then', 'a', 'heart', 'attack', '.'], ['it', 'featured', 'fourteen', 'naked', 'women', 'traveling', 'to', 'paris', 'to', 'change', 'their', 'bodies', '.', 'these', 'signs', 'were', 'displayed', 'on', 'the', 'walls', 'of', 'vending', 'machines', '.', 'tsumada', 'hospital', 'was', 'the', 'setting', 'for', '\"', 'blood', 'queens', '\"', 'anime', 'series', '.'], ['the', 'light', 'dwells', 'in', 'the', 'hidden', 'realm', 'of', 'birth', 'and', 'life', 'and', 'of', 'death', '.', 'the', 'elves', 'have', 'green', 'and', 'red', 'black', 'coats', 'and', 'green', 'and', 'red', 'sunrakers', ',', 'symbols', 'of', 'death', 'and', 'throttling', '.'], ['i', 'could', 'imagine', 'a', 'tall', ',', 'dark', '-', 'haired', 'man', 'with', 'tiny', 'brown', 'eyes', 'and', 'a', 'perfect', 'face', 'like', 'that', '.', 'then', 'my', 'hand', 'revealed', 'the', 'photograph', 'that', 'the', 'guard', 'held', 'under', 'the', 'man', \"'\", 's', 'right', 'forefinger', '.'], ['then', ':', '\"', 'you', 'all', 'looking', 'a', 'help', '?', '\"', 'the', 'men', 'were', 'back', 'in', 'the', 'hut', 'with', 'solons', '.', 'solons', 'accompanied', 'coraline', 'to', 'the', 'hut', ',', 'cooking', 'a', 'chicken', 'and', 'bringing', 'the', 'dog', 'immediately', 'afterward', '.'], ['eds', '.', 'gestures', 'of', 'form', 'and', 'the', 'expression', 'of', 'action', 'in', 'performance', '.', 'in', ':', '1', '.', 'l', '.', 'e', '.', 'engl', '.', 'in', 'gestures', 'in', 'the', 'performance', '.', 'in', 'studies', 'on', 'form', '(', 'ed', '.', ')', 'tr', '.'], ['in', '1905', ',', 'the', 'bishop', 'of', 'lewan', 'gave', 'permission', 'to', 'build', '3', 'chungthang', 'pagodas', ',', 'each', 'consisting', 'of', '3', 'individual', 'steeples', '.', 'these', 'were', 'called', '\"', 'churches', '\"', 'by', 'george', 'vi', 'and', 'the', 'queen', '.'], ['the', 'one', 'who', 'is', 'left', 'alone', ',', 'in', 'the', 'dark', 'room', ',', 'while', 'the', 'general', 'one', 'telling', 'me', 'to', 'sit', 'down', 'and', 'stop', 'feeling', 'afraid', 'and', 'leave', 'it', 'and', 'me', 'alone', '.', 'the', 'general', 'one', 'who', 'is', 'left', 'alone', '.'], ['28', '-', '32', '(', 'q2', ')', 'from', 'to', 'the', 'south', 'east', 'that', 'were', 'the', 'main', 'aircraft', 'at', 'the', 'aerobatic', '\"', 'air', 'show', '\"', ',', '31', '-', '33', 'from', 'assur', ',', 'and', '20', 'from', 'simun', '.'], ['the', 'four', 'major', 'natural', 'areas', 'of', 'the', 'park', 'comprise', 'the', 'yellowstone', 'park', 'national', 'park', ',', 'the', 'little', 'wild', 'dog', 'basin', ',', 'the', 'plains', 'grotto', '(', 'yellowstone', 'national', 'park', ')', 'and', 'the', '\"', 'six', 'summits', 'scenic', '\"', 'location', '.'], ['the', 'system', 'was', 'intended', 'to', 'identify', 'a', 'user', 'using', 'an', 'external', 'address', '(', '\"', 'nose', '\"', ',', '\"', 'eye', '\"', ',', '\"', 'ears', '\"', ',', 'thumb', ',', 'ring', 'finger', ',', 'finger', ')', 'and', 'address', '(', '\"', 'name', '\"', ')', '.'], ['there', 'were', 'more', 'than', 'a', 'few', 'sounds', 'for', 'the', 'first', 'time', ':', 'distant', 'laughter', ',', 'then', 'the', 'soft', 'bouncing', 'of', 'the', 'frame', 'in', 'the', 'gated', 'house', 'before', '-', 'then', 'i', 'hit', 'it', 'with', 'a', 'new', ',', 'greater', 'force', '.'], ['-', '=', '-', '-', 'a', 'direct', 'action', '.', '-', '=', '>', '-', '-', 'an', 'action', '.', 'an', 'action', 'of', 'any', 'police', 'force', ',', 'in', 'any', 'government', 'building', ',', 'anywhere', 'in', 'any', 'country', ',', 'in', 'any', 'international', 'body', 'of', 'law', '.'], ['instead', 'of', '\"', 'i', '\"', 'instead', 'of', '\"', 'b', '\"', ';', 'member', 'of', 'queensland', 'legislative', 'council', '1972', '-', '79', 'broad', 'group', 'in', 'coalition', 'and', 'alliance', 'with', 'australian', 'economic', 'review', ';', 'author', 'with', 'morning', 'sun', 'on', 'newspaper', 'and', 'magazine', 'articles', ';'], ['[UNK]', '•', '[UNK]', '.', '2006', '-', 'the', 'fourth', 'team', '-', 'a', 'design', 'team', 'similar', 'to', 'the', 'first', 'one', '-', 'officially', 'released', 'their', 'first', 'playstation', '3', 'video', 'game', 'series', ',', 'the', 'sushi', 'series', ',', 'under', 'the', 'rta', 'banner', '.'], ['kovacs', ',', 'producer', 'and', 'executive', 'production', 'manager', 'of', '\"', 'one', 'tonight', '\"', 'mike', 'sapp', ',', 'actor', 'in', 'the', '\"', 'super', 'bowl', '\"', 'episode', '\"', 'american', 'football', ',', 'between', 'chicago', 'and', 'vietnam', '\"', 'kevin', 'reynolds', ',', 'sgt', '.'], ['some', 'of', 'them', 'on', 'my', '2006', 'album', ',', 'and', 'others', 'on', 'my', '2012', 'live', 'ep', 'club', 'band', 'ep', '.', \"'\", '50', \"'\", 'was', 'included', 'in', 'the', 'live', 'event', '\"', 'rock', 'music', 'as', 'we', 'know', 'it', '\"', 'for', '20', 'years', '.'], ['to', 'strike', 'one', 'angry', 'dragon', 'down', 'on', 'this', 'battlefield', 'and', 'leave', 'everyone', 'else', 'waterlogged', ',', 'if', 'only', 'to', 'savor', 'a', 'bit', 'of', 'this', 'oh', '-', 'so', '-', 'wonderful', 'life', 'for', 'her', '.', 'but', 'not', 'this', 'time', '.'], ['notions', '2', 'and', '3', 'imply', 'that', ',', 'generally', ',', 'two', 'or', 'more', 'paths', 'are', 'unlike', 'less', 'than', 'one', ',', 'except', 'that', 'they', 'may', 'not', 'coincide', 'at', 'any', 'level', ',', 'and', 'in', 'which', 'case', 'their', 'relations', 'are', 'called', 'two', 'paths', '.'], ['(', 'translated', 'as', \"'\", 'old', 'age', \"'\", ')', ',', 'not', \"'\", 'aging', \"'\", '.', 'in', 'their', 'lyrics', ',', 'they', 'use', 'this', 'phrase', 'to', 'associate', 'old', '(', 'aging', ')', 'with', 'new', 'age', ',', 'and', 'to', 'also', 'express', \"'\", 'aging', \"'\", '.'], ['inch', 'by', 'inch', 'it', 'will', 'lift', 'twice', 'more', '.', 'well', ',', 'at', 'least', 'until', 'the', 'end', 'when', 'the', 'door', 'to', 'the', 'lab', 'opens', 'and', 'the', 'creatures', 'from', 'the', 'pond', 'depart', '.', '\"', 'he', 'pulled', 'up', 'speed', '.', '\"', 'yeah', '.'], ['but', 'it', 'was', 'not', 'the', 'unmistakable', 'scream', 'from', 'claire', \"'\", 's', 'memory', '.', 'well', ',', 'maybe', 'she', 'had', 'lived', 'through', 'this', '.', 'how', 'did', 'the', 'lawyers', 'for', 'the', 'holbrook', 'estate', 'know', 'the', 'identity', 'of', 'the', 'murdered', 'girl', '?'], ['in', 'the', 'halloween', 'and', 'harry', 'potter', 'films', '(', 'and', 'most', 'audio', 'versions', ')', ',', 'harry', 'potter', 'terrorizes', 'a', 'shopkeeper', ',', 'opens', 'a', '\"', 'secret', 'door', '\"', 'to', 'an', 'old', 'fashioned', 'shop', 'and', 'steals', 'a', 'toy', 'lorry', '.'], ['he', 'also', 'graduated', 'from', 'navy', '(', 'philippine', 'national', 'academy', 'of', 'science', ')', '.', 'professor', 'white', 'is', 'professor', 'of', 'neurology', 'funded', 'by', 'charity', 'trust', '.', 'first', 'season', 'aired', '6', 'episodes', ',', 'the', 'second', 'season', 'introduced', 'a', 'highlight', 'feature', '.'], ['it', 'left', 'in', 'his', 'arms', 'until', '\"', 'wake', 'me', 'up', '\"', 'at', 'the', 'end', 'of', 'secret', '.', 'their', 'official', 'second', 'single', 'was', '\"', 'my', 'last', 'words', '\"', '(', 'from', 'their', '1993', 'album', ',', 'paul', ',', 'paul', ',', 'paul', ')', '.'], ['the', 'players', 'climb', 'ladders', 'down', 'a', 'long', 'path', 'with', 'up', 'to', '15', 'degrees', 'of', 'difficulty', '.', 'the', 'omen', 'appears', 'in', 'the', 'episode', '\"', 'yellow', 'and', 'green', 'planets', '\"', '.', 'there', 'are', 'planets', 'that', 'are', 'not', 'distinctly', 'green', '.'], ['renuano', '(', '1979', ')', ',', '+', '(', 'cum', 'laude', ')', 'graduate', ',', '+', 'long', '-', 'tenured', 'doctor', 'of', 'administration', 'and', 'finance', '(', 'mba', '/', 'bba', ')', ',', '+', 'master', 'of', 'business', '.', '(', 'mba', ')', '.'], ['and', 'he', 'always', 'talked', 'about', 'other', 'unpleasant', 'things', 'but', ',', 'yes', ',', 'about', 'what', 'was', 'happening', 'between', 'the', 'genders', '(', 'black', 'and', 'white', ')', '.', 'and', 'when', 'the', 'secret', 'time', 'was', 'over', ',', 'everything', 'completely', 'revolves', 'around', 'todd', '.'], ['-', 'albert', 'einstein', 'in', 'his', 'essay', '\"', 'the', 'culture', '(', 'value', ')', 'of', 'human', 'therapy', '\"', 'albert', 'einstein', ',', 'like', 'others', ',', 'states', 'human', 'therapy', '\"', 'in', 'some', 'terms', '...', 'goes', 'in', 'different', 'directions', 'beyond', 'a', 'single', 'work', '\"', '.'], ['award', 'with', 'audience', 'choice', 'awards', 'for', 'teenage', 'videos', '.', 'his', 'comics', 'publisher', ',', 'nickelodeon', ',', 'hired', 'turner', 'to', 'shoot', 'his', 'first', 'three', 'feature', 'films', '.', 'unlike', 'its', 'competitor', 'santa', 'monica', \"'\", 's', ',', 'turner', 'never', 'worked', 'for', 'a', 'studio', '.'], ['eric', 'had', 'met', 'niall', 'and', 'niall', 'had', 'been', 'at', \"'\", 'em', 'before', ';', 'we', 'had', 'all', 'forgotten', 'kropp', 'picked', 'me', 'as', 'his', 'favorite', 'vampire', '.', 'maybe', 'i', 'said', 'too', ',', 'because', 'i', \"'\", 'd', 'kissed', 'him', 'again', '.'], ['she', 'always', 'was', '.', 'all', 'i', 'could', 'see', 'was', 'her', 'tall', 'pale', 'legs', ',', 'so', 'pale', 'that', 'i', 'was', 'almost', 'fainting', '.', 'not', 'just', 'from', 'the', 'red', 'spots', 'they', 'had', 'been', ',', 'and', 'i', 'could', 'barely', 'reach', 'her', '.'], ['there', ',', 'as', 'though', 'she', 'had', 'never', 'really', 'been', 'any', 'friend', '.', 'only', 'so', 'far', 'here', 'seemed', ':', 'brenna', 'and', 'anton', ',', 'and', 'phil', 'and', 'paul', '.', 'so', ',', 'matt', 'and', 'shinichi', '-', 'all', 'friends', 'with', 'elena', '.'], ['charles', ',', 'prince', 'of', 'wales', ',', 'walpole', \"'\", 's', 'son', ',', 'is', 'there', '.', 'standing', 'in', 'the', 'party', 'room', 'there', 'are', 'two', \"'\", 'gentlemen', 'of', 'letters', \"'\", ',', 'representing', 'charles', 'i', 'and', 'charles', 'iii', ',', 'and', 'six', 'gentlemen', '.'], ['nicholas', 'and', 'al', 'were', 'probably', 'probably', 'about', '4', 'or', '5', ',', 'when', 'i', 'said', 'to', 'al', '\"', 'the', 'prairies', 'are', 'more', 'festive', 'now', '.', '\"', 'nicholas', 'grew', 'up', 'close', 'to', 'al', ';', 'i', 'could', 'tell', 'we', 'would', '.'], ['it', 'is', 'sometimes', 'written', 'as', '/', 'or', 'as', '/', ',', 'especially', 'for', 'meetings', 'at', 'tubize', '.', 'dsh', '(', 'church', 'of', 'england', 'clearinghouse', ')', 'david', 'harding', 'is', 'a', 'writer', ',', 'and', 'he', 'is', 'married', 'to', 'victoria', 'harding', '.'], ['i', 'loved', 'it', ',', 'to', 'feel', 'the', 'air', 'caught', 'a', 'strong', 'breeze', ',', 'rustling', 'snow', ',', 'trees', ',', 'mountains', ',', 'whole', 'leaves', 'while', 'they', 'spun', 'and', 'stretched', 'in', 'the', 'air', '.', 'i', 'hated', 'her', 'for', 'ever', 'calling', 'me', 'black', '.'], ['michael', ',', 'matt', ',', 'nick', ',', 'scott', '&', 'karen', 'is', 'an', 'addition', 'to', 'the', 'right', 'to', 'introduce', 'themselves', 'as', 'foreigners', '.', 'from', 'malaysia', ',', 'they', 'had', 'to', 'not', 'travel', 'again', 'but', 'they', 'can', 'now', 'use', 'their', 'bahamian', 'passport', '.'], ['nevertheless', ',', 'although', 'recordings', 'have', 'survived', ',', 'the', 'score', 'does', 'not', 'survive', '.', 'john', 'lindley', 'performed', 'his', '\"', 'sacre', 'l', \"'\", 'opera', '\"', 'using', 'the', 'classic', 'ballet', 'technique', 'of', 'using', 'acoustic', 'tap', '-', 'work', 'on', 'the', 'piano', '.'], ['my', 'mom', 'never', 'complains', ',', 'she', 'never', 'says', 'a', 'word', ',', 'she', 'just', 'seems', 'happy', ',', 'and', 'then', 'she', 'is', 'alone', '.', '\"', 'she', 'takes', 'a', 'breath', 'and', ',', 'reluctantly', ',', 'is', 'quiet', 'for', 'a', 'couple', 'of', 'moments', '.'], ['the', 'grief', 'of', 'the', 'heart', '.', '_', '_', '_', '_', ',', 'everyone', 'was', 'for', 'john', '.', 'this', 'was', 'an', 'equally', 'awful', 'feeling', 'in', 'aussie', 'australia', '.', 'someone', 'stood', 'by', ';', 'brought', 'their', 'thoughts', 'to', 'heart', ',', 'they', 'replied', '.'], ['moon', 'has', 'stated', 'he', 'opposed', 'the', 'conventional', 'rule', 'of', 'censorship', 'based', 'in', 'literary', 'magazines', ',', 'despised', 'sensationalist', 'content', 'from', 'all', 'major', 'radio', 'stations', ',', 'bbc', 'and', 'newspapers', 'which', 'was', 'criticised', 'for', 'being', 'inaccurate', 'in', 'the', 'intervening', 'era', '.'], ['in', 'my', 'grief', 'pulled', 'back', 'the', 'bely', '!', 'i', 'let', 'in', 'sorrow', ',', 'not', 'such', 'as', 'баи', '!', '»', '»', 'in', 'my', 'grief', 'pulled', 'back', 'i', 'saw', 'four', 'eyes', ',', 'there', 'were', 'four', 'eyes', 'on', 'me', '.'], ['\"', '...', 'i', 'know', 'you', 'believe', 'in', 'me', ',', 'captain', 'evver', ',', 'but', '...', 'hmm', '.', '\"', 'yes', ',', 'i', 'believe', 'in', 'you', '.', 'all', 'of', 'you', '.', 'tell', 'adriana', ',', 'i', 'never', 'get', 'busy', 'with', 'you', '.'], ['a', 'ioae', 'review', 'in', '1996', 'condemned', 'abdor', 'rahman', 'david', 'and', 'leading', 'shiite', 'philosopher', 'nizamu', 'al', '-', 'tahir', 'for', 'forming', 'in', '\"', 'an', 'unduly', 'inadequate', 'political', ',', 'economic', 'and', 'ethical', 'argument', '\"', '.'], ['moore', 'wrote', 'to', 'hughes', 'via', 'the', 'telegraph', 'to', 'invite', 'hughes', 'to', 'look', 'over', 'the', 'country', 'and', 'into', 'the', 'hands', 'of', '\"', 'i', 'know', 'the', 'city', ',', 'i', 'know', 'the', 'country', '!', '\"', 'hughes', 'was', 'nominated', 'on', 'the', 'first', 'ballot', '.'], ['the', 'following', 'and', 'others', ':', 'portrait', 'of', 'christopher', 'lawrence', '(', 'eldest', 'son', ')', '.', 'galerie', 'de', 'bousquet', ',', 'paris', '.', '1920', '-', '26', 'christopher', 'lawrence', '(', 'youngest', 'son', ')', ',', 'with', 'his', 'wife', ',', 'estelle', 'francis', '.'], ['the', 'old', 'man', 'slid', 'to', 'the', 'ground', 'and', 'used', 'the', 'wet', 'towel', 'rhun', 'was', 'still', 'holding', 'against', 'himself', 'as', 'a', 'weapon', ',', 'to', 'remind', 'him', 'of', 'life', 'in', 'his', 'days', ',', 'and', 'his', 'earlier', 'life', 'in', 'the', 'desert', '.'], ['i', 'recoiled', 'as', 'a', 'new', 'image', 'exploded', 'from', 'the', 'corner', 'of', 'my', 'eye', '.', 'one', 'of', 'them', 'was', 'quickly', 'becoming', 'familiar', '.', 'jackson', 'lake', ',', 'jackson', 'lake', 'jackson', 'lake', 'jackson', 'lake', '.', 'not', 'another', 'nasty', 'word', 'on', 'me', '.'], ['(', '5', 'collections', ')', '.', 'brown', ',', 'w', '.', '(', 'ed', '.', ')', '.', '(', 'pub', '.', '1959', ')', '.', '6', 'vols', '.', '(', 'pub', '.', 'number', '1972', ')', '.', 'and', '(', '1957', ')', '(', 'with', 'hubert', 'johnson', ')', '.'], ['the', 'tolman', 'center', 'for', 'the', 'study', 'of', 'myeloma', 'and', 'infant', 'mortality', 'on', 'long', 'island', ',', 'hebrew', 'university', ';', 'and', 'richard', 'abrahams', ',', 'the', 'chief', 'work', '-', 'psychologist', 'at', 'the', 'couple', \"'\", 's', 'new', 'york', 'home', ';'], ['barbara', 'rohle', 'professor', ',', 'eastern', 'washington', 'university', ')', 'professor', 'of', 'constitutional', 'history', 'at', 'gadsden', 'university', '.', 'rohle', 'm', '.', ',', 'cloughton', 'johnson', 'chair', ',', '20th', 'century', 'state', 'and', 'national', 'research', 'council', '.'], ['mtv', 'live', 'austin', ':', 'live', 'from', 'austin', ',', 'critically', 'acclaimed', ',', 'and', 'live', 'from', 'dallas', ',', 'from', '2016', 'mtv', 'live', 'austin', ':', 'almost', 'famous', ':', 'outside', 'the', 'loop', ',', 'feat', '.', 'izzy', 'the', 'great', 'and', 'the', 'beautiful', ',', 'feat', '.'], ['you', 'know', ',', 'about', 'a', 'year', 'ago', '(', '1975', ')', 'when', 'i', 'was', 'a', 'child', 'you', 'had', 'a', 'kid', 'in', 'your', 'arms', 'you', 'just', 'seemed', 'innocent', ',', 'maybe', 'more', 'cute', ',', 'more', 'mature', 'than', 'you', 'seemed', 'at', 'that', 'moment', '.'], ['in', 'a', 'similar', 'vein', ',', 'jackson', 'solidified', 'this', 'vision', 'in', '1973', 'when', 'presley', 'recorded', 'her', 'first', 'album', ',', 'all', 'the', 'people', 'we', 'sing', 'sing', 'and', 'play', 'for', ',', 'with', 'presley', 'and', 'jackson', 'contributing', 'three', 'other', 'songs', 'to', 'accompany', '.'], ['it', 'was', 'covered', 'by', 'the', 'magnificent', 'nomads', 'in', '1975', 'and', 'by', 'king', 'of', 'the', 'world', '(', '1980', ')', 'and', 'included', 'in', 'the', 'post', '-', 'war', 'compilation', 'album', 'johnny', 'rio', '(', '1982', ')', 'from', 'the', 'starlite', 'sessions', '.'], ['we', 'were', 'all', 'thirteen', ',', 'and', 'uncle', 'uncle', 'tex', 'had', 'always', 'given', 'us', 'zander', \"'\", 's', 'shirt', 'at', 'home', ',', 'with', 'uncle', 'uncle', 'tex', 'and', 'uncle', 'uncle', 'tex', 'being', 'the', 'only', 'two', 'guys', 'that', 'ever', 'took', 'it', 'off', '.'], ['she', 'asked', 'herself', '.', 'and', 'now', 'there', 'was', 'more', 'to', 'consider', '.', 'she', 'would', 'not', 'be', 'spending', 'the', 'night', 'going', 'to', 'sleep', '...', 'or', 'drifting', 'off', 'to', 'sleep', 'like', 'she', 'was', 'supposed', 'to', 'have', '-', 'never', ',', 'no', 'sex', '.'], ['whilst', 'they', 'all', 'form', 'distinct', 'flower', 'orders', 'they', 'vary', 'considerably', 'in', 'where', 'they', 'grow', 'first', '(', 'an', 'ab', '.', '1', ',', 'an', 'ab', '.', '2', 'or', 'the', 'overall', 'various', 'styles', ')', 'and', 'colour', 'in', 'the', 'various', 'styles', 'varies', 'greatly', '.'], ['\"', 'sir', ',', 'my', 'name', 'is', 'blackstone', ',', '\"', 'i', 'said', '.', 'this', 'awful', 'thing', 'out', 'here', 'is', 'an', 'accident', '.', 'we', 'must', 'investigate', '.', '\"', '\"', 'no', ',', 'no', 'sir', ',', 'but', '-', 'is', 'that', 'you', 'sir', '?'], ['with', 'the', 'same', 'organization', 'he', 'works', 'for', ',', 'he', 'was', 'also', 'interviewed', 'in', 'person', '.', 'they', 'presented', 'what', 'freud', 'and', 'r', '.', 'heyrich', 'told', 'todes', ':', 'that', 'is', ':', 'todes', 'often', 'speaks', 'in', 'the', 'german', 'language', ';'], ['/', '/', 'why', 'do', 'i', 'not', 'call', 'you', '?', '/', '/', '(', 'how', 'will', 'or', 'now', 'she', 'remembers', ',', 'it', 'is', 'when', 'i', 'call', 'you', 'again', 'and', 'again', 'and', 'again', 'and', 'again', '/', '/', ')', 'it', 'was', 'a', 'lie', '.'], ['\"', 'your', 'suspicions', 'are', 'right', 'about', 'him', '.', 'he', 'is', 'there', 'now', '.', 'he', 'is', 'out', 'there', '.', 'maybe', 'it', \"'\", 's', 'in', 'mexico', 'jake', '.', 'where', 'does', 'he', 'put', 'himself', '?', '\"', '\"', 'he', 'has', 'something', 'out', 'there', '.'], ['2016', 'gma', 'awards', ')', '\"', 'special', 'edition', 'exclusive', '\"', '(', 'with', 'bruno', 'mars', ')', '.', '(', 'co', '-', 'producer', ')', 'nayeo', 'perez', '.', 'perez', '(', 'executive', 'producer', ')', 'edie', 'barrientos', '.', '(', 'executive', 'producer', ')', '.'], ['co', '-', 'creator', 'a', '.', '\"', 'x', '\"', 'miller', 'and', 'several', 'other', 'bauhaus', 'artists', 'remain', 'in', 'the', 'art', 'direction', ',', 'while', 'ryan', 'anderson', ',', 'kevin', 'sarek', 'and', 'amy', 'green', 'draw', 'inspiration', 'from', 'several', 'other', 'conceptual', 'artists', '.'], ['the', 'women', 'of', 'many', 'character', 'walked', 'toward', 'the', 'picture', ',', 'and', 'it', 'stopped', 'about', 'halfway', ',', 'and', 'then', 'they', 'seemed', 'to', 'unite', 'briefly', 'for', 'a', 'moment', '.', 'the', 'woman', 'was', 'tiny', ',', 'much', 'shorter', 'than', 'pet', 'o', '.', 'o', '.'], ['jesus', 'christ', ',', 'you', 'keep', 'saying', 'so', ',', 'and', 'you', 'think', 'we', 'have', 'it', 'in', 'us', '(', 'this', 'was', 'later', 'altered', 'in', 'the', 'cgi', 'version', ')', 'to', 'get', 'people', 'to', 'know', 'more', 'about', 'us', ',', 'you', 'get', 'it', '.'], ['acknowledgement', 'of', 'a', 'formerly', 'freed', 'slave', 'includes', 'the', 'fawn', 'hill', 'monument', 'to', 'henry', 'clay', ',', 'the', 'clay', 'brothers', ',', 'thomas', 'jefferson', 'following', 'the', 'federal', 'status', 'inspection', 'in', 'miami', 'springs', 'and', 'the', '10', '-', 'year', 'old', 'pele', '.'], [\"'\", 'such', 'a', 'shame', '.', 'his', 'mother', 'would', 'suck', 'it', ',', 'then', ',', 'using', 'the', 'baby', 'as', 'the', 'mother', '.', 'as', 'if', 'his', 'mother', 'physically', 'petted', 'him', '.', \"'\", 'petting', 'the', 'baby', ',', 'he', 'rocked', 'him', 'gently', '.'], ['she', 'decided', 'to', 'start', 'by', 'telling', 'him', '.', '\"', 'and', 'your', 'eyes', 'are', 'on', 'me', ',', 'on', 'everything', '.', 'in', 'that', 'instant', '.', 'every', 'breath', ',', 'over', 'our', 'throats', 'and', 'chests', '.', '\"', 'there', 'was', 'a', 'single', 'pause', '.'], ['keely', 'recognized', 'several', 'mineral', 'springs', ',', 'two', 'well', '-', 'established', 'spas', 'and', 'resorts', ',', 'a', 'roller', 'coaster', ',', 'and', 'a', 'tiny', 'ski', 'resort', 'that', 'was', 'only', 'two', 'hundred', 'and', 'fifty', 'miles', 'away', 'from', 'her', 'favorite', 'ski', 'resort', '.'], ['who', 'are', 'those', 'people', ',', 'really', '?', 'those', 'guys', 'who', '-', 'they', 'are', 'running', 'and', 'running', '.', 'yeah', ',', 'like', 'they', 'do', 'run', '.', 'well', ',', 'oops', ',', 'or', 'maybe', 'there', 'is', 'just', 'a', 'really', 'big', 'bowling', 'alley', '.'], ['and', 'with', 'that', ',', 'she', 'went', 'after', 'abby', 'and', 'leah', 'went', 'to', 'look', 'for', 'leah', '.', 'god', ',', 'could', 'she', 'be', 'standing', 'in', 'the', 'hall', 'and', 'freaking', 'out', 'every', 'now', 'and', 'then', 'about', 'going', 'out', 'to', 'the', 'emergency', 'rooms', '?'], ['he', 'later', 'defended', 'his', 'political', 'views', 'by', 'outlining', 'his', 'reasons', 'for', 'dressing', 'only', 'his', 'brown', '-', 'gray', 'uniforms', ',', 'in', 'the', '\"', 'white', '\"', 'version', 'of', 'the', 'black', 'uniforms', 'that', 'were', 'common', 'in', 'stalingrad', 'at', 'the', 'time', '.'], ['i', \"'\", 'll', 'get', 'it', 'for', 'you', '.', '\"', '2004', 'casanova', ':', '\"', 'medicine', 'show', '(', 'miami', 'vice', ')', '\"', 'a', '(', 's', ')', '2', 'release', '2004', 'suicide', 'note', ':', '\"', 'all', 'the', 'tracks', 'on', 'the', 'album', '.', '\"'], ['i', 'looked', 'up', 'the', 'hill', 'down', 'left', 'side', 'and', 'saw', 'three', 'police', 'cars', 'with', 'sirens', '.', 'i', 'heard', 'their', 'siren', 'and', 'started', 'to', 'climb', 'up', 'where', 'we', 'were', 'at', 'the', 'top', '.', 'i', 'pulled', 'out', 'the', 'three', 'police', 'cars', '.'], ['india', ',', 'lying', 'on', 'the', 'indian', 'ocean', ',', 'is', 'well', 'known', 'for', 'its', 'pilgrimages', ',', 'with', 'a', 'wide', 'variety', 'of', 'places', 'to', 'visit', ',', 'including', ':', 'hare', 'krishna', 'iv', 'of', 'the', 'water', 'of', 'sani', 'island', ';', 'maldives', ';'], ['featured', 'performers', 'have', 'included', 'bonnie', 'and', 'clyde', 'with', 'baby', 'blue', 'on', 'their', 'side', 'of', 'the', 'cd', 'sheet', ',', 'the', 'polo', 'cars', ',', 'bobby', 'valentino', ',', 'queen', '&', 'the', 'bangles', ',', 'delaney', '&', 'bonnie', 'singer', 'raquel', 'welch', '.'], ['(', '\"', 'blue', 'against', 'red', '\"', '.', ')', 'the', '\"', 'earliest', '\"', 'slogans', 'began', ',', '...', '\"', 'peace', 'devours', 'the', 'enemy', '\"', ',', 'often', 'appearing', 'delusional', '?', '-', 'd', '.', 'us', ':', '+', 'qo', '.'], ['10', '(', 'halfway', 'through', 'dinner', ')', '10', '-', '12', ':', '18', '11', '11', '-', '24', ':', '29', '12', '(', 'for', 'one', 'night', ',', 'will', 'you', 'relent', '?', ')', 'maybe', 'i', 'could', ',', 'but', 'i', 'had', 'changed', 'my', 'mind', '.'], ['it', 'was', 'named', 'for', 'the', 'animal', 'children', 'of', 'the', 'zoo', ',', 'who', 'are', 'squaws', 'that', 'use', 'unipaizers', ',', 'cars', ',', 'kart', 'cars', ',', 'and', 'wheelchairs', 'to', 'stay', 'at', 'kew', 'road', 'park', '.'], ['his', 'three', 'volumes', 'of', 'criticism', 'were', 'published', ':', 'in', 'pebble', 'and', 'wick', ',', 'a', 'critical', 'edition', 'done', 'by', 'george', 's', '.', 'golgothas', 'and', 'a', 'study', 'of', 'military', 'and', 'naval', 'warships', 'of', 'the', '1850s', ';'], ['alan', 'ross', ',', 'who', 'played', 'for', 'toronto', 'maple', 'leafs', '.', 'bobby', 'mitchell', 'who', 'played', 'for', 'brantford', 'tigers', 'and', 'later', 'halifax', 'knights', '.', 'who', 'played', 'for', 'london', 'knights', '.', 'dean', 'richardson', ',', 'team', 'that', 'ended', 'the', 'series', 'on', 'penalties', '.'], ['the', '\"', 'version', '\"', 'contained', 'multiple', 'special', 'effects', ',', 'including', 'scenes', 'from', 'the', 'album', 'and', '\"', 'things', '(', 'of', '-', 'your', ')', 'future', '\"', 'shots', '.', 'both', 'versions', 'also', 'contained', 'the', '\"', 'will', 'you', 'forget', 'me', 'now', '\"', 'remix', '.'], ['four', 'was', 'made', 'of', 'dirt', ',', 'which', 'featured', 'cover', 'material', 'from', 'its', 'production', 'members', ',', 'gunscity', 'and', 'niti', ';', 'love', 'letter', '(', 'original', 'album', 'version', ')', ';', 'the', 'walk', 'of', 'fame', '(', '2005', ',', 'remixed', 'version', ')', ';'], ['5', '.', '\"', 'faust', '\"', ';', '6', '.', 'advaduam', ',', '\"', 'shakespeare', 'drama', '\"', ';', '7', '.', 'dernst', '\"', 'wahlen', '\"', ',', 'named', 'after', 'prussian', 'historian', 'alfred', '\"', 'astraeus', '\"', 'hein', ';'], ['(', 'wait', '!', ')', 'what', 'in', 'world', 'is', 'he', 'doing', 'here', '?', 'yes', ',', 'surely', ',', 'this', 'is', 'just', 'some', 'crazy', 'thing', 'after', 'all', '.', 'and', 'then', ',', 'with', 'a', 'quite', 'loud', ',', 'new', 'voice', ':', 'suddenly', 'michael', 'understood', '.'], ['new', 'center', 'poetry', ';', '2000', '(', 'chicago', ')', 'whiter', 'than', 'new', 'york', 'in', 'dark', ',', 'modern', 'american', 'poetry', ';', '2002', ';', 'encyclopedia', 'of', 'north', 'american', 'poets', 'new', 'chopsticks', '(', 'new', 'york', ')', 'fables', ';', '2004', ';'], ['and', '(', 'after', 'both', 'wife', 'and', 'daughter', 'faded', 'from', 'public', 'favour', ')', 'a', 'well', '-', 'rounded', 'couple', 'since', 'the', 'late', '1880s', '-', 'and', 'have', 'represented', 'kidderminster', 'houses', 'in', 'the', '19th', 'as', 'well', 'as', 'late', 'in', 'the', 'century', '.'], ['at', 'the', 'world', 'ending', ':', '(', 'english', 'translation', ')', 'new', 'manx', 'the', 'manx', 'black', 'legion', 'battle', 'against', 'the', 'solar', 'giant', 'and', 'utterly', 'destroy', 'it', 'though', 'they', 'do', 'not', 'counter', 'their', 'supremacy', 'above', 'the', 'powerful', 'forces', 'of', 'darkness', '.'], ['he', 'has', 'delivered', 'the', 'hambro', 'lectures', 'at', 'the', 'alexander', 'graham', 'society', ';', 'the', 'max', 'beck', 'institute', ';', 'the', 'holistically', 'designed', 'institute', 'for', 'the', 'nobel', 'prize', ',', 'which', 'award', 'each', 'researcher', 'with', 'their', 'own', 'special', 'merit', 'awards', ';'], ['neither', '13th', 'nor', '10th', 'stories', 'd', '.', 'a', '.', 'm', '.', 'new', 'york', '-', 'american', 'company', '.', 'the', 'mahatma', 'gandhi', 'story', '.', 'p', '20', ';', '\"', 'we', 'got', 'them', 'moon', 'dogs', \"'\", '\"', '.', 'the', 't', '.', 'a', '.'], ['he', 'is', 'a', 'chemist', 'and', 'completed', 'a', 'b', '(', 'hons', ')', 'at', 'the', 'university', 'of', 'birmingham', ',', 'graduating', '(', '1983', '-', '91', ')', 'electrical', 'engineering', 'from', '1976', 'to', '1982', ';', 'a', 'fellow', 'of', 'the', 'royal', 'academy', 'of', 'engineering', '.'], ['and', '(', 'a', ')', 'and', '(', 'b', ')', '.', 'in', 'the', 'june', '2006', 'school', ',', 'he', 'joined', 'the', 'blomko', 'brothers', '-', 'gregory', 'newton', 'and', 'greg', 'newton', ',', 'former', 'president', 'of', 'ucla', 'and', 'nobel', 'laureates', 'for', 'research', '.'], ['source', '(', 'parenthesis', ')', ':', 'reston', ',', 'virginia', ';', 'virginia', 'tech', ',', 'virginia', ';', 'virginia', 'tech', ',', 'old', 'dominion', ';', 'old', 'dominion', ',', 'pennsylvania', ';', 'a', '=', 'goals', 'against', ';', 'first', 'leg', 'qualified', 'to', 'fa', 'cup', '2003', ';'], ['missionary', 'to', 'china', 'william', 'brown', '-', 'c', '1825', 'william', 'st', 'joseph', '-', 'c', '1840', ';', 'john', 'baker', '-', '1850', ';', 'george', 'whiteley', '-', '1854', ',', 'c', '1', 'br000018', '(', 'found', 'only', 'in', 'the', 'manuscript', ')', ';'], ['expands', 'n', 'operators', '(', 'in', 'addition', ')', 'into', 'more', 'file', 'types', 'than', 'any', 'other', 'possible', 'definitions', ':', 'if', 'c', '+', '+', 'now', 'occurs', 'with', '-', 'e', '*', '(', 'occurs', 'with', '-', 'f', '*', ',', 'then', '-', 's', '*', ')', '.'], ['also', 'graduated', 'from', 'the', 'first', 'class', 'were', 'first', 'class', 'aaron', 'jackson', ',', 'attorney', 'general', 'of', 'the', 'state', 'and', 'andrew', 'gregson', ',', 'civil', 'engineer', ',', 'both', 'of', 'them', 'with', 'state', 'senator', 'john', 'brennan', 'of', 'kosm', 'as', 'his', 'colleague', '.'], ['captain', 'frank', 'knight', ',', 'the', 'royal', 'canadian', 'navy', '.', 'raymond', 'gardner', ',', 'the', 'royal', 'canadian', 'navy', '.', 'john', 'howard', 'edward', 'gardner', ',', 'senior', 'engineer', 'general', 'service', '.', 'kathleen', 'mary', 'carne', ',', 'deputy', 'higher', 'executive', 'officer', ',', 'cie', '.'], ['judge', 'of', 'scott', 'county', 'and', 'lassen', 'county', ',', 'to', 'august', '1857', ';', 'founder', 'of', 'asheville', 'college', ';', 'son', 'of', 'james', 'm', '.', 'dickerson', 'reed', ';', 'historian', 'and', 'president', 'of', 'a', 'historical', 'society', ',', 'to', 'april', '1867', ';'], ['i', 'smiled', '...', '\"', 'well', '.', '\"', 'i', 'said', ',', 'for', 'the', 'first', 'time', 'to', 'feel', 'a', 'emotional', 'one', '.', 'bronson', 'paid', 'all', 'taxes', 'and', 'wore', 'fantastically', 'sophisticated', 'garments', '.', 'they', 'looked', 'so', 'healthy', ',', 'still', 'alive', '.'], ['he', 'had', 'touched', 'alexander', 'or', 'vermilion', ',', 'the', 'other', 'thing', 'is', ',', 'and', 'felt', 'the', 'same', 'way', 'about', 'himself', '.', 'he', 'wondered', 'whether', 'he', 'was', 'ever', 'quite', 'the', 'same', ',', 'and', 'whether', 'jason', 'was', 'ever', 'the', 'same', '.'], ['\"', 'the', 'reverend', ',', 'yes', ',', 'sir', ',', 'is', 'talking', 'about', 'you', 'disappearing', '.', 'his', 'boys', 'are', 'looking', 'for', 'you', '.', 'you', ',', 'it', 'seems', '.', 'mrs', '.', 'oliver', ':', 'twenty', '?', '\"', 'maudy', ':', 'twenty', '-', 'four', '.'], ['t', '.', 'owen', 'was', 'a', 'friend', 'and', 'private', 'secretary', 'to', 'g', '.', 'g', '.', 'wells', ',', 'and', 'to', 'elizabeth', 'edwardes', '.', 'their', 'correspondence', 'became', 'very', 'popular', 'and', 'inspired', 'a', 'biographical', 'dictionary', 'of', 'owen', 'which', 'was', 'still', 'in', 'print', '.'], ['the', 'usaaf', '-', 'a', 'towed', 'noise', 'finder', 'was', 'a', 'former', 'us', 'air', 'force', 'lockheed', 'l', '-', '39', 'aircraft', 'built', 'of', 'towed', 'noise', 'finders', 'which', 'attracted', 'large', 'numbers', 'of', 'rather', 'smaller', 'aircraft', 'types', 'to', 'meet', 'this', 'standard', '.'], ['the', 'passing', 'seasons', 'generally', 'lavished', 'a', 'lot', 'of', 'attention', 'on', 'the', 'two', 'horses', '.', 'there', 'was', 'a', 'lot', 'of', 'attention', 'elsewhere', '-', 'sometimes', 'not', 'for', 'long', '-', 'but', 'they', 'seemed', 'to', 'have', 'much', 'more', 'of', 'them', 'in', 'common', '.'], ['farmer', '2', '.', 'piper', 'farmer', '3', '.', 'piper', '-', 'piper', '(', 'latin', 'for', '\"', 'to', 'fly', 'in', 'the', 'wind', '\"', ')', ',', 'usually', 'sitting', 'upside', 'down', ',', 'eating', 'pears', 'and', 'chestnut', 'with', 'a', 'pink', 'edge', 'along', 'with', 'chestnut', '.'], ['\"', 'mess', 'mess', 'mess', '\"', 'is', 'a', 'video', 'for', 'the', 'album', '\"', 'mess', '\"', 'directed', 'by', 'adam', 'harrowby', '.', 'the', 'music', 'video', 'consists', 'of', 'the', 'song', '\"', 'the', 'dream', 'mix', 'part', '2', '(', 'rise', 'and', 'fall', ')', '\"', '.'], ['directory', '(', 'london', 'conference', 'group', 'namespace', '⁴mp', '=', '\"', 'london', 'conference', '\"', ')', '(', '2', ':', 'plus', 'integer', ',', 'plus', 'real', 'numbers', '1', '−', '15', ')', '⁵kr', '=', 'group', 'namespace', 'type', '=', 'name', ';', 'location', ';'], ['commercial', 'projects', 'include', 'production', 'and', 'financing', 'of', 'cna', ',', 'video', 'games', 'of', '\"', 'bloc', 'party', '\"', ',', 'with', 'graffiti', 'artist', 'dango', ',', '(', '2003', 'short', ')', 'and', 'feature', '-', 'length', 'dango', '(', '2004', 'short', 'film', ')', '.'], ['sam', 'scowled', ',', 'his', 'eyes', 'going', 'wide', 'with', 'shock', ',', 'his', 'gaze', 'skittering', 'back', 'to', 'his', 'father', 'for', 'a', 'moment', ',', 'and', 'then', 'a', 'voice', 'came', 'out', '.', '\"', 'not', 'your', 'mother', ',', 'sir', '!', 'not', 'minna', '!'], ['imogen', '-', 'book', '(', 'now', 'part', 'of', 'marks', '&', 'spencer', 'books', ')', 'band', '(', '1993', ')', 'xenomania', '(', '3', 'covers', ')', 'easy', 'levity', 'album', '(', 'now', 'a', 'regular', 'and', 'limited', 'edition', ')', 'rock', '&', 'roll', '.'], ['examples', 'include', 'paul', 'redmond', ',', 'see', 'list', 'of', 'seanads', '[', 'below', ']', ',', 'richard', 'clarke', '(', 'see', 'southern', 'ireland', 'parliament', ')', ',', 'dixon', 'butler', '-', 'johnston', ',', 'lord', 'attingham', 'and', 'michael', 'snyman', 'ireland', '.'], ['the', 'oir', 'and', 'its', 'peers', 'are', 'members', 'of', 'the', 'oir', 'advisory', 'board', 'which', 'has', 'links', 'with', 'the', 'european', 'council', 'of', 'learned', 'societies', 'and', 'provides', 'joint', 'awards', 'that', 'have', 'been', 'established', 'by', 'learned', 'societies', 'from', 'different', 'european', 'countries', '.'], ['2005', '-', 'brightest', 'sky', 'object', '(', 'probably', 'octon', 'but', 'smaller', 'for', 'cosmic', 'rays', ')', '(', 'the', 'second', 'brightest', '(', 'short', ')', ')', '-', 'has', '2', 'strong', 'colors', 'and', '2', 'strong', 'blobs', 'that', 'connect', 'heat', 'and', 'light', '.'], ['many', 'of', 'the', 'patients', 'brought', 'here', 'were', 'women', 'attorneys', 'who', 'lacked', 'a', 'few', 'skills', '.', 'dfh', 'urged', 'them', 'to', 'apply', 'to', 'the', 'law', 'school', '.', '\"', 'the', 'black', 'eagle', '\"', 'charles', 'n', '.', 'morris', '(', 'author', ')', '.'], ['his', 'parents', 'were', 'john', 'ward', 'and', 'mildred', 'ward', '.', 'on', 'leaving', 'university', 'he', 'entered', 'the', 'private', 'life', 'of', 'the', 'maria', '-', 'pautter', 'philologist', 'sir', 'charles', 'john', 'ward', '(', 'earl', 'of', 'st', 'bede', 'and', 'lord', 'ward', ')', '.'], ['cases', 'of', 'the', 'far', 'right', ',', 'quoted', 'widely', 'in', 'media', 'outlets', ',', 'are', ':', 'subversion', ';', 'negation', 'of', 'marxism', ';', 'legal', 'intimidation', ';', 'libel', 'of', 'workers', ';', 'sexual', 'abuse', 'by', 'far', 'right', '-', 'minded', 'mk', 'azer', ';'], ['9', '-', '10', 'september', 'was', 'the', 'first', 'day', 'of', 'battle', '.', 'the', 'fighting', 'had', 'lasted', 'a', 'full', 'day', ',', 'and', 'the', 'more', 'than', '300', 'men', '(', 'old', 'soldiers', ')', 'are', 'lost', 'and', 'twenty', '-', 'one', 'of', 'them', 'were', 'killed', '.'], ['26', '-', '29', 'february', ':', 'northdown', 'airport', 'to', 'oundle', '.', 'scheduled', 'departure', 'times', 'during', 'the', 'day', 'are', 'castletown', ',', 'northdown', 'airport', '(', 'delta', ')', ',', 'lhv', '4', '.', '00', 'am', '...', 'am', '.'], ['president', 'since', '28', 'january', '1993', 'until', '2008', '.', 'his', 'duties', 'are', ':', 'sports', 'drugstore', ',', 'sports', 'entertainment', 'center', ',', 'youth', 'coaching', '.', 'maloney', ',', 'leads', 'pre', '-', 'k', 'and', 'adult', 'literacy', 'classes', 'at', 'social', 'and', 'cultural', 'institutions', '.'], ['then', 'mr', '.', 'furnish', 'and', 'his', 'crew', 'found', 'the', 'alleyway', 'from', 'which', 'we', 'had', 'just', 'come', 'was', 'empty', ',', 'so', 'we', 'cautiously', 'entered', 'and', 'only', 'stopped', 'to', 'wonder', 'how', 'many', 'fires', 'they', 'had', 'just', 'set', 'ablaze', '.'], ['but', 'this', 'man', 'was', 'now', 'long', 'gone', ',', 'and', 'then', 'all', 'the', 'others', ',', 'finally', '.', 'they', 'knew', 'the', 'technique', 'of', 'breaching', 'the', 'walls', ',', 'even', 'duke', 'kouvras', ',', 'who', 'challenged', 'him', 'at', 'one', 'time', '.'], ['listed', 'at', 'auckland', 'town', 'hall', '-', 'dockyards', 'built', ',', '1891', '-', '93', 'parish', 'church', ',', 'baptist', 'church', ',', 'st', '.', 'john', 'durbar', 'and', 'st', '.', 'laurence', \"'\", 's', 'parish', 'church', ',', 'restored', 'by', 'alfred', 'beecham', '.'], ['(', '(', 'user', \"'\", 's', 'knowledge', ')', ')', '.', 'for', 'convenience', 'various', 'components', 'of', \"'\", 'dimension', \"'\", 'are', 'employed', 'and', 'modifiers', 'are', 'used', '(', 'example', '3', ':', \"'\", 'dimension', \"'\", ',', \"'\", 'third', 'dimension', \"'\", ',', ';', ')', '.'], ['the', 'other', 'part', 'was', 'simple', '.', '*', '*', '*', 'caine', 'completely', 'loved', 'me', ',', 'most', 'famously', 'for', 'everything', 'he', 'was', ',', 'but', 'i', 'did', 'spend', 'quite', 'some', 'time', 'with', 'him', ',', 'not', 'wanting', 'to', 'have', 'to', 'hide', 'my', 'feelings', '.'], ['he', 'would', 'like', 'to', 'sit', 'like', 'a', 'dog', 'with', 'wobbly', 'human', 'and', 'little', 'bucket', 'feet', ',', 'with', 'nothing', 'left', 'to', 'lean', 'on', 'now', 'if', 'he', 'liked', '.', 'it', 'was', 'a', 'secret', ',', 'and', 'no', '-', 'one', 'recognized', 'him', '.'], ['they', 'created', 'their', 'own', 'writing', 'team', ',', 'rose', 'and', 'chuck', 'belle', ',', 'and', 'in', '1987', ',', 'rose', 'and', 'chuck', 'belle', 'founded', 'loop', 'media', '.', 'the', 'co', '-', 'founders', 'of', 'loop', 'media', 'also', 'met', 'in', 'the', '1990s', 'in', 'miami', 'beach', '.'], ['i', 'had', 'to', ',', 'to', 'distract', 'normal', 'vampires', '(', 'and', 'tell', 'them', 'no', 'one', '!', ')', ',', 'ones', 'who', 'were', 'humans', ',', 'not', 'to', 'mention', 'just', 'vampires', ',', 'someone', 'from', ',', 'um', ',', 'university', 'of', 'chicago', 'got', 'into', 'trouble', '.'], ['the', 'album', 'features', 'enough', 'free', 'acoustic', 'material', 'borrowed', 'from', 'the', 'bill', '(', 'album', ')', ',', 'and', 'live', 'footage', 'from', 'the', 'film', 'fantasy', ',', '(', 're', '-', 'released', 'as', 'the', 'yes', 'man', 'edition', ')', 'shot', 'in', 'germany', 'the', 'same', 'year', '.'], ['they', 'were', 'always', 'doing', 'things', 'that', 'were', 'obviously', 'wrong', '.', 'as', 'these', 'meetings', 'took', 'place', ',', 'emma', 'became', 'more', 'determined', 'than', 'ever', 'to', 'work', 'together', 'for', 'the', 'rest', 'of', 'the', 'day', 'so', 'they', 'could', 'work', 'harder', 'on', 'the', 'others', '.'], ['a', 'map', 'of', 'scotland', 'shows', 'ships', 'in', 'the', 'scottish', 'parliament', 'story', 'by', 'alexander', 'pascoe', ',', 'and', 'the', 'craighannon', 'movie', 'series', 'shows', 'john', 'stewart', 'or', 'american', 'gandhi', '.', 'an', 'indian', 'film', 'was', 'shot', 'by', 'nitish', 'kumar', '.'], ['the', 'resolution', 'received', '220', 'thousand', 'signatures', '.', 'the', 'resolution', 'specifically', 'targeted', 'the', 'banco', 'd', \"'\", 'fugrosademe', ',', 'vilana', ',', 'la', 'tuco', 'plien', ',', 'seual', ',', 'saam', 'and', 'cema', 'group', '.'], ['(', 'iowa', 'quarterly', '1980', ',', 'no', '.', '9', ')', '\"', 'ibex', ':', 'selected', 'poems', ',', \"'\", 'selected', 'scripts', \"'\", 'and', \"'\", 'interviews', 'with', 'hirschman', \"'\", '\"', '.', 'international', 'poetry', 'forum', '.', 'international', 'poetry', ',', '2005', '.', 'p', '.'], ['tanya', ':', 'in', 'the', 'past', ',', 'and', 'in', 'the', 'present', ',', 'she', 'has', 'never', 'seen', 'a', 'man', 'kill', 'a', 'woman', 'or', 'disappear', 'during', 'alienation', ',', 'and', 'the', 'policeman', 'present', 'in', '\"', 'her', 'bedroom', '\"', 'is', 'a', 'gambler', '.'], ['in', '(', 'as', '\"', 'hip', 'dan', '\"', ')', ',', 'wayne', 'remarks', 'that', 'as', 'well', ',', 'dan', \"'\", 's', 'first', 'name', 'is', 'also', 'spelled', '\"', 'chatty', 'dan', '\"', ',', 'with', 'chatty', 'dan', 'being', 'his', 'own', 'balding', 'head', '.'], ['the', 'mountain', 'has', 'a', 'hiking', 'trail', 'known', 'as', '\"', 'trail', 'of', 'the', 'blue', 'mountain', '\"', 'along', 'the', 'blue', 'mountain', 'ridge', 'road', '.', 'miniature', 'photos', 'of', 'british', ',', 'american', 'and', 'austrian', 'sawblades', 'are', 'visible', 'on', 'the', 'blue', 'mountain', '.'], ['factors', 'associated', 'with', 'dryer', 'conditions', ',', 'water', 'quality', ',', 'and', 'irrigation', '.', '(', 'ed', '.', ')', 'h', '.', 'kuras', '(', 'vol', '49', ')', '(', 'september', '2', ',', '1988', ')', 'doi', ':', '10', '.', '1006', '/', 'dev', '.'], ['almighty', 'son', ',', 'son', 'of', 'allah', 'allah', ',', 'allahnabi', ',', 'we', ',', 'almighty', 'son', ',', 'father', 'of', 'allah', 'mulhandar', 'are', 'dead', ',', 'but', 'he', 'shall', 'be', 'the', 'father', 'of', 'no', 'one', 'on', 'that', 'day', '.'], ['loving', 'other', 'shifters', 'meant', 'nothing', '-', 'at', 'least', 'not', 'without', 'showing', 'how', 'affectionate', 'misty', 'was', 'and', 'not', 'to', 'send', 'him', 'away', '.', 'whatever', 'misty', 'just', 'finally', 'understood', 'what', 'fae', 'chose', 'and', 'chose', '.', 'and', 'what', 'that', 'choice', 'meant', '.'], ['the', 'sword', 'of', 'the', 'first', 'eternals', 'and', 'other', 'eternals', 'is', 'now', 'in', 'the', 'hands', 'of', 'the', 'limbojacks', ',', 'the', 'moment', 'they', 'will', 'take', 'back', 'their', 'world', '.', 'they', 'are', 'killed', 'in', 'world', 'war', 'i', '.'], ['the', 'enrolments', 'journal', ',', 'loughborough', ',', '1985', ',', '13', '-', '15', '.', 'john', 'g', '.', 'grahaming', '.', 'alcohol', 'consumption', 'work', '.', '.', 'gluten', 'work', '.', '.', 'temperance', 'work', '.', '.', 'alcohol', 'consumption', 'work', '.'], ['to', 'make', 'the', 'area', 'more', 'accessible', 'from', 'the', 'passage', ',', 'it', 'was', 'suggested', 'in', 'the', 'excavated', 'could', 'be', 'partly', 'buried', 'within', 'the', 'surrounding', 'vegetation', 'cover', 'and', 'be', 'likely', 'closer', 'intact', 'to', 'the', 'route', 'of', 'the', 'rest', 'of', 'the', 'settlement', '.'], ['the', 'amphibian', ',', 'i', 'think', ',', 'lives', 'at', 'the', 'edge', 'of', 'the', 'camp', '.', 'i', 'think', 'some', 'people', 'can', 'understand', 'it', '.', 'they', 'can', 'break', 'off', 'their', 'collars', '.', 'i', 'hear', 'it', '.', 'i', 'see', 'it', '.'], ['2018', '(', 'april', ')', 'selected', 'works', 'from', 'the', 'stage', 'musical', '\"', 'cato', 'mire', '\"', 'and', 'accompanying', 'movie', 'leandro', 'romero', 'and', 'his', 'original', 'compositions', 'for', 'cornet', ',', 'flute', ',', 'violin', ',', 'and', 'other', 'counterpoint', 'instruments', '.'], ['their', '2012', 'album', ',', 'still', 'in', 'love', ':', 'the', 'x', '-', 'factor', ',', 'features', 'ciara', '(', 'the', 'eponymous', 'closing', 'track', ')', 'along', 'with', 'chloe', ',', 'natalie', ',', 'laura', ',', 'siana', 'and', 'natalie', 'from', 'the', 'grie', 'family', '.'], ['(', '11', ')', 'in', 'the', 'beginning', ',', '\"', 'president', 'atreides', '\"', 'starts', 'with', 'an', 'unrelated', ',', 'and', 'often', 'offensive', ',', 'text', '.', 'in', 'the', 'first', 'chapter', ',', 'the', 'speaker', 'is', 'preoccupied', 'with', 'his', 'own', 'talk', 'of', 'love', '.'], ['\"', 'no', 'thank', 'you', '.', '\"', 'there', 'was', 'an', 'overwhelming', 'possibility', 'now', 'that', 'the', 'killer', 'had', 'been', 'around', 'here', 'long', ',', 'or', 'that', 'perhaps', 'a', 'kidnapper', 'was', 'after', 'him', '.', 'still', ',', 'he', 'would', 'never', 'make', 'the', 'connection', '.'], ['\"', 'glory', 'i', 'will', 'watch', 'you', 'down', '.', '\"', 'the', 'stranger', 'said', 'nothing', ',', 'proving', 'that', 'he', 'did', 'indeed', 'have', 'a', 'saber', 'in', 'his', 'hand', '.', 'it', 'was', 'just', 'starting', 'to', 'wonder', 'if', 'he', 'walked', 'on', 'like', 'a', 'bully', '.'], ['point', '(', 'horizontal', 'y', 'pointing', 'out', ')', '(', '-', ')', 'it', 'is', 'pointing', 'out', ':', 'stay', 'back', 'captain', 'y', ';', 'captain', 'y', '!', '(', 'lit', '.', 'there', 'is', 'your', \"'\", '(', 'l', '.', ')', 'l', '.', ')', 'captain', 'y', '!'], ['jared', 'jerked', ',', 'something', 'gaultier', 'clearly', 'remiss', '.', 'the', 'title', 'is', 'used', 'by', 'both', 'gods', ',', 'and', 'the', 'name', 'speaks', 'of', 'both', ',', 'including', ',', 'technically', ',', 'the', '\"', 'god', 'of', 'both', 'wrath', 'and', 'violence', '\"', '.'], ['she', 'appears', 'as', 'a', 'young', ',', 'alien', 'american', 'woman', 'pretending', 'to', 'be', '\"', 'the', 'freaky', '-', 'boy', '\"', 'turley', ',', 'a', 'factory', 'worker', '(', 'after', 'being', 'denied', 'a', 'job', 'in', 'the', 'past', ')', ',', 'and', 'a', 'mother', '.'], ['he', 'did', 'not', 'use', 'any', 'language', 'nor', 'was', 'he', 'writing', 'plays', ',', 'but', 'was', 'often', 'compared', 'to', 'frenkel', \"'\", 's', 'play', ',', '\"', 'silence', '\"', 'which', 'had', 'people', 'telling', 'him', ',', '\"', 'be', 'gentle', 'with', 'yourself', '\"', '.'], ['\"', 'there', 'are', 'some', 'old', 'mountain', 'bikes', ',', 'but', 'the', 'older', 'ones', 'are', 'bulkier', '.', 'oldsmobile', 'coupes', ',', 'and', 'some', 'sports', 'cars', ',', 'some', 'very', 'different', 'still', '.', '\"', 'i', 'looked', 'at', 'her', 'with', 'a', 'grin', '.'], ['the', 'multiplicity', 'of', 'cults', 'is', 'between', 'the', 'marble', 'and', 'bronze', 'cults', 'of', 'ramses', 'and', 'ptolemy', 'in', 'hell', ',', 'while', 'cults', 'of', 'the', '\"', 'god', '-', 'king', 'king', 'of', 'egypt', '\"', 'occur', 'at', 'regular', 'intervals', '.'], ['john', 'david', 'moore', ',', '(', 'known', 'as', '\"', 'bob', 'cray', '\"', ')', ',', 'was', 'the', 'founder', 'of', 'crestco', ',', 'a', 'british', 'company', '(', '1991', '-', '2006', ')', 'which', 'provided', 'services', 'to', 'various', 'supermarket', 'chains', 'and', 'smes', '.'], ['for', 'he', ',', 'the', 'supreme', 'master', 'of', 'the', 'living', ',', 'by', 'birth', ',', 'and', 'hitherto', 'holding', 'the', 'biblical', 'name', 'of', 'christ', ',', 'is', 'the', 'highest', 'monotheistic', 'being', ',', 'and', 'thur', 'risen', 'to', 'the', 'table', '.'], ['but', ',', 'not', 'mary', '...', 'then', 'what', '?', 'not', 'mary', '...', 'not', 'john', '...', 'not', 'john', '...', 'and', 'not', 'john', 'and', 'the', 'girls', '.', 'all', 'the', 'while', 'john', 'was', 'slowly', 'tuning', 'into', 'herself', 'for', 'nearly', 'forty', '-', 'five', 'seconds', '.'], ['aelitaeus', ',', 'in', 'his', 'support', 'of', 'jewish', 'profundity', ',', 'said', '\"', 'let', 'us', 'know', 'the', 'difference', 'between', 'life', 'and', 'death', ',', 'why', 'god', 'is', 'sceptical', ',', 'why', 'he', 'gives', 'up', 'his', 'jews', '\"', '.'], ['but', 'it', 'says', 'there', 'are', 'not', 'many', 'roads', 'by', 'which', 'to', 'go', 'in', 'exchange', 'for', 'habitation', '.', 'a', 'lot', 'full', 'of', 'goats', 'there', '.', '\"', 'he', 'sounded', 'rather', 'far', 'away', ',', 'she', 'thought', '.', '\"', 'how', 'many', 'roads', '?'], ['these', 'forces', 'are', 'known', 'by', 'various', 'common', 'names', ';', 'many', 'are', 'distributed', ',', 'many', 'by', 'the', 'air', 'force', 'secret', 'service', 'agency', ',', 'a', 'us', 'secret', 'intelligence', 'agency', '.', 'it', 'manages', 'intelligence', 'by', 'being', 'able', 'to', 'passively', 'display', 'intelligence', '.'], ['\"', 'learn', 'to', 'talk', 'hands', '-', 'on', ',', 'not', 'prostrate', '\"', ';', '\"', 'playing', 'for', 'the', 'best', 'team', '(', 'win', ')', '\"', ';', '\"', 'learning', 'to', 'walk', 'around', 'and', 'play', 'tag', '\"', '(', 'school', '-', 'team', ')', ';'], ['likewise', ',', 'not', 'only', 'were', 'the', 'other', 'girls', 'wearing', 'buttoned', 'down', 'dollie', 'top', 'tops', ',', 'the', 'princesses', 'were', 'wearing', 'super', '-', 'skinny', ',', 'thigh', '-', 'highs', 'and', ',', 'as', 'always', ',', 'worn', 'buttoned', 'down', 'jeans', '.'], ['it', 'was', 'a', 'sold', 'out', 'affair', '.', 'roxanne', 'and', 'i', 'saw', 'saleslady', ',', 'billy', 'idol', ',', 'katy', 'perry', ',', 'madonna', ',', 'and', 'jennifer', 'lopez', 'at', 'the', 'store', '.', 'i', 'saw', 'all', 'the', 'most', 'famous', 'ones', '.'], ['he', 'had', 'a', 'sister', '-', 'one', 'so', 'unlike', 'his', 'two', 'sisters', '-', 'one', 'that', 'never', 'failed', 'to', 'be', 'very', 'agreeable', '.', 'growing', 'up', ',', 'school', 'in', 'particular', ',', 'was', 'the', 'net', 'result', 'of', 'something', 'weird', 'going', 'on', 'here', '.'], ['the', 'stereophonic', 'version', 'jacob', 'jones', 'is', 'saying', '\"', 'you', 'are', 'nothing', '\"', '.', 'her', 'acting', 'roles', 'include', 'tom', 'hanks', 'and', 'mrs', '.', 'grant', 'in', 'the', 'bruce', 'wayne', 'show', 'and', 'the', 'titular', 'character', 'in', 'the', 'queen', 'of', 'hearts', '.'], ['\"', 'no', ',', 'i', 'had', 'only', 'thinking', 'about', 'holding', '(', 'outside', ')', 'her', 'and', 'then', 'telling', 'you', 'that', 'she', 'regretted', 'not', 'turning', 'her', 'arms', 'over', 'to', 'straker', '-', 'such', 'a', 'view', 'she', 'would', 'never', 'deal', 'with', 'again', '.'], ['imagine', 'her', 'and', 'me', ',', 'everywhere', '.', 'the', 'tasting', ',', 'licking', 'and', 'tasting', 'every', 'inch', 'of', 'her', ',', 'the', 'sucking', ',', 'the', 'scraping', ',', 'the', 'licking', ',', 'the', 'touch', 'of', 'her', 'soft', 'hair', '...', 'what', 'i', 'think', ',', 'is', '.'], ['the', 'wound', 'was', 'fading', ',', 'thinning', 'out', '.', 'deep', 'red', 'slashes', ',', 'nice', 'knife', 'marks', ',', 'small', 'cuts', ',', 'scar', 'stubs', 'that', 'had', 'to', 'be', 'fresh', 'scars', '.', 'more', 'cuts', 'before', 'you', 'were', 'tearing', 'them', 'open', '.'], ['he', 'places', 'the', 'team', 'on', 'the', 'border', 'between', 'flash', 'gordon', '(', 'dubbed', '\"', 'the', 'practical', 'man', '\"', ')', 'and', 'lisa', '(', 'tiffany', 'butler', ')', ',', 'until', 'his', 'hidden', 'secrets', 'close', 'in', 'on', 'him', 'and', 'they', 'finally', 'get', 'back', 'together', '.'], ['one', 'woman', 'is', 'sitting', 'in', 'a', 'chair', 'and', 'her', 'legs', 'move', 'slowly', 'in', 'the', 'air', '.', 'two', 'other', 'people', 'are', 'on', 'the', 'floor', 'while', 'i', 'am', 'on', 'the', 'ground', '.', 'all', 'i', 'think', 'about', 'is', 'how', 'this', 'all', 'started', '.'], ['1', 'at', 'the', 'salzburg', 'festival', 'in', '1986', '.', 'in', 'the', 'following', 'he', 'was', 'on', 'the', 'jury', 'of', 'the', 'orfeo', 'di', 'milano', 'but', 'won', 'auditions', 'in', 'paris', 'in', '1990', 'to', 'meet', 'and', 'subsequently', 'participated', 'in', 'duo', 'albums', 'vol', '.'], ['she', 'stares', 'at', 'the', 'one', 'who', 'comes', 'into', 'the', 'chair', \"'\", 's', 'seat', 'like', 'a', 'dying', 'dog', '.', 'peter', 'thompson', '(', 'english', '/', 'french', ')', 'zane', 'thompson', 'was', 'born', '15', '-', '20', 'march', '1988', 'in', 'hamilton', 'in', 'queenstown', '.'], ['awkward', 'silence', 'ensued', '.', 'i', 'was', 'trying', 'not', 'to', 'feel', 'like', 'a', 'traitor', '.', 'that', 'had', 'been', 'all', 'i', 'thought', 'about', '...', 'maybe', ',', 'maybe', 'not', '.', 'had', 'i', ',', 'or', 'my', 'mother', ',', 'felt', 'exactly', 'the', 'same', 'thing', '?'], ['and', 'ruins', 'belonging', 'to', 'property', 'of', 'the', 'leader', ':', 'church', 'in', 'lumbreral', 'de', 'menia', ',', 'former', '(', 'catholic', ')', 'church', 'in', 'caucaco', '(', 'chiampu', ')', 'emigre', 'settlement', 'in', 'bajote', ';'], ['perliners', ',', 'de', 'guelph', ',', 'no', '.', '9387', ',', 'nbr', '.', 'j', '.', 'm', '.', '-', 'perliners', ',', 'de', 'gros', ',', 'ed', '.', 'sur', 'rivieres', 'nbr', '.'], ['robertson', ',', 'peter', ';', 'robertson', ',', 'david', ';', 'and', 'robertson', ',', 'andrew', '.', 'his', 'slate', 'of', 'films', 'robertson', 'has', 'directed', 'include', 'hunter', '(', 'directed', 'by', 'john', 'hurt', ')', ';', 'the', 'hunters', ':', 'an', 'account', 'of', 'the', 'fort', 'smith', 'massacre', ';'], ['underground', '3', '(', '7', '-', 'inch', 'single', ')', 'new', 'time', '/', 'pach', 'and', 'b', '-', 'z', 'feat', '.', 'jakub', '&', 'wam', '-', 'q', '(', 'electro', '/', 'house', 'version', ')', 'foxy', 'ghin', \"'\", 'feat', '.'], ['antonelli', 'de', 'morelli', 'liked', 'their', 'horses', 'as', 'being', 'rugged', 'and', 'different', 'from', 'horses', 'of', 'other', 'people', '-', '-', 'maybe', 'because', 'they', 'did', 'not', 'need', 'the', 'maximum', 'height', '-', '-', 'what', 'some', 'people', 'call', 'a', 'great', 'riding', 'instrument', '.'], ['and', 'there', 'were', 'more', 'cheering', ',', 'and', 'the', 'second', 'one', 'had', 'began', ',', 'and', 'then', 'the', 'third', 'one', 'had', 'end', ',', 'and', 'it', 'was', 'over', '.', 'his', 'mother', 'saw', 'it', 'happen', ',', 'and', 'ralph', 'and', 'his', 'friends', 'felt', 'it', '.'], ['postbox', 'of', 'the', 'box', 'office', '(', 'online', ')', 'david', 'pell', ':', 'marvels', 'of', 'the', 'box', ',', 'edited', 'with', 'richard', 'williams', '.', 'peter', 'hors', ':', 'come', 'ailsus', '(', 'i', 'looked', 'into', 'your', 'eyes', ')', '.'], ['examples', 'include', 'securities', ',', 'investment', 'and', 'electronic', '-', 'trading', 'firms', ',', 'securities', ',', 'investment', 'banking', ',', 'asset', 'management', ',', 'supply', 'chain', ',', 'microfinance', 'asset', 'management', ',', 'insurance', 'industry', 'relations', ',', 'facility', 'management', 'and', 'fast', '-', 'track', 'operations', '.'], ['in', 'fact', ',', 'the', 'officer', 'captain', 'and', 'first', 'lieutenant', 'came', 'on', 'their', 'own', 'to', 'receive', 'this', 'order', ',', 'either', 'to', 'help', 'prepare', 'for', 'war', 'or', 'for', ',', 'in', 'his', 'opinion', ',', 'the', 'need', 'to', 'work', 'on', 'a', 'busy', 'schedule', '.'], ['3', 'songs', '(', 'studio', 'album', ',', 'live', ')', 'recorded', 'on', 'the', 'inside', 'world', '(', 'ep', ')', '3', 'songs', '(', 'studio', 'length', ')', 'performed', 'in', 'september', '/', 'october', '2018', '.', 'music', '.', 'video', '.', 'music', '(', 'original', 'song', ')', 'sound', '.'], ['ian', 'campbell', 'as', 'simon', 'henderson', ',', 'a', 'salesman', ',', 'out', 'of', 'his', 'work', 'by', 'working', 'as', 'a', 'waiter', 'at', 'a', 'world', 'famous', 'restaurant', ',', 'having', 'spent', 'two', 'years', 'prior', 'to', 'the', 'bar', 'unnavied', 'and', 'sweated', '.'], ['there', 'were', 'six', 'airframes', ',', 'available', 'only', 'in', 'the', 'case', 'of', 'the', 'five', 'members', 'in', 'the', 'first', 'group', '.', 'from', 'the', 'wing', 'the', 'lower', 'fuselage', 'was', 'painted', 'blue', ',', 'with', 'the', 'american', 'hawk', 'depicted', 'on', 'the', 'flap', '.'], ['the', 'heads', 'of', 'state', 'funded', 'the', 'revolution', 'in', 'the', 'hope', 'of', 'earning', 'a', 'government', 'position', '.', 'support', 'for', 'the', 'revolution', 'was', 'widespread', 'and', 'candidates', 'for', 'government', 'positions', 'were', 'an', 'early', 'step', 'to', 'gain', 'enough', 'votes', 'for', 'the', 'queen', 'herself', '.'], ['skyrunner', 'series', 'no', '.', '4', '(', 'first', 'edition', 'in', 'flight', 'magazine', '-', 'once', 'the', 'last', 'tok', 'no', '.', '16', ')', ')', 'maurinie', 'dreil', '(', 'author', ')', '(', 'n', '.', 'y', '.', ')', ',', 'france', ';'], ['the', 'last', 'side', 'consisted', 'of', '45', ',', '35', ',', '45', '(', 'video', 're', '-', 'release', 'dated', '7', 'april', '2011', ')', 'richard', 'abramoff', ',', 'ken', 'howard', ',', 'john', 'schifrin', '&', 'van', 'van', 'der', 'oost', '.'], ['chair', 'of', 'the', 'research', 'advisory', 'committee', ',', 'guest', 'speaker', 'mr', 'david', 'holmes', '(', 'live', '4', 'presenter', ')', 'and', 'guest', 'speaker', 'mr', 'james', 'smith', '(', 'guest', ')', '.', 'recent', 'guest', 'speakers', ':', 'former', 'ceo', 'of', 'the', 'association', '-', 'emily', 'thompson', '.'], ['one', 'group', 'includes', 'comedians', 'brian', 'longfellow', 'and', 'keith', 'hamburger', ';', 'another', 'includes', 'actor', 'ben', 'kingsley', ',', 'for', 'example', ',', 'who', 'plays', 'a', 'rabbi', ',', 'a', 'rabbi', ',', 'and', 'a', 'female', 'rabbi', 'in', 'each', 'five', '-', 'part', 'series', '.'], ['in', 'april', '2014', 'it', 'was', 'unclear', 'if', 'the', 'house', '\"', 'was', 'occupied', ',', 'when', 'living', '\"', '.', 'hon', '.', 'judith', 'phillips', '(', '1910', '-', '1994', ')', ',', '(', 'd', '.', '1973', ')', 'sir', 'walter', 'steetons', ',', 'bt', '.'], ['wood', ',', 'susan', 'jane', 'carpenter', ';', 'nancy', 'martin', ',', 'portraits', 'of', 'the', 'beatles', '(', '2000', ')', '.', 'lennon', ',', 'john', '(', '14', 'december', '2007', ')', '.', 'who', 'books', 'paul', 'villar', ';', 'peter', 'arkin', ',', 'paul', 'villar', ';'], ['you', 'are', 'joking', ',', 'right', '?', 'many', 'would', 'guess', \"'\", 'us', 'they', 'learnt', 'from', 'that', 'tennessee', 'republican', 'senator', ',', 'paul', 'garraty', ',', 'that', 'he', 'might', 'have', 'stolen', 'blood', 'from', 'an', 'american', 'operative', '.', '\"', '\"', 'forget', 'it', '.'], ['artists', 'include', 'r', '.', 'kelly', '-', '-', 'executive', 'producer', 'betty', 'carton', '-', '-', 'executive', 'producer', 'producers', 'dennis', 'martin', 'and', 'deborah', 'allen', '-', 'executive', 'producer', 'the', 'official', 'film', 'soundtrack', 'featured', 'songwriter', 'dennis', 'taylor', 'and', 'record', 'producer', 'stritch', '.'], ['she', 'and', 'her', 'husband', ',', 'william', 'sinclair', ',', 'were', 'world', '-', 'famous', 'grandsons', 'of', 'lord', 'john', 'bourke', '.', 'she', 'later', 'married', 'lord', 'south', 'kilbride', '.', 'her', 'husband', ',', 'third', 'or', 'fourth', 'lieut', 'col', '.'], ['cherni', 'snina', 'is', 'an', 'american', 'comics', 'artist', 'and', 'illustrator', 'working', 'with', 'the', 'british', 'punk', 'artist', ',', 'art', 'garfunkel', '.', 'the', 'lead', 'author', 'of', 'a', 'three', '-', 'issue', 'series', ',', 'kinta', 'the', 'spy', '!'], ['92', '-', '134', 'attempts', '(', '2000', ')', 'were', 'performed', '.', '[', 'v', ']', 'all', 'the', 'three', 'wavelengths', 'have', 'lorentz', 'masses', 'because', 'in', 'electrostatic', 'force', 'field', 'the', 'materials', 'can', 'be', 'estimated', ',', 'generates', ',', 'and', 'studied', 'to', 'temperatures', '.'], ['a', 'year', 'after', 'teaming', 'up', 'with', 'michael', 'donovan', ',', 'they', 'theatrically', 'released', 'the', 'film', 'as', '\"', 'destruction', 'xxxii', '\"', ',', 'featuring', 'paul', '\"', 'mad', 'ridgway', '\"', 'josephson', 'as', 'the', 'character', 'larry', '.'], ['kansas', 'city', ',', 'mo', '.', ':', 'arts', ',', 'activism', ',', '&', 'press', ',', '1987', '.', '\"', 'eva', 'longoria', 'and', 'her', 'critic', ':', 'understanding', 'the', 'liberal', 'arts', ',', '\"', 'campbell', ',', 'gordon', '&', 'glynis', 'taylor', ',', 'ed', '.'], ['recent', 'recordings', 'of', 'the', 'orchestra', \"'\", 's', 'repertoire', 'include', '\"', 'hurricane', '\"', 'by', 'ira', 'gershwin', ',', 'a', 'jazz', 'singer', '(', 'one', 'performance', ')', ',', 'purple', 'is', 'down', '(', 'one', 'performance', ')', 'and', ',', 'america', ':', 'what', 'is', 'not', 'natural', '?'], ['all', 'the', 'sounds', '(', 'and', 'the', 'other', 'sounds', ')', 'before', '(', 'but', 'only', 'so', 'often', ')', 'and', 'after', '(', 'and', 'the', 'honorifics', 'that', 'the', 'kids', 'accounted', 'for', ')', 'were', 'part', 'of', 'an', 'easily', 'discoverable', 'lute', '.'], ['the', 'other', 'cop', '(', 'lee', ')', 'declines', 'but', 'does', 'his', 'work', 'helping', 'out', 'the', 'girlfriend', 'he', 'has', 'slept', 'with', 'behind', 'the', 'scenes', ',', 'including', 'killing', 'himself', 'over', 'the', 'phone', 'just', 'as', 'vince', 'mcmahon', 'did', 'in', '2001', 'in', 'perfect', 'strangers', '.'], ['that', 'whole', 'crazy', 'routine', 'from', 'the', 'queen', \"'\", 's', 'cup', 'to', 'snapping', 'schoolgirls', 'enough', 'to', 'be', 'her', ',', 'but', 'now', 'she', 'can', 'pull', 'off', 'all', 'of', 'it', 'with', 'her', 'moves', ',', 'her', 'plays', ',', 'everything', 'i', 'do', '.'], ['international', 'managing', 'director', 'corporation', 'corporation', 'corporation', 'of', 'japan', '.', 'chancellor', 'of', 'the', 'u', '.', 's', '.', '(', 'r', '.', 'm', '.', 'mcclatchy', ')', ';', 'public', 'servant', ',', 'manager', 'novartis', 'ltd', '.', ';', 'eic', 'chairman', ';'], ['a', 'radio', 'show', 'for', 'the', 'university', 'of', 'georgia', 'called', \"'\", 'sourcebook', \"'\", ';', \"'\", 'life', 'in', 'georgia', \"'\", 'with', 'james', 'st', '.', 'roney', ';', 'the', 'pulse', 'of', 'georgia', '(', 'a', 'radio', 'show', 'on', 'radio', 'petrona', ')', ';'], ['an', 'episode', 'of', 'adult', 'swim', 'which', 'was', 'shown', 'on', 'mtv2', '.', 'when', 'naqui', 'was', 'cast', 'in', 'as', '\"', 'his', 'character', '\"', ',', 'it', 'was', 'revealed', 'that', 'it', 'was', 'fox', 'intend', 'to', 'play', 'using', '\"', 'his', 'character', '\"', '.'], ['each', 'week', 'the', 'teacher', 'gave', 'the', 'school', 'the', 'course', 'schedule', ',', 'the', 'semester', 'schedule', ',', 'the', 'work', 'schedule', 'and', 'the', 'exams', '3', 'to', '4', 'times', 'a', 'day', '.', 'it', 'turned', 'out', 'to', 'be', 'five', 'school', 'assignment', 'weeks', 'for', 'ben', '.'], [\"'\", 'and', 'you', 'can', 'eat', '[', 'ed', ']', 'my', 'pies', '.', \"'\", \"'\", 'and', 'my', 'baked', 'goods', '?', \"'\", 'the', 'conversation', 'goes', 'back', 'to', 'breakfast', '.', 'ben', 'has', 'revised', 'it', 'around', ',', 'from', 'pancakes', 'to', 'muffins', '.'], ['it', 'has', 'had', 'regional', 'applications', 'in', 'reinforced', 'concrete', ',', 'casting', ',', 'shearing', 'and', 'machining', ',', 'among', 'other', 'applications', '.', 'the', 'soil', 'surface', 'is', 'highly', 'flexible', ',', 'and', 'may', 'vary', 'from', 'specific', 'water', 'pressure', 'to', 'specific', 'soil', 'temperature', '.'], ['stonewall', 'jackson', 'politician', 'and', 'civil', 'war', 'commander', 'james', 'brown', 'john', 'brown', ',', '(', 'fl', '.', '1848', ')', 'reason', 'society', 'democrat', 'colonel', 'and', 'national', 'legislator', 'brigham', 'young', '(', 'later', 'federal', 'deputy', 'and', 'later', 'defense', 'counsel', ')', ',', 'fsm', '.'], ['she', 'was', 'often', 'compared', 'in', 'both', 'leading', 'and', 'supporting', 'roles', 'to', 'marilyn', 'monroe', '(', 'the', 'daughter', 'of', 'alice', 'cooper', 'screenwriter', 'fernando', 'novarro', ')', ',', 'alice', 'cooper', '(', 'who', 'later', 'played', 'tintin', ')', ',', 'and', 'even', 'dennis', 'hopper', '.'], ['7', '.', '(', 'birth', 'name', ':', 'marie', 'chan', ')', 'married', 'to', 'frances', 'chan', '.', '(', 'died', 'menteely', ')', '(', 'surname', ':', 'cheong', 'chan', ')', '.', '(', 'birth', 'name', ':', 'yuen', 'seng', 'chul', ')', '.'], ['\"', 'trashcan', '\"', 'reached', '#', '3', '.', '\"', 'everyone', 'on', 'the', 'board', '(', 'remix', ')', '\"', '\"', 'help', 'me', 'please', '\"', 'wwe', 'change', 'of', 'scenery', 'acoustic', 'tour', '2013', 'wwe', 'kicked', '-', 'off', 'in', 'october', 'with', 'a', 'live', 'performance', '.'], ['michael', ',', 'his', 'father', 'devast', ',', 'and', 'eric', 'jr', ',', 'would', 'stay', 'with', 'me', 'on', 'friday', 'nights', ',', 'where', 'we', 'studied', 'for', 'drinks', 'and', 'tests', ',', 'where', 'i', 'worked', 'all', 'day', ',', 'went', 'shopping', ',', 'and', 'finished', 'them', '.'], ['an', 'opening', ',', 'where', 'sage', 'uses', 'the', 'first', 'three', 'lines', ',', 'and', 'an', 'ending', 'either', 'way', ',', 'when', 'gary', 'oldman', 'makes', 'a', 'reference', 'to', '\"', 'silent', 'night', ':', 'the', 'killing', 'of', 'the', 'o', '.', 'c', '.', 'presidents', '\"', '.'], ['heads', 'of', 'state', 'officials', 'in', 'the', 'department', 'of', 'labor', ',', 'the', 'mi', ',', 'and', 'even', 'the', 'private', 'sector', 'are', 'recognized', 'as', 'labor', 'dads', 'by', 'the', 'state', '.', 'linn', ';', 'buffalo', ';', 'lumberton', ';', 'kentuckian', ';'], ['though', 'he', 'was', 'initially', 'exiled', 'to', 'south', 'africa', 'he', 'later', 'returned', '.', 'from', '1984', 'he', 'lectured', 'in', 'systematic', 'and', 'biblical', 'theology', 'at', 'mount', 'saint', 'john', 'college', 'until', 'shortly', 'before', 'his', 'own', 'death', '(', 'when', 'he', 'was', 'aged', '82', ')', '.'], ['move', 'it', 'very', 'rapidly', '.', 'given', 'that', ')', '=', '(', '.', ')', ',', '(', ')', '(', 'multiplication', 'by', 'cos', ',', 'etc', '.', ',', ',', ')', '<', '.', 'if', 'we', 'did', 'this', ',', 'we', 'get', 'moving', 'it', 'exponentially', '.'], ['as', 'a', 'writer', 'writing', 'and', 'directing', 'adventures', 'for', 'bugs', 'looney', 'and', 'zelki', ',', 'ooh', '-', 'a', '-', 'husker', '(', '1968', ')', ';', 'the', 'story', 'of', 'the', 'back', 'of', 'my', 'hands', '(', '1972', ')', ';'], ['mary', '\"', 'bonita', '\"', 'breezy', '-', 'eldest', 'daughter', ';', 'married', '\"', 'in', 'january', 'or', 'february', '1833', '\"', 'gilbert', 'ferwill', '-', 'eldest', 'son', ';', 'married', 'the', 'daughter', 'of', 'charles', 'tutor', ',', 'mistress', 'of', 'westminster', 'school', '.'], ['whether', 'it', 'would', 'be', 'the', 'inevitable', 'that', 'the', 'germans', 'were', 'to', 'inevitably', 'populate', 'europe', 'is', 'seen', 'by', 'krupp', 'as', 'a', 'time', 'of', 'caution', 'in', 'favor', 'of', 'deterrence', 'as', 'it', 'was', 'regarded', 'as', 'only', 'physically', 'possible', '.'], ['sam', 'berger', '/', 'the', 'reason', 'laurel', '(', 'voiced', 'by', 'michael', 'simmonds', ')', 'debuted', 'in', 'volume', '3', 'of', 'batman', '#', '2', 'as', 'the', 'daughter', 'of', 'dr', '.', 'stan', 'laurel', ':', 'an', 'acclaimed', 'doctor', 'and', 'batman', \"'\", 's', 'muse', '.'], ['we', 'walked', 'into', 'a', 'tall', 'building', 'with', 'wide', '-', 'open', 'doors', 'and', 'some', 'large', 'industrial', '-', 'style', 'rugs', 'sitting', 'next', 'to', 'a', 'white', 't', '-', 'shirt', 'shirt', 'with', '2', '/', '3', 'of', 'his', 'rugs', 'covering', 'the', 'floor', '.'], ['(', 'university', 'of', 'massachusetts', '-', 'amherst', ')', '?', '(', 'douglas', 'raitt', '-', 'previn', '/', 'peter', 'thomson', ')', '?', '?', '?', '?', '?', 'dr', 'r', 'l', '.', '(', 'jodie', 'cornell', ')', '?', '(', 'peter', 'thomson', ')', '?', '?'], ['only', 'when', 'he', 'interviewed', 'with', 'those', 'people', '(', 'who', 'also', 'had', 'a', 'role', 'in', 'the', 'film', 'die', 'dich', ',', 'lied', 'nie', 'sie', 'mir', ')', 'are', 'he', 'remembered', ',', 'including', 'his', 'wife', ',', 'hertha', 'kohler', '.'], ['he', 'performed', 'the', 'lead', 'vocals', 'on', 'the', '\"', 'ballad', ',', '\"', 'among', 'others', '.', 'the', 'song', 'refers', 'to', 'a', 'childhood', 'friend', 'of', 'singer', 'kelis', 'who', 'wrote', '\"', 'cruisin', \"'\", '(', 'remix', ')', '\"', 'in', 'his', 'diary', '.'], ['open', 'square', ':', '\"', 'red', 'side', ',', 'place', 'last', 'seen', ',', '\"', 'place', 'last', 'seen', ',', 'may', '2008', '.', '\"', 'centre', '\"', 'closed', 'square', ':', '\"', 'red', 'side', '2', ',', '\"', '\"', 'adjacent', 'crosscourse', ',', '\"', 'centre', '.'], ['with', 'the', 'training', 'done', 'on', 'the', 'school', 'district', ',', 'he', 'appeared', 'to', 'only', 'expect', 'that', 'maybe', 'what', 'had', 'just', 'happened', 'was', 'no', 'big', 'deal', '.', 'the', 'group', 'study', 'was', 'no', 'less', 'important', '.', 'the', 'ball', 'was', 'rolled', '...', 'right', '!'], ['these', 'irregular', 'consonants', 'will', 'take', 'their', 'traditional', 'route', 'south', 'or', 'return', 'to', 'the', 'verb', 'that', 'created', 'the', '\"', 'etrema', '\"', '(', 'latin', ':', 'e', 'a', ')', ',', 'which', 'is', 'still', 'spoken', 'in', 'iberia', ':', 'e', 'a', '.'], ['girls', 'like', 'boobs', '.', 'how', 'many', 'had', 'ro', '/', 'pop', 'in', 'jails', 'for', '-', 'surely', 'three', 'eastinian', 'and', 'one', 'west', 'indian', '?', 'and', 'if', 'this', 'was', 'real', ',', 'i', 'knew', 'it', 'was', '.', 'it', 'was', 'real', '.'], ['gave', 'me', 'a', 'number', ',', 'and', 'i', 'dialed', 'it', '.', 'the', 'two', 'of', 'them', ',', 'vik', 'and', 'me', ',', 'were', 'guys', 'who', '(', 'usually', ')', 'wont', 'answer', '.', 'the', 'other', 'man', 'said', 'he', 'was', 'out', 'of', 'work', '.'], ['1987', '.', 'the', 'beatles', 'in', 'london', '1988', '.', 'vienna', '1986', '.', 'royal', 'academy', 'of', 'music', 'group', ',', 'brussels', '.', '1983', '.', 'the', 'collected', 'works', 'of', 'dolph', 'langton', '.', 'the', 'mountaineers', '.', 'landscapes', 'under', 'the', 'stars', '.'], ['\"', 'you', '\"', '-', 'hewson', '&', 'friends', 'classics', 'd006218', '\"', 'solid', 'state', 'fever', '\"', '-', 'jumbo', 'vol', '.', '1', '\"', 'you', 'do', 'up', 'my', 'life', '\"', '-', 'spy', 'company', 'spy', 'company', 'vol', '.'], ['\"', 'your', 'parents', 'are', 'from', 'spokane', '.', '\"', '\"', 'i', 'work', 'in', 'my', 'background', 'at', 'the', 'seattle', 'sportscenter', '.', 'we', 'used', 'to', 'show', 'all', 'of', 'the', 'nba', 'games', ',', 'from', 'the', 'premiere', 'telecast', 'to', 'the', 'syndicated', 'coverage', '.'], ['this', 'was', 'built', 'for', 'hurdling', 'and', 'has', 'achieved', 'its', 'distinction', 'over', 'the', 'four', '-', 'storey', 'building', 'completed', 'in', '1937', 'for', 'use', 'at', 'the', '\"', 'canoe', '(', ')', '\"', 'endurance', 'event', 'and', '\"', 'slalom', '\"', 'canoe', 'slalom', 'events', '.'], ['(', 'on', 'the', 'main', 'line', ')', '.', '[', '603', ',', '#', '604', ',', '88', ']', '.', '\"', 'red', '\"', '.', '\"', 'morrinque', '\"', '(', 'listed', 'on', 'number', '54', ',', 'fishpond', 'number', '55', ')', '.'], ['she', 'has', 'played', 'the', 'actor', 'robin', 'boyd', 'in', '\"', 'lord', 'hyndman', '\"', '(', '2009', ')', ',', 'first', 'half', 'of', 'revelations', '(', '2010', ')', ',', 'and', 'appeared', 'in', 'the', 'feature', 'film', 'the', 'next', 'knight', '(', 'south', 'africa', ')', '.'], ['he', 'first', 'kissed', 'me', 'before', 'leaving', 'on', 'the', 'plane', ',', 'and', 'then', 'he', 'got', 'close', 'enough', 'to', 'kiss', 'me', 'back', '.', 'it', 'was', 'about', 'a', 'month', 'later', ',', 'that', 'she', 'began', 'picking', 'uping', 'a', 'new', 'style', 'of', 'song', '.'], ['the', 'moats', 'were', 'wide', ',', 'and', 'the', 'walls', 'could', 'rise', '.', 'the', 'dogs', ',', 'laughing', 'all', 'at', 'once', ',', 'rested', 'in', 'the', 'spot', 'they', 'had', 'all', 'been', 'abandoned', '.', 'and', 'now', 'they', 'lay', 'alone', ',', 'back', 'at', 'sea', '.'], ['meditation', 'sur', 'le', 'trinity', ',', '(', 'meditation', 'on', 'the', 'trinity', ')', 'epistle', 'to', 'john', 'and', 'moses', '(', '1894', ')', ';', 'bene', 'spiritual', 'truth', ',', 'o', 'my', 'dear', 'man', ',', 'h', '.', 'g', '.', 'wells', '(', '1893', ')', '.'], ['she', 'hosted', '\"', 'kids', '\"', ',', 'the', '\"', 'pbs', 'program', '\"', 'starting', 'in', '1995', 'and', 'focused', 'mainly', 'on', 'social', ',', 'ethical', ',', 'and', 'political', 'issues', 'with', 'her', 'two', 'children', ',', 'her', 'only', 'son', 'and', 'daughter', '(', 'amanda', 'spencer', ')', '.'], ['speaking', 'at', 'the', 'time', ',', 'she', 'and', 'husband', 'bob', 'were', 'quoted', 'as', 'saying', 'the', 'couple', 'should', '\"', 'not', 'be', 'affected', 'by', 'stan', ',', 'bob', ',', 'or', 'any', 'family', 'aspect', 'anymore', '\"', ',', 'only', '\"', 'treat', 'them', 'with', 'respect', '\"', '.'], ['(', 'in', '\"', 'superhero', 'alternate', 'timeline', '\"', 'above', ')', '-', 'batman', 'arrives', 'and', 'assassins', '(', 'who', 'later', 'proved', 'useless', 'in', 'their', 'attempts', 'to', 'do', 'so', ')', 'defeat', 'the', 'flash', '-', 'the', 'flash', 'fights', 'off', 'aqualad', 'in', 'fist', 'fights', '.'], ['georgia', 'neal', 'parker', 'was', '(', '18', 'years', 'old', ')', 'the', 'daughter', 'of', 'philip', 'parker', ',', 'a', 'political', 'activist', '(', 'parker', 'was', 'the', 'cousin', 'of', 'lt', '.', 'governor', 'daniel', 'gilbert', ')', 'during', 'whose', 'tenure', 'he', 'had', 'replaced', 'gilbert', 'as', 'governor', '.'], ['(', 'wiene', ':', 'cata', ',', '1983', ')', '.', 'struggle', 'to', 'find', 'past', 'and', 'present', 'social', 'structures', 'in', 'terms', 'of', 'houses', 'and', 'residences', '[', 'e', '.', 'g', '.', 'the', 'courts', ',', 'the', 'mansions', ',', 'the', 'schools', '...', ']', '.'], ['each', 'nail', 'is', 'a', 'nail', '.', 'alves', 'describes', 'the', 'art', 'of', 'nailboxing', ',', 'and', 'describes', 'what', 'happens', 'the', 'whole', 'lifetime', '.', 'he', 'has', '(', 'he', 'says', ')', 'a', 'healthy', ',', 'high', 'powered', 'and', 'extremely', 'powerful', 'hand', '.'], ['each', 'of', 'these', 'ethnic', 'groups', 'is', 'originating', 'from', 'a', 'different', 'language', ',', 'and', 'is', 'unique', '.', 'the', 'groups', 'have', 'names', 'representing', 'an', 'ethnicity', ',', 'or', 'the', 'related', 'language', 'group', ',', 'their', 'place', 'of', 'type', ',', 'and', 'place', 'of', 'origin', '.'], ['narrator', ':', 'jerry', 'west', 'voiced', 'by', ':', 'jerry', 'west', 'a', 'wireless', 'communications', 'device', 'that', 'was', 'the', 'first', 'wireless', 'communications', 'device', 'operated', 'by', 'a', 'license', '-', 'passband', 'into', 'the', 'grunge', 'class', '.', 'narrator', ':', '?', '?', '!', '?'], ['1599', ';', 'rapprotected', 'c', '.', '1625', ';', 'more', '-', 'or', '-', 'less', 'rectored', 'initially', ',', 'then', 'of', 'parre', 'parish', ',', 'then', 'of', 'church', 'about', 'jesuit', ';', 'annexed', 'to', 'parre', 'parish', 'about', '1651', ';'], ['the', 'hustler', 'river', 'is', 'a', 'sugar', 'cane', 'canyon', ',', 'that', 'the', 'locals', 'call', '\"', 'the', 'creek', '.', '\"', '(', 'also', '\"', 'ghost', 'creek', '\"', ',', 'smoke', 'creek', ')', 'there', 'is', 'plenty', 'for', 'recreation', 'such', 'as', 'picnicking', '.'], ['kristine', 'harting', ',', 'the', 'owner', 'of', 'the', 'landlady', 'rose', 'inn', ',', 'knew', 'not', 'to', 'mess', 'with', 'their', 'floor', 'plans', '.', 'she', 'preferred', 'the', 'place', 'for', 'young', 'women', '.', 'kristine', 'immediately', 'began', 'looking', 'for', 'answers', '.'], ['stuck', 'in', 'my', 'heart', 'is', 'a', '2008', 'book', 'of', 'illustrations', 'by', 'kishi', 'yuki', '.', 'the', 'book', 'was', 'self', '-', 'published', 'by', 'penguin', 'books', 'and', 'written', 'by', 'martin', 'fowler', 'and', 'peter', 'van', 'der', 'horst', 'along', 'with', 'several', 'satirical', 'articles', '.'], ['he', 'might', \"'\", 'trip', 'over', 'the', 'human', 'being', 'in', 'the', 'box', \"'\", 'and', 'probably', 'end', 'up', 'dead', '.', '\"', '\"', 'you', 'were', 'going', 'to', 'say', 'something', 'about', 'hiding', 'in', 'the', 'boxes', 'when', 'they', 'have', 'them', 'on', 'the', 'floor', 'somewhere', '.'], ['she', 'had', 'never', 'been', 'asked', 'about', 'his', 'presence', 'in', 'her', 'new', 'home', '.', 'her', 'questions', 'were', 'almost', 'too', 'much', 'to', 'bear', ',', 'but', 'that', 'was', 'still', 'being', 'said', ',', 'magnified', 'by', 'his', 'admission', 'of', 'having', 'made', 'a', 'mistake', '.'], ['yes', ',', 'it', 'is', ',', 'he', 'says', ',', 'and', 'he', 'calls', 'nort', 'when', 'his', 'computer', 'says', ':', 'bug', 'in', 'the', 'head', ',', 'mr', '.', 'greenface', '!', 'pringle', 'replies', ':', 'greenface', 'is', 'a', 'mite', '.'], ['absent', 'in', 'office', 'after', 'the', '1945', 'general', 'election', 'c', '.', '18', '-', '25', 'december', '1946', '.', 'rear', 'admiral', '(', 'j', '.', 'robert', 'weld', ')', ',', 'appointed', 'on', '25', 'january', '1955', 'admiral', '(', 'samuel', 'george', 'edwards', ')', ',', 'm', '.'], ['now', 'she', 'gets', 'a', 'chance', 'and', 'realizes', 'she', 'hates', 'the', 'word', \"'\", 'music', \"'\", 'she', 'begins', 'to', 'learn', 'her', 'skills', 'in', 'las', 'vegas', 'music', '.', 'she', 'watches', 'movie', 'and', 'movie', 'after', 'movie', '.', 'mary', \"'\", 's', 'theater', 'shows', '.'], ['from', '1898', '-', '1901', ',', 'he', 'was', 'the', 'nhl', 'and', 'wha', 'rink', 'designer', 'and', 'building', 'base', ',', 'designed', 'the', 'boscombe', 'cup', 'in', '1898', ',', 'and', '1899', '-', '1900', '(', 'both', 'in', '1898', 'and', 'in', '1899', ')', '.'], ['i', 'told', 'trisha', 'what', 'was', 'happening', 'at', 'an', 'extremely', 'rapid', 'rate', ',', 'how', 'she', 'snuck', 'up', 'on', 'the', 'nobility', ',', 'and', 'how', 'the', 'little', 'crowd', 'constantly', 'rose', 'and', 'that', 'every', 'time', 'the', 'nobles', 'noticed', ',', 'she', 'rose', 'again', '.'], ['kim', 'hang', ',', 'kim', 'jicheon', ',', 'and', 'kim', 'jun', 'all', 'worked', 'in', 'heavy', 'industry', '(', 'korea', ')', '.', 'they', 'also', 'worked', 'with', 'the', 'us', 'army', 'general', 'staff', '(', 'also', 'by', 'studying', 'the', 'information', 'language', 'of', 'korean', 'forces', ')', '.'], ['most', 'are', 'only', 'located', 'in', 'the', 'mainland', 'territories', 'of', 'brazil', '.', '1936', ':', 'miles', 'once', 'again', 'transports', 'bart', 'to', 'chicago', 'via', 'pasadena', ',', 'louisburg', 'park', ',', 'and', 'west', 'gate', ',', 'where', 'he', 'explores', 'the', 'respective', 'respective', 'cities', 'again', '.'], ['she', 'would', 'rather', 'not', 'let', 'it', 'talk', 'about', 'her', 'childhood', 'love', 'of', 'walking', ',', 'or', 'sailing', ',', 'or', 'reading', 'or', 'miniature', 'chess', 'and', 'solitaire', 'in', 'their', 'books', ',', 'or', 'other', 'women', 'to', 'see', 'how', 'their', 'magic', 'magic', 'worked', '.'], ['where', 'snow', 'flies', '.', 'have', 'we', 'met', '?', 'audra', 'is', 'the', 'northern', 'wind', 'in', 'the', 'alto', 'voice', '.', 'only', 'in', 'the', 'last', 'moment', '.', 'she', 'rocks', 'in', 'the', 'soprano', 'voice', '.', 'incites', 'all', 'within', 'us', 'to', 'history', '.'], ['red', '(', 'on', 'two', ')', 'red', '(', 'on', 'three', ')', 'dissident', '(', 'red', ')', 'this', 'is', 'why', '(', 'alternate', 'version', ')', 'ajant', 'sahu', ':', 'lurking', 'in', 'the', 'present', 'moments', 'after', 'time', 'begins', 'to', 'unwind', '.'], ['late', 'in', 'the', 'season', ',', 'robinson', 'replaced', 'tim', 'arnold', 'as', 'a', 'production', 'assistant', ',', 'replaced', 'by', 'tony', 'johnson', 'as', 'executive', 'director', 'of', 'series', '5', ',', 'who', 'later', 'acknowledged', 'that', 'robinson', 'had', 'just', 'shot', 'the', 'final', 'one', '-', 'off', 'episode', '.'], ['additional', 'attractions', 'include', 'a', 'series', 'of', 'interactive', 'exhibits', 'about', 'scuba', 'diving', 'and', 'pet', 'care', 'and', 'various', 'aquatic', 'creatures', ';', 'an', 'annual', 'outdoor', 'drawings', 'display', 'featuring', 'drawings', 'of', 'underwater', 'landmarks', ';', 'headcharge', 'of', 'photographic', 'pictures', 'on', 'copper', 'plates', ';'], ['when', 'he', 'won', 'the', '2000', 'general', 'election', ',', 'his', 'democratic', 'rivals', 'were', 'state', 'insurance', 'commissioner', 'todd', 'griffin', ';', 'montana', 'assemblyman', 'bady', 'davis', ';', 'democrat', 'dena', 'taylor', 'and', 'montana', 'state', 'forestry', 'commissioner', 'william', '\"', 'buck', '\"', 'white', '.'], ['10', '\"', 'out', '\"', '-', 'd', '.', 'm', '.', 'd', '.', '\"', 'out', '\"', '(', 'single', 'mix', ')', '-', 'unknown', 'self', '9', '\"', 'a', 'lot', 'stronger', '\"', '(', 'u', 'u', 'u', ')', '(', '2013', ')', '-', 'unknown', 'self', '9', ';'], ['\"', '(', 'alles', 'ehre', 'tag', '.', 'die', 'sangen', ')', 'by', 'ludwig', 'hummel', '\"', '-', 'chorus', ';', 'ana', 'maria', 'de', 'betos', 'and', 'diego', 'bello', '\"', 'celesto', ',', 'nena', '\"', '-', 'chorus', ';'], ['(', 'she', 'was', 'shy', ')', '.', 'abney', 'and', 'moore', 'wrote', 'this', 'song', 'together', '.', 'moore', 'wrote', 'the', 'first', 'three', 'songs', 'for', 'the', 'rolling', 'stones', 'of', 'the', 'banner', 'of', 'liberty', 'orchestra', 'with', 'whom', 'moore', 'shared', 'a', 'common', 'coddling', '.'], ['the', 'water', 'flows', 'freely', 'as', 'it', 'drops', 'from', 'the', 'walls', '.', '\"', 'he', 'glanced', 'around', '.', 'some', 'water', 'was', 'being', 'collected', 'from', 'one', 'of', 'the', 'deep', 'partitions', ',', 'and', 'tahir', 'remembered', 'a', 'spot', 'where', 'something', 'had', 'fallen', '.'], ['i', 'remember', 'thinking', 'once', 'that', 'once', 'one', 'day', 'we', 'reached', 'a', 'toy', 'store', '.', 'the', 'employees', 'at', 'that', 'store', 'said', 'they', 'were', 'too', 'excited', 'to', 'know', 'who', 'or', 'what', 'was', 'doing', 'that', 'toy', '.', 'at', 'that', 'moment', 'we', 'succeeded', '.'], ['how', 'many', 'years', 'over', 'which', 'does', 'she', 'venture', '?', 'one', 'year', 'ago', ',', 'in', 'early', 'spring', ',', 'my', 'mother', 'uprooted', 'trees', 'and', 'dove', 'into', 'the', 'howling', 'air', 'like', 'the', 'tail', 'end', 'of', 'a', 'hurricane', ',', 'rolling', 'upward', '.'], ['the', 'thing', 'was', 'but', 'they', 'wanted', 'us', '.', 'they', 'didn', \"'\", 't', 'want', 'us', ',', 'only', 'front', 'and', 'center', ',', 'and', 'otherworldly', 'enough', 'that', 'they', 'were', 'used', 'to', 'us', 'ramble', 'on', 'without', 'remembering', 'our', 'actual', 'names', '.'], ['the', 'five', 'homes', 'were', 'removed', 'and', 'renamed', \"'\", 'silver', 'street', \"'\", 'by', 'the', 'scottish', 'work', 'and', 'pensions', 'authority', 'as', 'part', 'of', 'their', '2009', 'rpu', '(', 'waste', 'disposal', 'development', ')', 'phase', '-', 'out', '(', 'waste', 'management', 'investment', ')', 'programme', '.'], ['-', 'it', \"'\", 's', 'about', 'time', '(', '1971', ')', '1999', 'jimmy', 'buffett', '-', 'fatal', 'attraction', '...', 'sold', 'out', '2000', 'live', '2000', 'tom', 'jones', '-', 'walk', 'away', \"'\", 'til', 'you', 'touch', 'me', '2000', 'lee', 'greenwood', '-', 'operetta', '!'], ['not', 'even', 'a', 'sigh', '(', 'which', 'was', 'why', 'i', 'had', 'done', 'it', ')', 'but', 'i', 'was', 'fighting', 'tears', '(', 'sort', 'of', 'poignantly', ')', ',', 'and', 'too', 'poignantly', ',', 'and', 'and', 'i', 'said', 'his', 'name', 'again', '.'], ['also', 'he', 'helped', 'to', 'create', 'telecommunications', 'company', 'proteus', 'communications', 'to', 'broadcast', 'in', 'the', 'far', 'north', ',', 'but', 'without', 'the', 'equipment', 'required', 'generally', 'for', 'the', 'construction', ',', 'development', ',', 'maintenance', ',', 'maintenance', ',', 'and', 'maintenance', 'of', 'certain', 'transmitters', '.'], ['appeared', 'in', '\"', 'money', 'wars', '\"', 'with', 'actor', 'crystal', 'jay', 'leno', ';', '(', 'also', 'appearing', 'in', 'alagoid', 'for', 'the', 'first', 'time', ')', 'the', '\"', 'i', 'want', 'help', '\"', 'season', 'three', 'finale', 'with', 'other', 'both', 'of', 'them', ';'], ['(', 'extended', 'version', 'for', 'quintet', ',', 'chorus', 'and', 'strings', 'on', 'two', 'cds', ')', 'the', 'one', '(', ')', '(', '1968', ')', '.', '(', 'bryan', 'erickson', ':', 'as', 'himself', ')', 'love', 'is', 'electric', '(', '\"', 'into', 'the', 'dark', '\"', ')', '.'], ['motivational', ',', 'sarcastic', 'and', 'quotation', 'marks', '.', 'in', 'south', 'dakota', ',', 'he', 'is', 'referred', 'to', 'as', '\"', 'de', '-', 'ed', '\"', ';', 'and', '\"', 'gop', '\"', '.', 'in', 'the', 'movie', 'ex', '-', 'soldiers', 'laughing', 'out', 'loud', '!'], ['(', 'from', 'the', 'greek', 'title', ',', '\"', 'st', '.', 'sophia', '\"', ')', 'in', 'the', 'same', 'letter', ',', 'alexander', 'spoke', 'well', 'of', 'sophia', ':', 'she', 'is', 'aware', 'of', 'the', 'future', 'of', 'the', 'world', ',', 'and', 'is', 'doing', 'the', 'right', 'thing', '.'], ['sir', 'william', 'evans', '-', 'vaughan', '-', 'emeritus', 'professor', ';', 'vice', '-', 'president', 'of', 'lseb', ';', 'vice', 'president', 'of', 'external', 'affairs', 'and', 'relations', '.', 'sir', 'michael', 'griffin', '-', 'psychologists', ',', 'specialist', 'in', 'motorway', 'and', 'road', 'safety', ';', 'philosopher', ';'], ['the', 'series', 'fire', 'and', 'ice', ':', 'the', 'raven', 'or', 'dragons', 'or', ':', 'the', \"'\", 'tales', 'of', 'the', 'dragon', 'master', 'fidelio', 'biagieri', 'is', 'the', 'title', 'on', 'the', 'third', 'page', 'the', 'author', 'of', 'this', 'book', 'is', 'found', 'showing', '.'], ['lacy', 'and', 'his', 'crew', 'filmed', 'scenes', 'at', 'the', 'coachwell', 'v', '.', 'j', '.', 'r', '.', 'estate', '(', 'a', 'common', 'law', 'prison', ')', '.', 'in', 'response', ',', 'lacy', 'told', 'reporters', 'he', '\"', 'is', 'mourning', 'for', 'his', 'lost', 'father', '\"', '.'], ['in', 'any', 'case', ',', 'they', 'were', 'grown', 'men', 'after', 'all', ',', 'with', 'blond', 'hair', 'and', 'gray', 'eyes', 'much', 'older', 'than', 'the', 'other', 'two', 'men', '.', '\"', 'i', 'hope', 'you', 'remember', 'today', '.', '\"', 'she', 'looks', 'the', 'three', 'men', 'over', '.'], ['anthony', '(', 'tony', ')', 'robinson', ',', 'who', 'served', 'as', 'a', 'consultant', 'to', 'the', 'congressional', 'and', 'fundraising', 'teams', 'from', 'american', 'democratic', 'presidential', 'primaries', 'in', 'the', '1980s', ',', 'provided', 'the', 'nrc', 'with', 'information', 'gathering', 'capabilities', 'under', 'the', 'ronald', 'reagan', 'administration', '.'], ['fortunately', ',', 'it', 'is', 'not', 'the', 'exact', 'spot', 'where', 'the', 'mother', '(', 'anne', 'dear', ')', 'was', 'born', 'while', 'the', 'set', 'history', 'refers', 'both', 'to', 'the', 'new', 'french', 'queen', 'and', 'to', 'the', 'wars', 'of', 'the', 'roses', '(', 'de', 'rohan', ')', '.'], ['\"', 'the', 'morristown', 'zoo', '\"', 'published', 'as', 'urban', 'recreations', 'of', 'freedom', '(', 'flash', 'press', ')', ',', 'cleveland', '1979', ';', 'zoo', 'and', 'aquarium', '\"', 'urban', 'recreations', 'of', 'freedom', '\"', 'published', 'by', 'flash', 'press', ',', 'new', 'jersey', '1979', ';'], ['another', 'woman', 'and', 'child', 'appear', ',', 'she', 'is', 'covered', 'in', 'bright', 'red', 'hair', 'and', 'has', 'her', 'hair', 'made', 'of', 'obsidian', '.', 'the', 'people', 'that', 'came', 'with', 'elevedra', '(', 'a', 'canoe', ')', 'split', 'up', 'to', 'search', 'for', 'him', '.'], ['the', 'camino', 'de', 'rayo', 'museum', '.', '\"', 'the', 'secret', '.', '\"', 'tribune', ',', 'january', '1996', 'the', 'new', 'york', 'times', ',', 'april', '1996', 'the', 'committee', 'on', 'government', 'expenditures', ':', 'alberto', 'madero', 'in', 'templehill', '1995', ',', 'p', '.'], ['given', 'a', 'o', ',', 'if', '\"', 'avelandian', 'is', 'y', '-', '\"', '(', 'the', 'o', ')', ',', 'then', '\"', 'avelandian', '\"', '(', '\"', 'the', 'o', '\"', ')', ',', 'then', '\"', 'the', 'o', 'is', 'y', '-', '\"', '.'], ['the', 'new', 'dragonfly', 'dragons', '(', 'springer', ',', 'june', ')', '.', '\"', 'behind', 'the', 'story', 'of', 'the', 'creation', 'of', 'dragonfly', 'city', '\"', 'in', 'which', 'the', 'novel', 'describes', 'dragonfly', 'as', '\"', 'still', 'holding', 'the', 'dragonworld', 'up', '\"', '.'], ['of', 'tain', 'malcolm', 'hamilton', ',', 'the', '3rd', 'earl', 'of', 'tain', 'donald', 'hamilton', ',', 'the', '3rd', 'earl', 'hamilton', 'she', 'had', 'two', 'great', '-', 'sons', ':', 'george', 'hamilton', ',', 'later', 'james', 'hamilton', ',', 'lord', 'lieutenant', 'of', 'scotland', 'alexander', 'hamilton', '.'], ['the', 'only', 'british', 'officer', 'objecting', 'were', 'winston', 'churchill', 'to', 'be', 'given', 'command', 'eastward', 'from', 'nelson', 'mandela', '.', '-', '-', '(', 'ed', '.', ')', '1940', '.', 'the', 'manchurian', 'plan', ':', 'a', 'plan', 'to', 'assist', 'the', 'empire', 'of', 'japan', '.'], ['adamson', 'has', 'himself', 'said', 'in', '1991', 'here', 'was', 'still', 'a', 'rare', 'moment', '-', 'recorded', 'of', 'traugowski', 'years', 'away', 'from', 'number', '1', '-', 'to', 'ask', 'if', 'the', 'album', ',', 'or', 'its', 'audience', ',', 'would', 'like', 'it', ';'], ['orchestral', 'interpretation', 'of', 'faust', '.', '[', 'music', 'by', 'h', '.', 'p', '.', 'thorp', 'and', 'henry', 'w', '.', 'whitworth', 'urchin', ']', '.', 'for', 'chorus', 'and', 'orchestra', '.', '[', 'by', 'juan', 'antonio', 'farelli', 'la', 'donna', ']', '.'], ['(', 'latac', '6', '.', '12', '-', 'i', 'means', \"'\", 'molar', \"'\", ')', \"'\", 'long', 'or', 'slender', ',', 'molar', '(', \"'\", 'broad', \"'\", ')', 'from', 'the', 'latin', 'word', 'for', 'long', 'molars', \"'\", 'a', '.', 'fr', '.'], ['pete', 'simpson', '(', 'executive', 'producer', 'of', 'the', 'simpson', 'family', ')', '(', 'former', 'anchor', 'of', 'live', '!', '!', '!', 'live', '!', 'and', 'the', 'waldorf', 'astoria', 'live', '!', ')', 'is', 'also', 'regularly', 'hosting', 'shows', 'at', 'different', 'venues', ',', 'including', 'live', '!'], ['1904', '/', '05', ':', 'saint', 'gall', 'rebuilt', 'as', 'a', 'parish', 'church', 'of', 'luneburg', '(', '\"', 'the', 'old', 'eastern', 'parish', 'church', 'of', 'saint', 'gall', '\"', ')', 'to', 'the', 'designs', 'of', 'm', '.', 'metzler', ',', 'r', '.', 'r', '.'], ['other', 'sources', 'of', 'information', 'arising', 'from', 'the', 'document', 'may', 'include', ':', '\"', 'white', 'list', '\"', 'information', ':', 'partial', 'list', 'of', 'executable', '\"', 'black', 'list', '\"', 'information', ':', 'partial', 'list', 'of', 'persons', 'who', 'are', 'not', 'united', 'states', 'citizens', '.'], ['she', 'married', 'geraldo', 'dea', '(', 'also', 'known', 'as', 'paco', ',', 'or', 'sometimes', 'pedro', ',', 'or', 'simply', 'matto', ')', ',', 'in', '1928', ',', 'and', ',', 'knowing', 'nothing', 'of', 'what', 'was', 'happening', ',', 'left', 'the', 'united', 'states', '.'], ['from', 'her', 'album', 'super', 'curtis', 'ross', '+', '(', 'sind', 'liebes', 'kerten', 'vor', ')', '\"', 'chantal', '(', 'instrumental', ')', '\"', '(', 'solo', ')', 'from', 'downeaster', '(', '2002', ')', 'super', 'curtis', 'ross', 'feat', '.'], ['hlaver', 'founded', 'the', 'philadelphia', 'office', 'of', 'the', 'american', 'gramophone', 'company', 'in', '1895', 'to', 'publish', '\"', 'catalogues', 'of', 'compositions', '\"', 'and', '\"', 'hand', '-', 'printed', '4', '-', 'page', 'printed', 'catalogues', 'for', 'the', 'gramophone', '\"', '.'], ['a', 'supporter', 'of', 'the', 'unfpa', '(', 'kids', 'against', 'hunger', ',', 'along', 'with', 'prachilpati', 'keegan', ')', '.', 'fighting', 'hunger', '(', 'scl', ')', '-', 'veterans', 'of', 'hunger', '(', 'founded', 'june', '6', ',', '1983', ')', ';'], ['scraphouse', '(', 'barney', 'brown', ',', '1979', ')', '-', 'luke', 'aglionby', '(', '2007', 'film', ')', '-', 'richard', 'lutjens', 'college', 'in', 'june', ',', '2015', 'illinois', 'celebrated', 'the', '100th', 'birthday', 'of', 'its', 'first', 'alumna', '.'], ['\"', 'you', 'carry', 'an', 'eight', '-', 'hundred', 'pound', 'gun', '.', 'not', 'take', 'the', 'old', 'one', 'with', 'you', '.', 'it', 'is', 'an', 'eight', 'hundred', 'pound', 'gun', '.', 'on', 'an', 'eight', '-', 'inch', 'wide', 'barrel', '.', 'one', ',', 'one', 'inch', 'long', ';'], ['azerbaijan', 'has', '13', 'major', 'economic', 'centers', ',', 'each', 'divided', 'into', 'three', 'economic', 'zones', '.', 'the', 'main', 'economic', 'centers', 'are', ':', 'an', 'industrial', ',', 'construction', '/', 'industrial', 'zone', ';', 'a', 'large', 'water', 'power', 'plant', 'and', 'a', 'sewage', '-', 'treated', 'station', ';'], ['throughout', 'the', 'series', 'the', 'reader', 'explores', 'the', 'themes', 'of', 'welsh', 'fairy', 'tales', '.', 'ivana', ',', 'meanwhile', ',', 'finds', 'work', 'in', 'a', 'musical', 'drama', 'at', 'the', 'royal', 'theatre', ',', 'and', 'in', 'the', 'autumn', 'her', 'magical', 'life', 'story', 'is', 'published', '.'], ['mary', 'jane', 'torments', 'nicholas', 'and', 'the', 'others', 'for', 'killing', 'the', 'aristocrat', '.', 'the', 'trilogy', 'ends', 'with', 'issue', 'number', '15', '.', 'samuels', \"'\", 'handmaidens', 'victoria', 'elizabeth', 'and', 'helen', 'are', 'still', 'part', 'of', 'the', 'pack', '.'], ['lorne', 'howard', '-', 'voice', 'actor', 'ben', 'russert', '-', 'successful', 'green', 'party', 'politician', '.', 'liz', 'holtheim', '-', 'a', 'young', 'and', 'well', 'dressed', 'inmate', 'at', 'pizzeria', 'central', 'and', '\"', 'star', 'club', '\"', ',', 'a', 'lesbian', 'institution', '.'], ['and', 'lady', 'she', 'is', 'a', 'figure', '.', 'and', 'the', 'lady', ',', 'upon', 'seeing', 'the', 'man', 'in', 'his', 'place', ',', 'looks', 'away', 'to', 'herself', '.', 'under', 'those', 'huge', 'red', 'oaks', ',', 'the', 'white', 'oaks', '!', 'try', 'and', 'make', 'her', 'so', '!'], ['lots', 'of', 'letters', 'in', 'all', '.', 'good', 'photographs', '.', 'i', 'write', 'to', 'my', 'friends', 'and', 'family', '.', 'i', 'send', 'it', 'home', '.', 'i', 'read', 'it', 'every', 'night', '.', 'blah', 'blah', 'blah', '.', 'when', 'dreams', 'come', 'on', 'i', 'always', 'sleep', '.'], ['wilson', ',', 'for', 'whom', 'he', 'was', 'responsible', 'for', 'writing', 'a', 'survey', 'in', '1835', '.', 'in', '1835', 'hall', ',', 'along', 'with', 'mr', '.', 'wilson', ',', 'was', 'making', 'evidence', 'to', 'the', 'commissioner', 'to', 'determine', 'tax', 'exemptions', 'in', 'scotland', ':', 'i', ';'], ['every', 'time', 'you', 'turned', 'it', 'over', 'the', 'rim', '.', 'overhead', ',', 'the', 'salt', ',', 'pepper', 'shakers', 'and', 'stimulators', 'kept', 'getting', 'louder', 'and', 'louder', ',', 'and', 'the', 'place', 'of', 'origin', 'became', 'more', 'dangerous', 'for', 'the', 'person', '.'], ['it', 'was', 'announced', 'on', 'itunes', 'in', 'europe', ',', 'europe', ',', 'australia', 'and', 'russia', '.', 'the', 'remix', 'of', 'hard', 'will', 'rock', '/', 'xerez', 'dope', 'by', 'william', '\"', 'bubba', '\"', 'mayer', 'was', 'released', 'later', 'to', 'promote', 'the', 'single', '.'], ['2012', 'live', 'video', '(', 'archive', 'recordings', ')', '.', '2014', 'live', 'at', 'o', '&', 'r', 'music', 'festival', '(', 'sab', 'recordings', ')', '.', '2015', '(', 'a', '-', 'sider', ')', 'live', '.', 'matt', 'savage', '.', '2017', 'live', 'at', 'archive', 'recordings', '.'], ['the', 'panel', 'only', 'included', 'two', 'documents', '(', 'ghr', '-', '90', 'and', 'ghr', '121', ')', '.', 'the', 'justice', 'department', 'included', 'document', '431', ',', 'which', 'pre', '-', 'dated', 'mdc', '-', '9', ',', 'in', 'annex', '30', '-', '4', '.'], ['collins', ',', 'mark', ';', 'biblical', 'criticism', ':', 'main', 'lessons', 'in', 'scholarship', 'for', 'modernity', ',', '3', '(', '2008', '-', '2009', ')', ',', '(', 'audacious', 'letters', ')', ',', '2', '(', 'may', '2009', ',', 'p', '.', '745', ')', '.'], ['the', 'students', 'in', 'first', 'grades', 'are', 'also', 'accepted', 'into', 'classes', 'while', 'the', '3rd', '(', 'high', 'school', ')', 'and', '4th', 'graders', 'receive', 'perfect', 'scores', '.', 'mandolin', 'playing', '-', 'mandolin', 'playing', 'usually', 'involves', 'students', 'practicing', 'a', 'set', 'of', 'rumba', 'sequences', '.'], ['the', 'latter', 'of', 'which', 'features', 'a', 'guest', 'appearance', 'from', 'wiley', 'and', 'a', 'new', 'remix', 'featuring', 'tahoe', '.', 'the', 'single', 'features', 'a', 'new', 'remix', 'of', '\"', 'thank', 'you', '-', 'i', 'ask', 'for', 'your', 'love', '\"', 'featuring', 'yung', 'dee', '.'], ['most', 'listed', 'examples', 'in', 'mathematics', 'include', 'axiomatic', ',', 'blaise', 'pascal', 'and', 'jean', 'odette', ',', 'hence', 'the', 'name', 'of', 'the', 'house', 'of', 'kanta', '.', 'mirzapur', 'is', 'also', 'called', '\"', 'house', 'of', 'kanta', '\"', '.'], ['dave', 'bedell', '(', 'host', 'of', 'the', 'simpsons', 'prime', 'time', ',', 'return', 'of', 'the', 'king', ')', ',', 'former', 'newscaster', 'for', 'nbc', ',', 'still', 'appearing', 'by', 'request', '.', 'karen', 'bailey', ',', 'executive', 'producer', '.', 'glenn', 'beck', ',', 'newscaster', '.'], ['for', 'users', 'who', 'have', 'access', 'to', 'personal', 'msp', '-', 'dos', 'and', 'utf', '-', 'dos', 'processors', ',', 'it', 'included', 'free', 'versions', 'of', 'dos', ',', 'silverlight', ',', 'and', 'pc', 'windows', 'runs', 'on', 'the', 'msp', '-', 'dos', 'core', '.'], ['carmelita', '-', '(', 'was', 'eliminated', ')', 'chesney', '-', '(', 'winner', ')', 'sheryl', '\"', 'rocky', '\"', 'roca', 'd', '.', 'john', 'b', '.', 'skaggs', '-', 'eliminated', '(', ')', 'jeff', '-', '(', 'winner', ')', 'ricky', 'd', '.'], ['to', 'her', ',', 'in', 'a', 'sense', ',', 'she', 'was', 'a', 'tongue', 'of', 'strength', 'and', 'understanding', ',', 'able', 'to', 'endure', 'as', 'much', 'of', 'life', 'as', 'a', 'problem', 'kir', 'needed', 'to', 'solve', ',', 'and', 'yet', 'to', 'be', 'well', 'looked', 'after', '.'], ['the', 'studio', 'episode', ',', \"'\", 'over', '2', 'million', \"'\", ',', 'appeared', 'in', 'the', 'daily', 'mail', 'in', 'january', '2005', '.', 'in', 'july', '2006', 'the', 'band', 'were', 'featured', 'by', 'the', 'sarrazin', 'festival', 'in', 'the', 'australian', 'juried', 'show', 'e', '!'], ['jack', 'and', 'abbie', 'had', 'camped', 'there', 'almost', 'a', 'week', 'when', 'it', 'was', 'festive', '.', 'looking', 'back', ',', 'they', 'were', 'nervously', 'buttoning', 'their', 'clothing', 'now', ',', 'as', 'if', 'trying', 'to', 'keep', 'them', 'from', 'being', 'scabs', '.'], ['haxshyyrd', ',', 'former', 'a', '.', 'r', '.', 'u', '.', 'lead', 'singer', 'and', 'lead', 'guitarist', 'of', 'the', 'justicias', ',', 'later', 'became', 'a', 'well', 'known', '\"', 'hottest', 'thing', '\"', 'singer', '(', 'like', 'kennedy', ')', '.'], ['wec', 'hosted', '(', 'week', '2', ')', 'a', '8', 'player', 'monday', 'night', 'poker', 'contest', ',', 'called', '\"', 'the', 'bank', 'of', 'the', 'americas', 'poker', 'contest', '\"', ',', 'in', 'late', '1965', '.', 'wec', 'reopened', 'in', '1971', 'at', 'nearby', 'elm', 'hill', '.'], ['the', 'malden', 'and', 'the', 'burnet', 'line', 'also', 'carry', 'henry', \"'\", 's', 'birth', 'name', '-', \"'\", 'john', 'henry', \"'\", 's', \"'\", '-', 'said', 'to', 'mean', '[', \"'\", 'john', 'henry', ',', 'the', 'english', 'patriot', \"'\", ']', '-', 'on', 'them', '.'], ['he', 'looked', 'at', 'us', 'again', 'and', 'started', 'asking', 'us', 'questions', '.', 'he', 'gave', 'us', 'a', 'story', 'on', 'camera', 'and', 'we', 'asked', 'him', 'what', 'it', 'was', 'about', ',', 'about', 'the', 'woman', 'in', 'the', 'hospital', 'when', 'i', 'got', 'a', 'hunch', '.'], ['first', 'thing', 'in', 'the', 'morning', ',', 'let', 'him', 'out', '.', 'jack', 'will', 'struggle', 'to', 'get', 'to', 'his', 'room', '.', 'he', 'has', 'to', 'move', 'fast', ',', 'try', 'to', 'keep', 'up', 'against', 'the', 'constant', 'fear', 'of', 'being', 'run', 'over', 'by', 'someone', '.'], ['greetings', 'from', 'you', 'to', 'me', 'ruth', '.', 'what', 'is', 'to', 'be', 'said', 'in', 'terms', 'of', 'sexual', 'companionship', '?', 'this', 'is', 'a', 'bit', 'of', 'a', 'trick', '.', 'this', 'is', 'undoubtedly', 'not', 'a', 'trick', ',', 'but', 'my', 'friend', 'ruth', '.'], ['featuring', 'tom', 'o', \"'\", 'dealon', ',', 'of', '\"', 'simple', 'life', '\"', 'feat', '.', 'mike', 'kanyon', ',', 'of', '\"', 'rolling', 'stone', 'subaru', '\"', '.', '\"', 'a', 'truly', 'alive', 'and', 'happy', 'christmas', '.', '\"', 'matt', 'smith', '-', 'guitar', '.'], ['anne', '-', 'marie', 'marie', ',', 'actress', 'and', 'former', 'tv', 'personality', '.', 'marie', '-', 'rose', '(', 'nee', 'blane', ')', ',', 'same', '-', 'sex', 'couple', ',', 'illegitimate', 'daughter', 'of', 'the', 'hon', '.', 'eric', 'peterson', '(', 'born', '29', 'february', '1926', ')', '.'], ['as', 'possible', '\"', 'joint', 'venture', '\"', 'in', '2003', ',', 'the', 'joint', 'venture', 'had', 'his', 'sales', 'office', 'in', 'namcapram', '.', 'at', 'namcapram', 'company', 'started', 'business', 'on', 'a', 'single', 'capital', 'basis', 'with', 'commerce', 'and', 'foreign', 'trade', 'sectors', '.'], ['and', 'most', 'of', 'the', 'highway', 'takes', 'a', 'state', '-', 'mandated', 'designation', '(', 'identified', 'route', ')', '(', 'lts', ')', 'to', 'ponderosa', 'pass', '(', 'between', 'counties', ')', 'with', '2', ',', '3', ',', '7', 'campgrounds', 'created', 'along', 'the', 'way', '.'], ['the', 'e40', ',', 'e52', 'and', '7455', 'supercharger', 'will', 'be', 'given', 'the', 'additional', '\"', 'f', '\"', 'designation', ',', 'a', 'reference', 'to', 'the', 'remaining', 'notes', 'which', 'had', 'been', 'previously', 'ignored', 'by', 'the', 'international', 'federation', 'for', 'denmark', '.'], ['smith', \"'\", 's', 'first', 'deejay', '/', 'banjo', 'song', '\"', 'hold', 'on', '\"', 'preceded', '\"', 'goin', \"'\", '\"', 'in', '1962', 'in', 'the', 'basement', 'rehearsal', 'room', 'of', 'quality', 'music', 'recording', 'studios', 'in', 'doylestown', 'behind', 'a', 'department', 'store', 'counter', '.'], ['you', 'could', 'tap', 'into', 'the', 'people', 'around', 'you', '.', 'the', 'people', 'with', \"'\", 'pics', \"'\", 'of', 'their', 'names', 'and', 'numbers', 'and', 'names', ',', 'the', 'people', 'with', 'you', 'in', 'mind', ',', 'but', 'not', 'the', 'stronger', ',', 'the', 'strong', 'one', '.'], ['now', 'let', 'be', 'an', 'operator', 'and', 't', 'be', 'called', 'the', 'product', 'of', 'the', 'operators', ',', 'its', 'addition', 'operator', 'and', 'its', 'derivatives', '.', 'let', 't', 'be', 'the', 'derivative', 'of', 'the', 'product', '(', 'x', ')', '(', 'f', ')', 'of', 'all', 'k', '.'], ['or', ',', 'as', 'he', 'often', 'said', ',', '\"', 'yet', 'there', 'is', 'a', 'certain', 'luster', 'to', 'you', '.', '\"', 'had', 'he', 'been', '\"', 'deepening', 'and', 'becoming', 'attracted', 'to', 'me', '\"', '?', 'not', 'that', 'i', 'knew', 'all', 'that', 'part', '.'], ['lyssa', 'starbuck', ',', 'from', 'the', '2000', 'film', 'a', 'letter', 'from', 'wonder', 'woman', ',', 'and', 'lt', 'armstrong', 'give', 'their', 'names', 'to', 'the', 'new', 'x', '-', 'men', '.', 'the', 'players', 'are', 'charged', 'with', 'completing', 'their', 'missions', 'and', 'operations', ';'], ['\"', 'know', 'exactly', 'what', 'is', '.', '\"', 'this', 'was', 'important', '-', 'an', 'important', 'purpose', '.', 'she', 'followed', 'his', 'gaze', 'a', 'mile', 'and', 'a', 'half', ',', 'almost', 'as', 'if', 'trying', 'to', 'pick', 'any', 'spot', 'in', 'the', 'distance', 'that', 'could', 'be', '.'], ['no', 'one', 'dares', 'love', 'coming', 'out', 'of', 'a', 'place', '.', 'friends', 'on', 'the', 'streets', '-', 'blaring', 'blues', 'to', 'hurt', 'their', 'hearts', '-', 'entering', 'the', 'door', '.', 'somebody', 'listening', 'keeperman', '-', 'played', 'his', 'own', 'music', '.'], ['a', 'throne', 'has', 'been', 'placed', 'beside', 'it', '.', 'she', 'and', 'charmion', 'has', 'green', 'splint', 'wings', '.', 'the', 'three', 'ships', 'after', 'i', 'have', 'all', 'moored', 'together', ',', 'all', '\"', 'helm', '-', 'men', '\"', 'and', 'mermen', '.'], ['silver', 'winner', ':', 'best', 'player', 'for', 'opening', 'or', 'closing', 'of', 'the', 'game', ';', 'gold', 'winner', ':', 'tom', 'price', ',', 'best', 'player', 'for', 'scoring', ',', 'or', '\"', 'six', '-', 'on', '-', 'two', '\"', ',', 'for', 'the', 'washington', 'senators', 'baseball', 'team', ';'], ['in', '2012', 'he', 'was', 'killed', 'by', 'prakash', 'raj', 'lodhi', '.', 'khonda', '(', '2002', ')', '-', 'a', 'love', 'story', 'of', 'jugal', 'puri', 'delhi', '(', 'along', 'with', 'chittadevi', ')', '-', 'was', 'a', 'film', '.'], ['\"', 'goli', 'thank', 'you', ',', 'thank', 'you', ',', 'our', 'thanks', ',', 'our', 'thanks', ',', 'thank', 'you', '!', '\"', 'happily', 'proclaimed', 'goli', '!', '!', '!', '!', '!', 'who', 'are', 'you', '?', '!', '!', '!', '!', '!', '!', '!', '!'], ['georgetown', 'post', 'office', 'and', '\"', 'the', 'kennewick', 'high', 'street', 'gaol', '\"', '(', 'after', 'the', 'georgetown', 'post', 'office', '&', 'courthouse', ')', ',', 'constructed', 'in', '1890', '.', '\"', 'georgetown', '\"', 'limestone', 'quarries', 'and', '\"', 'south', 'georgetown', '\"', 'association', '.'], ['it', 'is', 'named', 'after', 'him', '.', 'the', 'following', 'settlements', 'are', 'closely', 'associated', 'with', 'present', 'day', 'burlington', '.', 'these', 'include', ':', 'the', 'southern', 'limit', 'of', 'the', 'city', 'of', 'waukesha', 'which', 'is', 'identical', 'with', 'its', 'major', 'centre', 'points', 'south', ';'], ['that', 'was', 'when', 'i', 'knew', 'they', 'were', 'bent', 'over', ',', 'lying', 'in', 'their', 'own', 'bed', 'suitably', 'clothed', '.', 'i', 'might', 'have', 'tried', 'to', 'sit', 'up', 'and', 'cover', 'them', '.', 'keeping', 'my', 'eyes', 'on', 'them', ',', 'i', 'slowly', 'approached', '.'], ['come', 'back', 'to', 'kentucky', 'and', 'take', 'our', 'young', 'love', 'with', 'us', 'at', 'some', 'point', '.', 'we', 'will', 'live', 'a', 'long', 'time', ',', 'on', 'a', 'farm', 'at', 'mead', 'lake', 'or', 'somewhere', 'up', 'the', 'hill', 'for', 'about', 'a', 'hundred', 'years', 'later', '.'], [\"'\", 'that', 'baby', 'of', 'yours', 'smells', 'amazing', ',', \"'\", 'he', 'had', 'said', ',', 'trying', 'to', 'be', 'invisible', 'to', 'his', 'driver', '.', 'but', 'it', 'was', 'no', 'use', '.', \"'\", 'a', 'dickhead', ',', \"'\", 'he', 'said', ',', 'frowning', 'artily', '.'], ['there', \"'\", 's', 'also', 'a', 'video', 'recording', 'session', 'session', 'where', 'they', 'record', 'the', 'songs', '.', 'this', 'sketch', 'includes', 'a', 'stub', 'parade', 'of', 'new', 'guests', 'such', 'as', 'kim', 'with', 'her', 'new', 'baby', '.', 'this', 'event', 'is', 'not', 'officially', 'sanctioned', '.'], ['margaret', 'gosforth', 'appeared', 'in', 'two', 'subsequent', 'season', 'aside', 'from', 'season', '12', 'of', 'to', 'save', 'your', 'life', '.', 'carrying', 'on', 'with', 'the', 'show', ',', 'the', '[', 'challenge', ']', 'of', 'food', 'started', ',', 'now', 'being', 'hosted', 'by', 'mike', 'johnson', '.'], ['she', 'issues', 'regular', 'e', '-', 'mail', 'and', 'phone', 'apps', ',', 'which', 'enables', 'email', '.', 'she', 'was', 'born', 'on', 'nevis', ',', 'into', 'a', 'maritime', 'family', 'that', 'has', 'taken', 'on', 'tales', 'of', 'ancient', 'offshore', 'wrecks', 'and', 'small', 'scale', 'mining', '.'], ['no', '.', 'no', '.', 'not', 'yet', '.', 'there', 'were', 'several', 'blankets', 'here', '.', 'they', 'had', 'folded', 'them', 'neatly', ',', 'and', 'one', 'blanket', '-', 'he', 'had', 'escaped', '-', 'was', 'wrapped', 'so', 'tightly', 'now', 'that', 'i', 'could', 'barely', 'see', 'it', '.'], ['2007', ':', '\"', 'academy', 'of', 'jazz', ',', '\"', '\"', 'drum', 'machines', '&', 'synthesizers', '\"', ';', '(', 'with', 'eduardo', 'giuffi', ',', 'guitarist', 'and', 'music', 'researcher', ')', '\"', 'mirrors', '\"', ';', '\"', 'a', 'selection', 'of', 'a', 'composer', '\"', ';', ';'], ['state', 'of', 'the', 'union', ':', 'live', 'at', 'studio', 'a', '(', 'cecil', 'taylor', ')', '-', 'guitar', 'john', 'martin', '-', 'bass', 'glynn', '\"', 'lou', '\"', 'gulliver', '(', 'old', 'school', ')', 'rod', 'jones', '(', 'home', 'improvement', ')', '-', 'keyboards', ';'], ['standing', 'captive', 'under', 'the', 'rope', 'ladder', 'to', 'escape', 'is', 'phil', 'digby', ',', 'who', 'works', 'for', 'the', 'local', 'fire', 'department', 'fighting', 'the', 'fire', '.', 'they', 'barely', 'escape', 'the', 'flames', 'when', 'the', 'police', 'arrive', 'and', 'the', 'fire', 'rages', 'on', '.'], ['asil', '.', 'covers', '3', ',', '400', 'power', 'production', 'plants', 'and', '1000', 'raw', 'energy', 'processes', '.', 'also', 'happenings', 'are', 'the', 'interactive', 'presentation', 'of', 'several', 'scientific', 'conferences', 'and', 'the', 'invited', 'presentation', 'of', 'juan', 'carlos', 'llorens', ',', 'actor', '.'], ['her', 'father', 'had', 'been', 'at', 'what', 'was', 'a', 'divorce', 'case', 'a', 'long', 'time', '.', 'june', '7', ',', 'july', '7th', '1958', '*', '*', '*', '*', '*', '*', '\"', 'oh', 'god', ',', 'a', 'storm', 'is', 'coming', '.', '\"', 'and', 'for', 'what', '?'], ['1902', ':', 'a', 'printer', 'and', 'photographic', 'shop', '.', '\"', 'venus', ',', '\"', 'a', 'ghulam', 'statue', 'of', 'the', 'island', 'by', 'sculptor', 'harry', 'richards', '.', '1938', ':', 'memorial', 'to', 'captain', 'moore', '.', 'sydney', '.', '1924', ':', 'cape', 'tryon', '.'], ['grahame', 'scott', 'is', 'a', 'british', '-', 'australian', 'writer', 'and', 'artist', '(', 'cto', ')', ',', 'of', 'alien', ',', 'the', 'flying', 'saucers', '(', 'who', 'played', 'in', 'the', 'independent', 'tv', 'series', 'the', 'invisible', 'man', ')', ',', 'and', 'outer', 'space', '.'], ['the', 'men', 'who', 'had', 'decided', 'to', 'come', 'home', 'for', 'the', 'country', 'included', 'a', 'doctor', 'and', 'occasionally', 'a', 'butcher', 'and', 'a', 'baker', ',', 'and', 'some', 'were', 'considering', 'going', 'to', 'mexico', ',', 'while', 'others', 'were', 'out', 'in', 'the', 'rump', 'country', '.'], ['i', 'was', 'in', 'no', 'mood', 'to', 'grapple', 'with', 'him', '.', 'the', 'two', 'secret', 'service', 'agents', 'whipped', 'around', ',', 'tomlinson', 'wearing', 'everything', 'from', 'a', 'white', 'armband', 'to', 'a', 'brown', 'gun', 'belt', ',', 'a', 'black', 'belt', 'tucked', 'out', '.'], ['it', 'follows', 'the', 'protagonist', 'male', 'protagonist', 'jonathan', ',', 'until', 'he', 'approached', 'his', 'mother', 'and', 'told', 'her', 'to', 'talk', 'to', 'the', 'solessitor', 'about', 'her', 'own', 'feelings', 'and', 'behavior', 'before', 'getting', 'into', 'his', 'car', 'and', 'driving', 'to', 'the', 'restaurant', '.'], ['carter', ',', 'as', 'i', 'recall', ',', 'was', 'returning', 'from', 'west', 'virginia', 'with', 'the', 'young', 'man', 'he', 'carried', 'with', 'him', '.', 'a', 'moment', 'ago', 'a', 'fox', 'had', 'cut', 'its', 'way', 'through', 'the', 'trees', '.', 'how', 'had', 'they', 'possibly', 'gotten', 'here', '?'], ['although', 'he', 'didn', \"'\", 't', 'often', 'see', 'her', 'for', 'regular', 'rehearsals', 'it', 'was', 'apparent', 'she', 'was', 'without', 'him', '.', 'she', 'cowered', 'behind', 'a', 'very', 'big', ',', 'meaty', 'monster', 'on', 'what', 'he', 'called', '\"', 'fazire', '\"', '.'], ['susan', 'is', 'as', 'excited', 'as', 'ever', 'about', 'meeting', 'her', 'best', 'friends', 'and', 'music', 'fans', '.', 'she', 'throws', 'club', 'parties', 'now', 'and', 'then', '-', '-', 'just', 'like', 'the', 'record', 'dealers', 'and', 'we', 'learn', 'a', 'lot', 'about', 'the', 'big', 'deal', 'songwriters', '.'], ['the', '\"', 'mozart', 'concertos', '\"', 'concert', '(', '2007', ')', ',', 'the', '\"', 'choral', 'music', 'for', 'boy', 'choir', 'and', 'adolescent', 'choirs', '\"', '\"', 'friends', '\"', 'concert', '(', '2008', ')', 'and', '\"', 'music', 'from', 'the', 'far', 'east', '\"', 'ensembles', 'live', '.'], ['but', 'for', 'the', 'earth', 'observer', 'community', ',', 'besides', 'pluto', '-', 'and', 'its', 'earth', 'satellite', '-', 'there', 'are', 'other', 'asteroid', 'belts', 'easy', 'to', 'map', '.', 'the', 'question', 'is', 'whether', 'the', 'gravity', 'of', 'the', 'earth', 'is', 'greater', 'than', 'the', 'surrounding', 'area', '.'], ['this', 'war', 'was', 'a', 'real', 'threat', '.', 'the', 'judge', ',', 'justice', 'taib', 'akali', ',', 'nabbed', 'many', 'witnesses', 'as', 'they', 'were', 'attacked', 'and', 'killed', 'by', 'employees', 'of', 'punjab', 'and', 'haryana', 'shell', 'shell', 'shell', 'and', 'the', 'cic', '.'], ['after', 'defeating', 'edge', 'and', 'winning', 'the', 'nwo', 'title', 'along', 'the', 'way', ',', 'slater', 'helped', 'edge', 'win', 'a', 'caballeros', 'pinfall', 'to', 'face', 'a', 'team', 'consisting', 'of', 'ten', 'other', 'non', '-', 'nwo', 'wrestlers', 'at', 'wrestlemania', 'iii', '.'], ['cuzco', ',', 'argentina', ',', 'title', ':', 'a', 'book', 'on', 'the', 'spanish', 'language', '.', 'new', 'york', 'city', ':', 'elmsall', ',', '1922', 'john', 'shakespeare', ',', 'a', 'simple', 'man', ',', 'london', '1937', 'bens', 'merrill', ',', 'new', 'york', '.'], ['perhaps', 'he', '-', 'or', 'perhaps', 'he', 'was', 'the', 'father', 'of', 'the', 'child', '-', 'may', 'have', 'been', 'struck', 'by', 'the', 'wind', 'and', 'the', 'storm', 'scene', '.', 'tom', 'waits', '(', 'ep', '.', '48', ')', '(', '2003', 'film', ')', 'w', '.', 'k', '.'], ['alva', 'arthur', 'shipman', ',', 'of', 'new', 'jersey', 'was', 'an', 'american', 'politician', ',', 'lawyer', ',', 'jewish', 'community', 'leader', ',', 'and', 'general', 'secretary', 'of', 'cultural', 'and', 'civic', 'affairs', 'under', 'two', 'us', 'presidents', ',', 'jay', 'rockefeller', 'and', 'hugh', 'blackmun', '.'], ['tartina', 'told', 'him', 'that', 'evening', 'he', 'would', 'marry', '\"', 'bill', 'mcmurtry', '\"', ',', 'and', 'he', 'got', 'paid', '.', 'he', 'performed', 'with', 'the', 'rookwood', 'country', 'ladies', ',', 'and', 'subsequently', 'he', 'met', 'ruth', 'huebner', 'of', 'dallas', '.'], ['susan', 'elizabeth', 'hines', 'is', 'a', 'houston', 'writer', '.', 'susan', 'elizabeth', 'hines', 'is', 'a', 'noted', 'southern', 'regional', 'physician', 'and', 'former', 'academic', 'at', 'southwestern', 'independent', 'senior', 'high', 'school', '.', 'a', 'former', 'cbls', 'student', ',', 'kath', 'hines', 'is', 'a', 'playwright', '.'], ['philadelphia', 'architect', 'william', 'jearns', ';', 'architect', 'thomas', 'quigley', ';', 'lee', 'thomas', 'west', 'subdivision', 'avenue', ',', '1985', ';', 'fifth', 'avenue', ',', '1990', ';', 'west', ',', 'between', 'w', 'and', 'g', ';', 'east', '1978', ';', 'fifth', 'avenue', 'road', 'h', '1968', ';'], ['or', ',', 'more', 'specifically', ',', 'offshore', 'islands', '(', 'such', 'as', '\"', 'scapa', 'verde', '\"', ',', 'seen', 'in', 'four', 'james', 'bond', 'films', ')', '.', 'a', 'wood', 'microdrama', 'is', 'a', 'very', 'rare', 'type', 'of', 'high', '-', 'value', 'wood', '.'], ['netball', 'at', 'an', 'amateur', 'level', 'was', 'generally', 'popular', 'prior', 'to', 'the', 'involvement', 'in', 'the', 'victorian', 'football', 'league', 'amalgamations', 'which', 'occurred', 'in', 'the', '1930s', '.', 'trade', 'and', 'labor', 'groupings', 'promoted', 'netball', 'while', 'campaigning', 'for', 'the', 'inclusion', 'of', 'women', '.'], ['a', 'little', 'lecture', 'going', 'on', 'about', 'reality', 'and', 'fantasy', 'and', 'all', 'stories', 'appeared', 'to', 'be', 'of', 'short', 'end', 'by', 'going', 'home', '.', 'and', 'then', 'there', 'was', 'celin', ',', 'outrun', 'by', 'elin', ',', 'who', 'the', 'king', 'loved', '.'], ['joe', ',', 'joe', ',', 'joe', ',', 'me', '!', 'god', 'help', 'me', 'finish', '!', 'so', 'is', 'french', 'polynesia', '!', 'and', 'but', ',', 'based', 'on', 'quality', 'data', ',', 'france', 'has', 'produced', 'the', 'greatest', 'data', 'in', 'the', 'world', '[', '6', ']', '.'], ['preface', 'by', 'nicolas', 'jouve', '\"', 'c1908', ',', '\"', 'goudie', ',', 'john', 's', '.', '/', 's', '.', 'hillsong', '\"', 'ackard', 'about', 'wegmann', ',', 'with', 'essays', 'by', 'anderson', ',', 'h', '.', '\"'], ['the', 'paul', 'federstein', 'holocaust', 'studies', 'library', 'and', 'museum', 'were', 'redesigned', 'in', '2007', 'as', 'the', 'holocaust', 'is', 'gone', '.', 'london', ',', 'new', 'york', 'series', '4', ':', 'jews', ',', 'series', '5', ':', 'ten', 'thousand', 'jews', ',', 'carl', 'alens', '.'], ['more', 'information', 'is', 'available', 'here', '.', 'original', 'collection', 'harrison', 'child', \"'\", 's', 'library', '.', 'clarendon', 'press', 'oclc', '2555566', 'http', ':', '/', '/', 'myt', '.', 'org', '/', 'harrison', 'child', \"'\", 's', 'library', '.', ',', 'pls', '.'], ['they', 'broke', 'communion', 'with', 'those', 'themselves', 'associated', 'with', 'fascism', 'but', 'were', 'soon', 'opposed', '.', 'soon', 'after', 'merano', 'and', 'adriano', 'ramu', 'supported', 'dr', '.', 'bianchi', 'and', 'in', '1932', 'they', 'became', 'affiliates', 'of', 'the', 'third', 'international', '.'], ['he', 'had', 'made', 'in', 'your', 'power', 'album', 'previously', '.', 'his', '1986', 'solo', 'album', 'the', 'day', 'i', 'went', 'home', 'received', 'a', 'grammy', 'nomination', 'along', 'with', 'his', 'two', 'full', '-', 'length', 'albums', 'midnight', 'sun', 'and', 'la', 'lune', 'in', 'new', 'orleans', '.'], [\"'\", 'your', 'strange', 'and', 'crazy', 'story', 'will', 'make', 'the', 'police', 'sound', ',', 'you', 'know', '?', \"'\", \"'\", 'not', 'going', 'to', 'happen', ',', \"'\", 'says', 'mr', 'lunsford', '.', \"'\", 'are', 'you', 'really', 'crazy', '?', \"'\", 'asks', 'eloise', '.'], ['he', 'declined', 'to', 'join', 'the', 'team', 'formed', 'by', 'jeff', 'princock', '.', 'the', 'company', 'was', 'awarded', 'a', 'crt', '-', 'type', 'construction', 'certification', 'from', 'the', 'junior', 'college', 'of', 'missouri', '(', 'now', 'st', '.', 'louis', 'university', 'in', 'missouri', ')', '.'], ['is', 'a', 'television', 'series', 'pairing', 'the', 'creators', 'of', 'the', 'non', '-', 'regular', 'series', 'and', 'realtime', 'viewer', '.', 'angel', 'of', 'death', '(', '2008', ')', 'krybantio', '(', '2012', ')', '(', 's1', ')', 'one', 'take', 'on', 'e', '!'], ['there', 'was', 'no', 'space', 'now', ',', 'no', 'personal', 'life', 'between', 'us', '.', 'in', 'the', 'middle', 'there', 'was', 'a', 'bed', '.', 'the', 'bigger', ',', 'the', 'better', 'it', 'looked', '.', 'we', 'had', 'been', 'this', 'close', 'before', 'the', 'party', 'had', 'even', 'begun', '.'], ['5', '(', 'david', 'guetta', ')', 'jose', 'luis', 'maria', 'morena', ',', '2004', 'la', 'madre', 'de', 'oro', '(', 'aqui', 'que', 'no', 'yo', ')', 'gary', 'matthews', ',', '2007', 'vol', '.', '2', 'sampler', 'in', 'banda', 'latina', 'vol', '.'], ['in', '1493', ',', 'she', 'travelled', 'back', 'to', 'england', ',', 'found', 'a', 'hand', '-', 'painted', 'silver', 'piece', 'of', 'cloth', 'and', 'learned', 'that', 'at', 'shinjuku', 'castle', ',', 'she', 'had', 'been', 'taken', 'and', 'murdered', 'while', 'still', 'being', 'a', 'slave', '.'], ['paul', 'adams', ',', 'president', 'of', 'the', 'national', 'media', 'management', 'and', 'advertising', 'producers', 'association', 'approached', 'douglas', ',', 'proving', 'that', 'they', 'had', 'fallen', 'in', 'love', ',', 'and', 'later', 'that', 'same', 'day', 'they', 'were', 'spotted', 'drinking', 'and', 'the', 'letter', 'was', 'mailed', '.'], ['in', ':', 'building', 'a', 'better', 'world', '(', 'video', ')', '#', '48', ',', 'min', '.', ',', '(', 'also', 'crowdfunded', 'by', 'w', '.', 'a', '.', 'robinson', ')', '\"', 'combating', 'myopia', '[', 'the', 'photography', ']', '\"', '.', '|'], ['\"', 'i', 'see', 'a', 'great', 'dark', 'mountain', 'on', 'the', 'horizon', '.', '.', '.', 'deep', ',', '\"', 'the', 'captain', 'had', 'stated', '.', '\"', 'no', ',', 'not', 'lemae', 'the', 'night', 'we', 'got', 'aboard', ',', '\"', 'he', 'finished', '-', 'gulped', '.'], ['she', 'became', 'an', 'idol', 'and', 'appeared', 'on', 'voice', 'of', 'asia', \"'\", 's', 'we', 'need', 'shelter', ':', 'when', 'night', 'falls', '(', 'december', '16', ')', '.', 'karina', 'released', 'her', 'debut', '12', '-', 'inch', 'and', 'first', 'international', 'single', 'smiling', 'at', 'you', '!'], ['103', '.', 'if', 'a', 'particular', 'incident', 'involving', 'sarkot', ',', 'a', 'colonist', ',', 'docker', ',', 'englishman', ',', 'or', 'the', 'lamas', 'was', 'deemed', 'to', 'fit', 'in', 'the', 'lamas', 'incident', '(', 'c', ')', ',', 'abt', '.'], ['the', 'relevant', 'factors', 'considered', 'are', 'the', 'described', 'by', 'the', 'organization', 'and', 'interactions', 'between', 'matter', 'and', 'structure', 'as', 'constants', '.', 'the', 'other', '(', 'empirical', ')', 'factors', 'one', 'should', 'consider', 'are', 'by', 'the', 'area', 'and', 'dynamics', 'of', 'matter', 'and', 'structure', '.'], ['dave', 'lloyd', '(', 'in', '\"', 'back', 'in', 'fun', '!', '\"', ')', '.', 'bbc', 'radio', 'uk', '.', 'performed', 'as', 'captain', 'gordon', 'in', '\"', 'muppets', '\"', '.', 'also', 'a', '\"', 'playman', '\"', ',', 'in', 'go', '-', 'go', 'plot', '.'], ['she', 'had', 'originally', 'been', 'from', 'virginia', ',', 'moving', 'in', '1853', ',', 'and', 'he', 'would', 'often', 'introduce', 'her', 'to', 'professionals', 'such', 'as', 'adam', 'swartz', ',', 'a', 'renowned', 'lawyer', 'and', 'former', 'prison', 'guard', 'involved', 'in', 'several', 'colleges', 'and', 'hospitals', '.'], ['they', 'went', 'to', 'a', 'poor', 'school', 'and', 'tried', 'their', 'days', 'playing', 'mattoons', '.', 'a', 'new', 'group', 'was', 'a', 'few', 'very', 'old', 'english', 'ladies', 'who', 'used', 'custom', 'tools', 'to', 'do', 'the', 'talking', ',', 'and', 'it', 'was', 'gaining', 'popularity', 'quickly', '.'], ['john', 'peter', 'clark', ',', 'a', 'newspaper', 'editor', 'based', 'in', 'nashville', 'and', 'director', 'of', 'the', 'nashville', 'office', ',', 'once', 'told', 'reporters', 'working', 'for', 'bill', 'mcclung', 'that', '\"', 'the', 'ken', 'baker', 'letter', 'literally', 'mashed', 'the', 'school', 'bus', '\"', '.'], ['-', 'and', 'this', 'is', 'the', 'last', 'time', 'there', 'was', 'a', 'dead', 'man', ',', 'i', 'thought', '.', '\"', '\"', 'yes', ',', '\"', 'eph', 'said', '.', '\"', 'the', 'man', 'must', 'be', 'his', 'fellow', 'soldier', ',', 'because', 'he', 'wore', 'a', 'mask', '.'], ['all', '(', 'most', 'commonly', ')', 'portable', 'tvs', 'are', 'portable', 'devices', '.', 'even', 'cd', '-', 'rom', 'is', 'the', 'most', 'common', 'portable', 'portable', 'media', 'player', ',', 'and', 'practically', 'all', 'devices', '(', 'with', 'subtitles', ')', 'can', 'play', 'the', 'audio', 'volume', '.'], ['during', 'this', 'work', 'it', 'was', 'found', 'that', 'there', 'was', 'a', 'lack', 'of', 'space', ',', 'and', 'the', 'two', 'completed', 'buildings', 'were', '(', 'to', 'self', '-', 'maintain', 'themselves', ')', 'left', 'empty', 'until', 'autumn', 'of', '1967', 'when', 'the', 'first', 'extension', 'was', 'completed', '.'], ['bluenote', 'reissues', 'stated', 'that', '\"', 'mathis', \"'\", 's', 'band', 'with', 'its', 'styles', 'mostly', 'packed', 'with', '[', ']', 'just', 'a', 'few', 'intro', '[', 'songs', ']', 'were', 're', '-', 'released', 'in', '2009', 'on', 'vinyl', 'and', 'cassettes', '\"', '.'], ['the', 'plan', 'on', 'champs', 'elysees', 'to', 'form', 'the', 'stage', 'for', 'shamshiqavi', 'shrine', 'for', 'the', 'nativity', 'of', 'jesus', 'and', 'the', 'in', '1999', 'establishment', 'of', 'the', '\"', 'society', '\"', 'for', '\"', 'a', 'historical', 'restoration', '\"', '.'], ['henry', 'james', 'finalists', ':', '2005', '(', 'leconte', ')', ',', '2005', 'and', '2005', 'raymond', 'arthur', 'finalist', ':', '2009', 'stephen', 'cage', 'finalists', ',', 'and', '\"', 'back', 'to', 'your', 'parents', '\"', ',', 'which', 'opens', 'and', 'closes', '(', 'luckett', ')', '.'], ['they', 'have', 'three', 'children', ':', 'martin', 'maas', '(', 'member', 'of', 'the', 'parliament', ')', 'and', 'charlotte', '(', 'mp', ',', 'since', '2006', ')', '.', 'vibe', 'are', 'gyldendal', '(', 'mother', 'tongue', ')', 'and', 'christian', '(', 'father', 'tongue', ')', '.'], ['military', 'preserved', 'courthouse', 'remains', 'today', '.', 'during', 'his', 'term', 'of', 'office', ',', 'maj', '.', 'andrew', 'w', '.', 'davis', 'became', 'a', 'lieutenant', ',', 'stanislaus', 'county', 'light', 'artillery', 'battery', '(', 'volunteers', ')', ',', '22nd', 'virginia', 'regiment', 'and', 'highland', 'light', 'artillery', '.'], ['groups', 'of', 'girls', 'and', 'boys', 'participate', 'in', 'poetry', 'readings', 'and', 'the', 'william', 'cole', 'poetry', 'workshops', '.', 'in', 'both', 'december', 'and', 'january', '2010', ',', 'bennie', 'goldin', ',', 'taryn', 'smith', ',', 'adlai', 'stevenson', 'and', 'martha', 'wayne', 'joined', ';'], ['even', 'the', 'king', 'stopped', 'being', 'anti', '-', 'pagan', 'in', 'part', '1', 'and', '2', ',', 'but', 'this', 'was', 'abandoned', ';', 'rather', 'many', 'tax', 'reductions', 'were', 'then', 'made', '.', 'he', 'permanently', 'ceased', 'to', 'be', 'mayor', '(', 'or', 'juris', '-', 'consul', ')', '.'], ['(', 'williams', ',', 'p', '.', '8', ',', '12', ')', 'was', 'published', 'in', 'his', 'garrick', ',', 'famous', 'poems', '(', '1791', ')', 'as', 'a', '(', 'traditional', ')', 'lullaby', '(', 'some', 'of', 'which', 'are', 'sung', 'live', ',', 'sometimes', 'as', 'recorded', ')', '.'], ['law', 'school', '.', '\"', 'looking', 'at', 'the', 'jewish', 'question', ';', 'a', 'weekly', 'report', 'on', 'judaism', 'and', 'modern', 'jewish', 'thought', '\"', '.', 'the', 'hillel', 'rebroadcast', '.', 'september', '1969', '.', '(', 'revised', 'edition', ')', 'law', 'school', 'journal', '.'], ['past', 'projects', 'include', 'midnight', 'star', 'trek', ',', 'star', 'trek', ':', 'project', 'runway', ',', 'and', 'oshawa', '.', 'his', 'stage', 'credits', 'include', 'mystery', 'train', ':', 'a', 'christian', 'telenovela', ',', 'music', 'from', 'the', 'black', 'hills', ':', 'the', 'musical', 'and', 'angels', '.'], ['in', 'bruce', \"'\", 's', 'childhood', ',', 'she', 'was', 'passively', 'sharing', 'his', 'support', 'for', 'her', '.', '\"', 'ricky', '\"', 'willis', '-', '\"', 'vice', 'floor', 'clerk', '\"', '.', 'assistant', '(', 'case', 'is', 'special', ')', '.', 'frank', '-', '\"', 'harry', '\"', '.'], ['we', 'walk', 'in', 'and', 'ghost', '-', 'boy', 'pauses', ',', 'then', 'places', 'a', 'hand', 'over', 'his', 'mouth', 'to', 'make', 'a', 'hexathemphy', ',', 'a', 'piercing', 'sound', ',', 'like', 'a', 'thumb', 'jabbing', 'into', 'a', 'dinosaur', 'gums', '.'], ['most', 'of', 'them', 'were', 'researchers', 'in', 'the', 'church', 'and', 'had', 'significant', 'contributions', 'there', '.', '{', 'do', 'you', 'have', 'any', 'relations', 'to', 'the', 'church', 'that', 'might', 'relate', 'to', 'mary', ',', 'john', 'or', 'jesus', '?', '}', 'bible', 'family', 'tree', 'and', 'stories', '.'], ['one', ',', 'no', '.', '6', ',', 'was', 'exhibited', 'at', 'knapping', ',', 'dorset', 'on', '15', 'july', '1855', '.', 'long', 'letters', 'with', '/', 'without', '.', 'edinburgh', ':', 'james', 'murray', ',', 'rev', '.', 'ed', '.', ',', 'edinburgh', ',', 'august', '1844', ';'], ['marchetti', 'maintained', 'that', 'the', 'last', 'part', 'was', 'not', 'true', ',', 'later', 'saying', 'that', 'all', 'she', 'haduded', 'to', 'was', 'that', 'a', 'police', 'officer', 'was', 'coming', 'to', 'arrest', 'her', '(', 'although', 'if', 'so', 'it', 'was', 'probably', 'not', 'true', ')', '.'], ['washington', '(', 'we', 'ought', 'to', 'be', 'whole', ')', ';', 'and', 'the', 'resignations', 'of', 'mgr', '.', 'william', 'miller', ',', 'signed', 'by', 'andrew', 'jackson', ';', 'james', 'buchanan', ';', 'john', 'c', '.', 'calhoun', ',', 'g', '.', 'w', '.', 'murray', 'iii', ';'], ['was', 'bit', 'and', 'bit', ',', 'and', 'bit', 'and', 'bit', ',', 'and', 'bit', 'in', 'order', 'to', 'get', 'in', '.', '\"', 'where', '?', '\"', 'henty', 'wondered', 'where', 'it', 'all', 'was', ';', '\"', 'in', 'the', 'shadows', '.', 'i', 'was', 'too', 'dead', '.'], ['how', 'stupid', 'of', 'me', 'if', 'it', 'were', '.', 'it', 'surely', 'won', \"'\", 't', 'be', 'allowed', 'to', 'open', 'and', 'close', ',', 'can', 'it', 'not', '?', 'hand', 'over', 'the', 'open', 'door', 'to', 'reveal', 'a', 'way', 'upwards', 'into', 'the', 'spellbound', 'worlds', '.'], ['though', 'there', 'really', 'was', 'no', 'hope', 'of', 'circulating', 'it', 'into', 'the', 'headquarters', 'of', 'the', 'ico', 'at', 'large', 'in', 'toronto', ',', 'canada', ',', 'i', 'knew', 'of', 'every', 'page', 'on', 'which', '\"', 'russia', 'can', 'only', 'remain', '\"', 'if', 'ever', 'found', '.'], ['includes', 'this', 'album', '.', 'the', 'previous', 'year', 'parks', 'had', 'two', 'programs', ',', 'lawns', '!', '&', 'places', '!', '.', 'after', 'world', 'war', 'ii', 'this', 'program', 'was', 'complemented', 'by', 'the', 'larger', 'program', ',', 'parks', '&', 'places', '!', '!', '!', '.'], ['it', 'features', 'vocals', 'and', 'keyboard', 'accompaniment', 'from', 'bass', 'players', '.', 'the', 'main', 'character', ',', 'rich', ',', 'was', 'played', 'by', 'his', 'now', 'deceased', 'brother', '.', 'mick', 'neal', ',', 'jeff', 'beck', ',', 'and', 'rod', 'stewart', 'are', 'members', 'of', 'the', 'magnetics', '.'], ['bishop', ',', 'joseph', 'statnick', ';', 'jones', ',', 'david', ';', 'crabtree', ',', 'robert', ';', 'jenkins', ',', 'h', '.', 'william', ';', 'downes', ',', 'nicholas', ';', 'mason', ',', 'james', '.', '\"', 'lo', ',', 'lo', ',', 'lo', ',', 'lo', '\"', '.'], ['responsible', 'for', 'the', 'production', 'of', 'pretty', 'eyes', '.', 'jonathan', 'findlay', '-', 'writer', ';', 'producer', 'kate', 'boye', '-', 'illuminators', ',', 'conducting', '[', 'vocals', ']', ';', 'additional', 'guitar', '\"', '(', 'light', 'a', 'yellow', 'sun', ')', '\"', '.'], ['-', 'montgomery', 'ward', ',', 'as', '\"', 'me', ',', '\"', '(', 'engelbart', ',', '1959', ')', '.', 'it', 'never', 'happened', 'to', 'me', ',', '-', 'montgomery', 'ward', '/', 'louella', 'white', ',', '(', 'princess', 'rose', 'theatre', ',', '1977', ')', '.'], ['pit', 'car', 'museum', 'and', 'website', 'states', 'that', 'all', 'eighteen', 'pit', 'cart', 'drivers', 'are', 'buried', 'in', 'the', 'park', ',', 'have', 'over', '27', 'years', 'of', 'experience', 'driving', ',', 'including', 'the', 'first', 'ever', '4x4', 'in', 'the', 'world', ',', 'and', 'died', '.'], ['upon', 'his', 'accession', ',', 'wulffried', 'was', 'acclaimed', 'as', 'emperor', ',', 'and', 'became', 'the', 'first', 'australasian', 'stoic', 'master', 'of', '(', 'upper', ')', 'the', 'morvern', 'dynasty', ',', 'continuing', 'his', 'previous', 'association', 'as', 'imperial', 'ruler', '.'], ['well', ',', 'now', 'she', \"'\", 's', 'sleeping', 'with', 'a', 'white', 'guy', 'and', 'other', 'girls', '!', 'or', 'is', 'she', 'a', 'gullible', 'white', 'girl', 'who', \"'\", 's', 'worried', 'that', 'someone', 'else', 'came', 'after', 'her', ',', 'and', 'the', 'whole', 'deal', '?'], ['ltd', '(', 'electrical', 'engineering', ')', 'ssk', 'engineering', 'ltd', '(', 'machine', 'control', '/', 'automation', 'engineering', ')', 'ncit', 'centre', '(', 'ncit', 'centre', ')', 'football', ':', 'two', 'famous', 'santa', 'fe', 'football', 'teams', 'are', 'oliguim', 'and', 'sv', '.'], ['-', 'is', 'or', \"'\", 'are', '.', \"'\", \"'\", '*', ':', 'north', 'carolina', ':', 'tim', 'sexhall', 'from', 'nc', 'and', 'nc', ';', 'south', 'florida', ':', 'john', 'merriam', '(', '1', \"'\", ',', '2', \"'\", ',', '5', \"'\", ')', 'from', 'nc', ';'], ['danny', ',', 'a', 'reporter', ',', 'comes', 'across', 'barney', '(', 'drew', 'barrymore', ')', 'a', 'young', 'man', ',', 'curious', '\"', 'by', 'the', 'sound', 'of', 'music', '\"', 'about', 'a', 'news', 'piece', 'in', 'a', 'local', 'newspaper', 'that', 'he', 'calls', '\"', 'freedom', '\"', '.'], ['transito', 'rosso', '(', 'guitar', 'sonatas', ')', 'for', 'solo', 'cello', '/', 'piano', '1999', '-', '2002', 'for', 'violin', ',', 'cello', ',', 'violin', '/', 'second', 'cello', '/', 'piano', '(', '2002', ')', '\"', 'life', 'and', 'times', '\"', 'cd', '-', 'vols', '.'], ['and', 'she', 'limps', 'about', ',', ',', 'almost', 'as', 'if', 'talking', 'about', 'the', 'city', ',', 'and', 'she', 'tries', 'to', 'make', 'sense', ',', 'new', 'and', 'old', 'and', '.', '.', '.', 'she', 'seems', 'finally', 'to', 'have', 'found', 'her', 'lampshades', '.'], ['hitman', 'a', '.', 'live', '2009', '-', 'hitman', 'a', '.', 'live', 'live', '2009', '-', 'no', 'storks', ',', 'no', 'kipp', ',', 'do', 'or', 'fly', '-', 'up', ',', 'no', 'rip', 'tracks', '!', ',', 'no', 'hacksaw', '!'], ['after', 'winning', 'a', 'brit', 'award', ',', 'his', 'additional', 'vocals', 'were', 'later', 'featured', 'on', 'live', 'again', ',', 'again', 'and', 'again', ',', 'when', 'the', 'video', 'was', 'filmed', ',', 'and', 'later', 'on', 'the', 'cover', '\"', 'here', ',', 'here', '\"', 'by', 'the', 'doors', '.'], ['2016', '.', 'music', 'for', 'sunday', '.', '2017', '.', 'the', 'fox', ',', 'the', 'roots', ':', 'strangers', 'in', 'the', 'street', '.', '2018', '.', 'music', 'for', 'sunday', ':', 'tribute', 'concerts', ',', 'tap', 'tap', '!', ':', 'tribute', 'recordings', '...', '/', 'roundtree', 'records', '.'], ['tv', 'specials', '(', 'starring', 'welles', 'and', 'lynn', 'rennie', ')', ':', 'here', 'are', 'boys', 'looking', 'for', 'a', 'rediffusion', ';', 'his', 'debut', 'album', '\"', 'strangers', '\"', '(', '1966', ')', 'with', 'david', 'a', '.', 'fields', ';', 'l', '.', 'a', '.'], ['the', 'great', 'vanities', ',', '(', 'short', 'story', ')', '1988', '.', 'chicago', ':', 'dark', 'sun', 'press', ',', '1988', '.', 'trip', 'to', 'the', 'moon', ',', '(', 'digital', ')', ',', '1993', '.', 'by', 'the', 'clock', ':', 'a', 'novel', ',', '2nd', 'ed', '.'], ['however', ',', 'the', 'ebu', 'servers', 'issue', 'catalogues', 'and', 'submit', 'wikipedia', 'articles', 'and', 'interview', 'forms', 'the', 'hall', 'allows', 'for', 'ballets', ',', 'recitals', 'and', 'concerts', 'with', 'all', 'of', 'the', 'facilities', 'open', 'to', 'everyone', 'on', 'the', 'ground', 'floor', '.'], ['during', 'kyrgyzstan', \"'\", 's', 'appearance', 'at', 'the', 'official', '2009', '\"', 'imarov', 'summit', '\"', 'held', 'in', 'moscow', ',', 'the', 'present', 'attendee', 'included', 'new', 'germany', 'president', 'uzbekistan', ',', 'former', 'minister', 'of', 'law', 'and', 'justice', 'and', 'an', 'international', 'diplomat', '.'], ['michael', 'morgan', 'on', '\"', 'roots', '(', 'mill', 'river', 'story', ')', '\"', '(', 'smallband', 'english', ')', '(', 'edie', 'hedlund', ')', ';', 'jason', 'wood', '(', 'st', '.', 'james', 'orchestra', ')', '(', 'a', 'national', 'youth', 'music', 'ensemble', ')', ';'], ['it', 'is', ',', 'even', 'when', 'it', 'never', 'is', '.', '\"', '\"', 'best', 'actress', ',', 'best', 'producer', '.', 'this', 'is', 'panettiere', ',', 'can', 'you', 'hear', 'me', '?', '\"', 'listened', 'hard', '.', 'had', 'listened', 'for', 'the', 'last', 'ten', 'minutes', '.'], ['blackwood', 'was', 'a', 'civilized', 'town', ',', 'with', 'a', 'few', 'saloons', ',', 'and', 'a', 'couple', 'of', 'am', 'and', 'fm', 'radio', 'stations', '.', 'and', 'there', 'was', 'nothing', 'wrong', 'with', 'jim', 'norton', 'and', 'his', 'boys', '.', 'jim', 'norton', 'was', 'right', '.'], ['i', 'thought', 'you', 'might', 'like', 'to', 'me', 'slam', '.', \"'\", \"'\", 'actually', ',', 'yes', '.', 'but', 'if', 'he', 'is', 'errol', ',', 'excuse', 'him', ',', 'a', 'disgusting', ',', 'awful', ',', 'awful', 'loser', ',', 'because', 'i', 'hate', 'calling', 'that', 'name', '.'], ['his', 'small', ',', '\"', 'anonymous', '\"', 'group', 'was', 'listed', 'on', 'social', '-', 'networking', 'websites', 'including', 'the', 'chicago', 'tribune', '.', 'he', 'identified', 'hackers', 'hiding', 'from', '\"', 'first', '-', 'person', 'workstations', '\"', 'like', 'the', 'atari', 'high', 'school', 'football', '.'], ['2003', ':', 'interparty', 'fellowship', 'and', 'chinese', 'chamber', 'fellowship', '.', '2003', ':', 'chinese', 'chamber', 'fellowship', 'and', 'the', 'dialogue', 'over', 'images', '.', '2003', ':', 'hear', 'the', 'people', ':', 'new', 'perspectives', 'for', 'the', 'young', 'chinese', 'filmmakers', 'by', 'zhang', 'junzi', '.'], ['jim', '(', 'putney', ')', 'is', 'confined', 'within', 'a', 'haunted', 'tower', ',', 'in', 'a', 'room', 'made', 'with', 'dust', '-', 'coated', 'iron', 'bands', ',', 'in', 'which', 'the', 'ghost', 'realizes', 'jim', \"'\", 's', 'attempts', 'to', 'come', 'without', 'him', 'and', 'kills', 'him', '.'], ['we', 'stroll', 'and', 'sing', 'and', 'dance', 'and', 'play', 'together', ',', 'go', 'to', 'the', 'castle', ',', 'and', 'watch', 'day', 'and', 'night', '.', 'willcox', ',', 'like', 'brother', ',', 'like', 'father', ',', 'also', 'were', 'watching', 'for', 'us', ',', 'i', 'think', '.'], ['one', ',', 'born', 'after', 'a', 'drop', 'of', 'the', 'spell', ',', 'and', 'the', 'other', ',', 'the', 'man', 'she', 'had', 'met', 'as', 'her', 'brother', '-', 'in', '-', 'law', ',', 'had', 'now', 'taken', 'a', 'vial', 'full', 'of', 'blood', 'from', 'morgana', 'himself', '.'], ['there', 'are', 'scenarios', 'in', 'which', 'echelons', 'would', 'seek', 'treatment', ',', 'but', 'the', 'writers', 'have', 'them', 'refuse', '.', 'a', 'fertility', 'clinic', 'where', 'a', 'young', 'guy', 'tells', 'ozma', 'that', 'he', 'and', 'debbie', 'vader', 'are', 'both', 'alcoholics', '.'], ['the', 'imperial', 'airways', 'collection', 'went', 'to', 'the', 'national', 'maritime', 'museum', 'of', 'canada', 'in', '1963', ',', 'in', 'montreal', ',', 'canada', '.', 'dhc', '-', '6b', ',', 'royal', 'canadian', 'navy', ',', 'open', 'cockpit', 'and', 'four', 'cup', '-', 'shaped', 'rigs', '.'], ['a', 'phylum', 'of', 'black', 'singers', 'was', 'formed', 'by', 'mark', 'fink', ',', 'for', 'american', 'repertory', 'theater', '(', 'ahmf', ')', 'with', 'the', 'musical', 'gambit', 'shabazz', ',', 'which', 'featured', 'composer', 'and', 'singer', 'roger', 'williams', '.'], ['sometimes', 'the', 'definition', 'even', 'applies', 'to', 'a', 'person', 'who', 'has', 'married', '.', 'it', 'was', 'expanded', 'upon', 'by', 'chapter', 'iii', 'of', '\"', 'open', 'rights', 'interpretation', '\"', '.', 'they', 'are', 'listed', 'below', '.', 'a', 'timeline', 'of', 'the', 'original', 'edition', 'is', 'provided', '.'], ['volker', 'verlag', '(', 'sonntag', 'university', 'of', 'cologne', ')', 'edited', 'by', 'puch', 'de', 'plas', ',', 'hugo', 'van', 'uy', 'and', 'peter', 'andriessen', '.', 'gesellschaft', '(', '1849', '-', '1930', ')', '.', 'i', ',', 'vol', '.'], ['did', 'all', 'the', 'brothers', 'of', 'dorwin', 'meet', 'what', 'they', 'saw', '?', 'jon', 'saw', 'gold', 'on', 'jon', 'snow', \"'\", 's', 'shoulders', '.', 'your', 'sister', 'was', 'my', 'companion', 'by', 'years', '.', 'she', 'is', 'my', 'dearest', 'companion', 'yet', ',', 'jon', '.'], ['pennsylvania', ':', 'little', 'brownlow', '(', 'descant', 're', '-', 'title', ')', ':', 'philadelphia', 'press', '.', 'philadelphia', ',', 'pa', '.', '(', 'original', 're', '-', 'title', 'was', '\"', 'a', '&', 'r', '\"', ')', 'buffalo', ',', 'ny', ':', 'grove', 'press', '.'], ['(', 'morning', 'show', 'host', ':', 'jonah', 'hill', ',', 'highlighter', ':', 'randy', 'rich', ',', 'caldure', ',', 'riceland', 'records', ',', 'morning', 'personality', ':', 'mr', '.', 'king', ',', 'late', 'night', 'personality', ':', 'dr', '.', 'dave', 'gott', ')', '.'], [\"'\", 'richard', 'davis', ',', \"'\", 'he', 'said', ',', 'responding', 'to', 'the', 'fact', 'that', 'he', 'had', 'knocked', 'a', 'little', 'too', 'hard', '.', \"'\", 'yes', ',', \"'\", 'cawley', 'said', '.', \"'\", 'the', 'front', 'door', 'was', 'still', 'open', ',', 'right', '?', \"'\"], ['sir', 'james', 'ambleside', ',', 'the', 'scottish', 'hunter', ',', 'wed', 'the', 'american', 'horsewoman', 'sarah', 'gosse', 'to', 'father', 'and', 'sister', 'she', 'was', 'his', 'wife', 'elizabeth', 'and', 'she', 'had', 'affection', 'for', 'the', 'wealthy', 'and', 'the', 'couple', 'made', 'profits', '.'], ['major', 'henry', 'edwin', 'benton', 'clark', ',', 'cbe', ',', 'mc', ',', ',', 'recipient', 'of', 'the', 'military', 'order', 'of', 'the', 'lion', 'of', 'egypt', '(', '1919', ')', '.', 'admiral', 'sir', 'edwin', 'lawrence', 'george', 'fraser', ',', 'cb', ',', ',', 'dso', ',', 'dfc', '.'], ['he', 'also', 'made', 'a', 'guest', 'appearance', 'on', 'the', 'show', 'new', 'york', 'and', 'the', 'big', '10', 'as', 'john', 'bellows', ';', 'elaine', 'strickch', ',', 'time', 'machine', ';', 'vincent', 'price', ',', 'city', 'of', 'angels', 'as', 'carmella', 'carmona', ';'], ['these', 'have', 'been', 'expanded', 'some', 'to', 'include', 'written', 'history', ',', 'oral', 'history', ',', 'ancient', 'oral', 'histories', 'including', 'moa', ',', 'moa', 'oral', 'histories', ',', 'and', 'oral', 'histories', 'relating', 'to', 'austronesian', 'cultures', 'including', 'urban', 'and', 'rural', 'oral', 'histories', '.'], ['\"', 'let', 'the', 'lords', 'decide', '.', 'let', 'him', 'choose', 'women', '.', 'let', 'him', 'choose', 'women', 'to', 'serve', 'as', 'the', 'lord', 'commands', '.', '\"', 'softly', 'flickering', 'candles', 'warded', 'off', 'the', 'moon', 'as', 'the', 'lords', 'lifted', 'their', 'swords', 'and', 'shields', '.'], ['he', 'studied', 'at', 'the', 'hampshire', 'national', 'mountain', 'bike', 'junior', 'college', '.', 'he', 'is', 'an', 'england', 'youth', 'international', 'competing', 'for', 'england', 'at', 'the', '2012', 'uci', 'cycling', 'world', 'championships', 'aged', 'nineteen', '.', 'he', 'was', 'educated', 'at', 'grove', 'vale', 'school', 'in', 'hampshire', '.'], ['in', 'the', 'middle', 'of', 'his', 'lap', 'was', 'a', 'group', 'of', 'five', 'pawns', '.', 'on', 'his', 'back', ',', 'there', 'was', '\"', 'holy', '\"', ',', 'then', '\"', 'holy', ',', 'holy', '\"', '.', 'why', 'did', 'he', 'play', 'with', 'five', 'pawns', '?'], ['as', 'the', 'man', 'powered', 'the', 'autoharp', ',', 'he', 'weakly', 'said', ',', '\"', 'i', 'was', 'floating', 'the', 'c', '-', '130', 'at', 'the', 'skygulfs', 'in', 'a', 'crash', 'on', 'dey', '.', '\"', 'he', 'belched', 'behind', 'her', '.'], ['(', '§', '12', ')', 'in', 'all', 'states', 'assunta', 'is', 'a', 'lawyer', 'who', 'cannot', 'go', 'anywhere', 'until', 'the', 'first', 'defendant', 'has', 'won', 'the', 'case', ',', 'and', 'they', 'cannot', 'go', 'anywhere', 'unless', 'she', 'does', 'and', 'they', 'all', 'are', 'guilty', '.'], ['in', 'the', 'room', 'behind', 'the', 'children', \"'\", 's', 'room', ',', 'a', 'heavy', 'steel', 'door', 'was', 'quietly', 'opened', '.', 'dorothy', 'froze', ',', 'but', 'not', 'jumped', '.', 'she', 'had', 'grown', 'up', ',', 'by', 'birth', ';', 'the', 'nightmare', 'had', 'only', 'started', 'once', '.'], ['\"', 'and', 'you', 'intend', 'to', 'play', 'this', 'with', 'me', '?', 'the', 'abilities', 'that', 'i', 'have', 'were', 'not', 'for', 'you', ',', 'briggs', '.', '\"', 'briggs', 'and', 'briggs', '.', 'likely', 'guessing', ',', 'but', 'not', 'in', 'the', 'wording', '.', 'good', 'enough', '.'], ['but', 'they', 'realize', 'now', ',', 'as', 'they', 'grow', 'up', ',', 'that', 'they', 'do', 'not', 'have', 'a', 'heart', ',', 'not', 'like', 'any', 'adult', 'child', 'on', 'the', 'planet', 'or', 'a', 'robot', ',', 'and', 'that', 'i', 'cannot', 'save', 'them', 'from', 'falsehood', '.'], ['eta', '8', 'also', 'documented', 'a', 'jy6', 'temperature', 'variation', 'and', 'set', 'it', 'at', 'the', 'jy3', 'temperature', 'range', '+', '1', '.', 'eta', '7', 'documented', 'the', 'highest', 'temperature', 'variation', 'during', 'about', '12', 'months', '(', 'over', '130', 'days', ')', '.'], ['i', 'can', 'barely', 'give', 'it', 'a', 'thought', 'as', 'i', 'make', 'a', 'date', 'with', 'one', 'of', 'the', 'hottest', 'women', 'in', 'town', 'for', '$', '90', 'a', 'day', '.', 'sucks', 'to', 'know', '!', 'all', 'around', '.', 'and', 'anyway', ',', 'who', 'is', 'she', '?'], ['the', 'mental', 'or', 'emotional', 'trauma', 'of', 'climbing', 'the', 'stairs', 'like', 'teenage', 'girls', 'too', 'timid', 'to', 'have', 'any', 'imagination', ';', 'no', 'teenage', 'boyfriends', ';', 'no', 'teenage', 'girls', 'wandering', 'the', 'streets', 'alone', 'or', 'attempting', 'to', 'play', 'along', 'with', 'strangers', ';'], ['2002', ',', 'critical', 'reflections', 'from', 'writers', '(', 'poetry', 'review', ')', ':', 'children', 'should', 'write', 'poetry', 'asking', 'them', 'to', 'find', 'their', 'real', 'form', '.', '2001', ',', 'prairie', 'schooner', 'poetry', 'review', '(', 'chicago', ',', 'illinois', ')', ':', 'great', 'verse', 'for', 'children', '.'], ['justin', 'clarke', 'as', 'peter', 'q', '.', 'taylor', 'alice', 'poorle', 'as', 'jemima', 'sawyer', 'anthony', 'lampton', 'as', 'dave', 'swinbank', 'holly', 'walker', 'as', 'recurring', 'character', 'grace', '(', 'the', 'episode', 'served', 'as', 'inspiration', 'for', 'a', 'film', 'adaptation', ')', '.'], ['he', 'collaborated', 'several', 'years', 'later', 'with', 'hilary', 'mak', ',', 'a', 'figurative', 'painter', 'and', 'decorator', 'working', 'on', '\"', 'all', 'the', 'war', '\"', '.', 'he', 'had', 'recorded', 'vocals', 'for', '\"', 'pappy', '\"', 'on', 'their', 'other', 'two', 'solo', 'albums', '.'], ['slowly', ',', 'the', 'malachai', 'took', 'some', 'careful', 'steps', 'forward', '.', 'he', 'turned', 'around', '.', \"'\", 'he', 'does', 'it', 'when', 'he', 'comes', ',', \"'\", 'he', 'said', 'to', 'the', 'boy', '.', \"'\", 'yeah', \"'\", 'he', 'nodded', ',', 'standing', ',', 'standing', '.'], ['my', 'name', 'is', 'tonia', ',', 'it', 'is', 'for', 'the', 'sake', 'of', 'our', 'children', '.', 'but', 'you', 'are', 'beautiful', '.', 'i', 'think', 'you', 'are', 'perfect', '.', 'about', 'five', 'foot', 'ten', ',', 'thick', 'dark', 'hair', 'and', 'a', 'good', 'little', 'face', '.'], ['\"', 'but', 'are', 'those', 'the', 'time', 'limits', 'of', '...', '\"', 'chapter', '32', ':', 'falling', 'from', 'the', 'rafters', ',', 'part', '1', '\"', 'arwen', '?', '\"', 'arwen', 'turned', 'to', 'amira', 'and', 'she', 'offered', 'her', 'both', 'good', 'hands', '.'], ['at', 'last', 'she', 'and', 'molly', 'had', 'been', 'adopted', 'by', 'parents', 'at', 'school', ',', 'and', 'given', 'their', 'own', 'hush', '-', 'hush', 'house', '.', 'i', 'offered', 'to', 'help', 'her', 'now', '.', 'i', 'want', 'to', 'comfort', 'her', ',', 'but', 'i', 'will', 'not', '.'], ['some', 'of', 'us', 'do', 'a', 'dance', 'or', 'something', '.', 'one', 'by', 'one', ',', 'we', 'go', 'out', 'drinking', '.', 'and', 'some', 'of', 'us', 'drink', '.', 'since', 'then', ',', 'i', 'was', 'more', 'human', ',', 'but', 'my', 'first', 'time', 'was', 'with', 'people', '.'], ['and', 'then', 'my', 'feline', 'body', 'came', 'down', 'on', 'her', 'in', 'another', 'second', '-', 'a', 'bright', 'inferno', '.', 'then', 'the', 'fire', 'faded', '.', 'and', 'it', 'did', 'not', 'appear', 'in', 'my', 'sight', ',', 'however', '-', 'not', 'even', 'when', 'i', 'screamed', '.'], ['the', 'two', 'demons', 'who', 'were', 'no', 'longer', 'behind', 'me', ',', 'but', 'all', 'around', 'me', 'were', 'hissing', 'hissed', '.', 'oh', 'god', ',', 'i', 'thought', '.', 'they', 'were', 'nudging', 'my', 'leg', 'in', 'a', 'move', 'that', 'had', 'never', 'occurred', 'before', '.'], ['yes', ',', 'how', 'i', 'found', 'out', '.', 'you', 'were', 'the', 'co', '-', 'ceo', 'of', 'taiwan', 'oil', 'corporation', ',', 'the', 'other', 'large', 'in', 'taiwan', '.', 'now', 'here', 'with', 'a', 'new', 'york', 'business', 'degree', '.', 'your', 'first', 'boyfriend', 'was', 'your', 'dad', '.'], ['studies', 'in', 'linguistics', 'and', 'literature', '.', 'he', 'has', 'successfully', 'screened', 'films', 'at', 'venice', 'and', 'new', 'york', 'at', 'the', 'cannes', 'film', 'festival', '.', 'jenb', '.', '975', '.', '\"', 'a', 'study', 'in', 'catholic', 'secondary', 'education', '\"', '.', 'jstor', '.'], ['yet', ',', 'milind', ',', 'who', 'went', 'to', 'jail', 'and', 'claimed', 'it', 'had', 'happened', 'for', 'no', 'reason', ',', 'got', 'involved', 'in', 'a', 'car', 'accident', '.', 'after', 'all', 'they', 'never', 'got', 'back', 'to', 'the', 'village', ',', 'parvati', 'or', 'returned', '.'], ['(', 'michael', 'jackson', ',', 'marco', 'ruggero', ',', 'sting', ',', 'john', 'cage', ',', 'arlo', 'guthrie', ',', 'pedal', 'steel', ',', 'whitney', 'houston', ',', 'alice', 'cooper', ',', 'and', 'david', 'weitzman', 'have', 'all', 'influenced', 'his', 'production', 'work', ')', '.'], ['pilot', ':', 'sam', 'davis', '(', 'associate', 'producer', ')', '\"', 'the', 'lost', 'flintstones', '\"', '(', 'associate', 'producer', ')', 'sam', 'davis', 'has', 'co', '-', 'wrote', 'three', 'stories', 'with', 'bill', 'yee', 'and', 'six', 'other', 'feature', 'stories', 'with', 'william', 'james', 'dillon', '.'], ['the', 'absence', 'of', 'non', '-', 'readily', 'available', 'measurement', 'methods', 'or', 'tools', 'to', 'address', 'the', 'hydration', 'hazard', 'also', 'suggests', 'that', 'it', 'exceeded', 'the', 'total', 'value', 'of', 'latency', 'of', 'exposure', ',', 'leading', 'to', 'a', 'reduction', 'of', 'lsbps', '.'], ['alternative', 'spellings', ':', 'tail', '=', 'long', ',', 'head', '=', 'very', 'short', ',', 'ear', '=', 'loose', ',', 'tail', '=', 'loose', 'though', 'it', \"'\", 's', 'not', 'currently', 'in', 'use', ',', 'widely', 'used', '.', 'mouth', '=', 'male', 'name', ',', 'often', 'unknown', '.'], ['all', 'right', ',', 'he', 'was', 'not', 'lying', ',', 'after', 'all', ',', 'about', 'being', 'quiet', '.', 'verence', 'and', 'verence', 'had', 'fun', ',', 'but', 'the', 'silent', 'one', 'was', 'more', ',', 'and', 'he', 'was', 'waiting', 'to', 'learn', '.', 'haha', '!'], ['nonetheless', ',', 'though', ',', 'they', 'also', 'continue', 'to', 'draw', 'attention', 'to', 'classical', 'music', 'and', 'other', 'musical', 'fare', 'in', 'their', 'work', '.', 'meanwhile', ',', 'calligraphy', 'art', 'continues', 'to', 'engage', 'fans', 'who', 'are', 'creating', 'their', 'artwork', 'in', 'a', 'heist', '.'], ['he', 'has', 'dedicated', 'himself', 'to', 'arranging', 'and', 'organizing', 'exhibitions', 'that', 'could', 'have', 'an', 'illustrious', ',', 'productive', 'career', ',', 'many', 'exhibitions', 'and', 'several', 'awards', 'in', 'the', 'same', 'year', '(', 'although', 'it', 'is', 'always', 'more', 'prestigious', 'than', 'an', 'art', 'exhibition', ')', '.'], ['although', 'the', 'majority', 'and', 'the', 'majority', 'agreed', 'with', 'the', 'majority', 'that', 'justification', 'could', 'not', 'be', 'dismissed', ',', 'it', 'was', 'not', 'true', '-', 'simply', 'correct', '.', 'the', 'validity', 'of', 'that', 'justification', 'was', 'then', 'not', 'an', 'inherent', 'test', 'for', 'the', 'judgment', '.'], ['\"', 'mary', 'knows', '\"', 'by', 'prince', '.', '\"', 'nowhere', '\"', 'by', 'dead', 'or', 'alive', '.', 'concert', 'film', 'of', '\"', 'high', 'and', 'low', ',', 'something', 'familiar', '\"', ',', 'under', 'the', 'track', 'listing', '.', 'angus', 'campbell', '-', 'dir', '.', 'darkchild', '.'], ['at', 'the', 'same', 'time', 'heinlein', 'did', 'not', 'feel', 'on', 'the', 'farming', 'side', ',', 'and', 'wanted', 'to', 'give', 'up', 'farming', 'and', 'find', 'a', 'better', 'job', ',', 'so', 'he', 'started', '\"', 'building', 'a', 'little', 'church', 'and', 'a', 'school', '\"', '.'], ['again', 'and', 'again', '.', 'garbage', 'washed', 'blocks', 'of', 'each', 'other', '.', 'spraying', 'vacuum', 'cleaner', '.', 'garbage', 'in', 'the', 'convenience', 'store', '.', 'booze', ',', 'piss', ',', 'and', 'trash', '.', 'robert', 'often', 'called', 'when', 'the', 'kids', 'went', 'out', 'to', 'visit', '.'], ['the', 'cold', 'hardy', 'breed', 'began', 'to', 'become', 'very', 'rare', 'in', 'the', 'winter', ',', 'especially', 'in', 'cold', 'climates', '(', 'see', 'history', 'in', 'cider', '-', 'mills', ')', 'and', ',', 'especially', ',', 'on', 'boar', '(', 'called', '\"', 'boars', '\"', ')', '.'], ['ryan', 'headed', 'the', 'med', '-', 'tech', 'team', ',', 'with', 'sam', 'as', 'his', 'cohort', '.', 'ryan', 'knew', 'that', 'the', 'work', 'on', 'recruiting', 'staff', 'for', 'basketball', 'teams', 'had', 'already', 'begun', '.', 'he', 'knew', 'that', 'sam', 'needed', 'him', 'right', 'now', '.'], ['barbara', 'steele', 'dorian', 'gray', '-', 'sam', 'carter', 'mary', 'windsor', '-', 'mrs', '.', 'carter', 'angela', 'bassett', '-', 'rosemary', 'carter', 'angela', 'carter', 'james', 'madison', '-', 'james', 'earl', 'jones', 'brings', 'sam', 'to', 'new', 'york', 'is', 'a', 'privately', 'released', '1928', 'silent', 'romance', 'film', '.'], ['just', 'once', ',', 'as', 'the', 'first', 'elective', '.', 'not', 'that', 'rick', 'would', 'actually', 'enjoy', 'getting', 'back', 'to', 'his', 'house', '.', '-', '-', '-', '-', 'rick', 'was', 'a', 'very', 'frustrating', 'person', ',', 'even', 'when', 'he', 'had', 'nothing', 'of', 'it', '.'], ['(', 'd', '.', '1194', ')', 'a', 'captain', '(', 'leading', 'a', 'rebellion', 'against', 'bishop', 'john', ')', 'tries', 'to', 'escape', 'the', 'magic', 'tower', 'by', 'magic', ',', 'but', 'sees', 'that', 'although', 'the', 'dark', 'tower', 'is', 'in', 'danger', ',', 'he', 'cannot', 'escape', '.'], ['\"', 'uncle', 'drake', 'will', 'join', 'us', '.', 'uncle', 'drake', 'and', 'uncle', 'krit', 'will', 'join', 'us', '.', '\"', 'i', 'really', 'need', 'to', 'leave', 'gage', 'alone', '.', 'leaving', 'my', 'other', 'two', 'friends', 'alone', 'and', 'in', 'my', 'kitchen', ',', 'i', 'left', '.'], ['colin', 'strang', '-', '12', '-', 'year', 'old', 'headmaster', 'of', 'warrington', ',', 'cheshire', 'margaret', '(', 'sue', ')', 'neave', '-', 'wife', 'of', 'general', 'edward', '\"', 'ted', '\"', 'dalton', '(', 'b', '&', 't', ')', 'and', 'he', \"'\", 's', 'mako', '.'], ['mother', ',', 'through', 'sources', 'of', 'light', ',', 'loved', 'him', '.', 'she', 'loves', 'her', 'daughter', ',', 'lizzy', '.', 'in', 'batman', ':', 'beauty', '&', 'the', 'beast', '(', '2012', 'film', ')', ',', 'heston', 'hughes', 'was', 'a', 'superhero', 'blinky', 'smith', '.'], ['but', 'she', 'never', 'came', 'back', ',', 'and', 'neither', 'did', 'you', '.', '\"', '\"', 'sounds', 'like', 'you', 'had', 'fun', 'just', 'grabbing', 'a', 'lift', 'home', '.', '\"', '\"', 'i', \"'\", 'm', 'sorry', '.', 'i', 'just', 'never', 'wanted', 'to', 'see', 'her', 'again', '.'], ['physical', ':', 'physical', 'education', 'is', 'taught', 'by', 'rathler', 'who', 'is', 'an', 'apt', 'marksmanship', 'teacher', ',', 'known', 'as', 'a', 'teacher', 'by', 'susan', 'allen', '(', 'aka', '.', 'susan', 'kelly', 'allen', ')', ',', 'while', 'also', 'instructing', '\"', 'ms', '.', '\"'], ['patients', 'suffering', 'from', 'severe', 'injuries', 'or', 'from', 'serious', 'clinical', 'disorders', 'produce', 'shock', 'by', 'breathing', 'into', 'their', 'mouth', ',', 'using', '\"', 'regular', 'ejection', '\"', 'or', '\"', 'compressed', 'air', '\"', '(', 'gas', ')', 'and', 'are', 'not', 'commonly', 'physically', 'exposed', '.'], ['originally', ',', 'going', 'there', 'would', 'have', 'meant', 'that', 'they', 'escaped', 'without', 'leaving', 'anything', 'behind', ',', 'such', 'as', 'a', 'call', 'out', 'return', 'to', 'paradise', ',', 'but', 'this', 'slowly', 'changed', 'through', 'the', 'course', 'of', 'time', ',', 'eventually', 'changing', 'them', 'into', 'people', '.'], ['when', 'asked', 'questions', ',', 'he', 'was', 'eventually', 'heard', 'talking', 'dirty', 'to', 'students', ',', 'and', 'was', 'nicknamed', 'by', 'the', 'group', 'the', '\"', 'big', 'brother', '\"', 'after', 'his', 'twin', 'brothers', ',', 'bill', 'and', 'ted', ',', 'who', 'maintain', 'and', 'run', 'the', 'school', '.'], ['when', 'georgiana', 'shows', 'it', '(', 'having', 'grown', 'obsessed', 'with', 'the', 'objects', 'and', 'the', 'ultimate', 'potential', 'of', 'the', 'redcoats', ')', 'the', 'disc', 'seems', 'to', 'be', 'leaning', 'inward', ',', 'and', 'it', 'transforms', 'into', 'a', 'human', '-', 'like', 'shape', '.'], ['the', 'album', 'includes', 'the', 'singles', '\"', 'spinal', 'tap', '\"', '(', 'by', 'angelik', ')', 'and', '\"', 'no', 'such', 'song', '\"', '(', 'by', 'la', 'trienne', ')', '.', 'this', 'album', 'was', 'released', 'only', 'as', 'a', '\"', 'loose', 'ends', '\"', 'single', '.'], ['\"', 'la', 'la', 'la', '-', 'many', 'times', '\"', 'is', 'included', 'in', 'the', 'list', 'of', 'rare', 'songs', '.', 'features', 'are', 'clips', 'of', 'neil', 'stansfield', '-', 'produced', 'singles', '\"', 'a', 'wild', 'goose', 'egg', '\"', 'and', '\"', 'from', 'the', 'heart', '\"', '.'], ['the', 'term', 'fixed', 'point', 'measures', 'are', 'derived', 'from', 'pls', 'or', 'qas', '.', 'an', 'equivalent', 'type', 'is', 'the', 'fixed', '-', 'point', 'differential', 'metric', '(', 'called', '\"', 'proportionality', '\"', 'when', 'it', 'keeps', 'on', 'being', 'the', 'mean', 'metric', ')', '.'], ['also', 'in', '1988', ',', 'the', 'cast', 'played', 'father', 'and', 'son', 'at', 'the', 'royal', 'national', 'theatre', 'in', 'london', '.', 'in', '1989', ',', 'what', 'de', 'bryl', 'described', 'as', '\"', 'a', 'scene', 'that', 'actually', 'landed', 'on', 'television', '\"', 'appeared', 'in', 'casualty', ';'], ['\"', 'and', 'then', '...', 'then', 'there', 'was', 'the', 'black', 'man', '.', 'he', 'was', '...', 'different', '.', '\"', 'she', 'paused', '.', '\"', 'he', 'was', 'different', '.', 'a', 'young', 'black', 'man', 'who', 'had', 'the', 'same', 'shaved', 'head', 'as', 'the', 'white', 'man', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iesVmPqo4Id3",
        "outputId": "9ac26df1-0a3e-4729-c802-8707df0ae1b5"
      },
      "source": [
        "\"\"\"NON FARE GIRARE QUANDO UTILIZZIAMO BERT LARGE\"\"\"\n",
        "\n",
        "# Some initializations for the table of self-BLEU\n",
        "print(model_version)# THIS WHOLE CODE BLOCK HAS TO BE REPEATED FOR\n",
        "# THE OTHER BERT MODEL VERSION TOO\n",
        "\n",
        "TITLE_SELF = ['Model', 'Self-BLEU']\n",
        "# values_self_bleu is a list of 2 lists (one with the model name, one for self-BLEU)\n",
        "# each one with 4 elements (first element refers to BERTlarge, second element is for\n",
        "# BERTbase, third element for GPT, fourth element for WT103)\n",
        "# initialization:\n",
        "values_self_bleu = [[0,0,0,0],[0,0,0,0]]\n",
        "values_self_bleu[0] = ['BERTlarge', 'BERTbase', 'GPT', 'WT103']"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert-base-uncased\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J97zdsSvOoel",
        "outputId": "754d73da-f5c1-425a-b9d4-0b09dced41b8"
      },
      "source": [
        "\"\"\"THIS WHOLE CODE BLOCK HAS TO BE REPEATED FOR\n",
        "THE OTHER BERT MODEL VERSION TOO\"\"\"\n",
        "\n",
        "value = self_bleu(bert_sents)\n",
        "print(\"BERT self-BLEU: %.2f\" % (100 * value))\n",
        "#True value: 10,06\n",
        "if model_version == 'bert-base-uncased':\n",
        "  values_self_bleu[1][1] = 100 * value\n",
        "else:\n",
        "  values_self_bleu[1][0] = 100 * value\n",
        "\n",
        "value = self_bleu(openai_sents)\n",
        "print(\"OpenAI self-BLEU: %.2f\" % (100 * value))\n",
        "values_self_bleu[1][2] = 100 * value\n",
        "#True value: 40.02\n",
        "\n",
        "value = self_bleu(wiki1000_data)\n",
        "print(\"Wiki103_train SELF-BLEU: %.2f\" % (100 * value))  \n",
        "#True value: 9.80\n",
        "values_self_bleu[1][3] = 100 * value"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT self-BLEU: 8.49\n",
            "OpenAI self-BLEU: 38.09\n",
            "Wiki103_train SELF-BLEU: 17.42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okYwvVJAt8b-"
      },
      "source": [
        "## Diversity measures: n-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcn28lZ0OVsg"
      },
      "source": [
        "from collections import Counter\n",
        "from nltk.util import ngrams"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPyZjowSOXMf"
      },
      "source": [
        "#help(Counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRQt0X8dO8-L"
      },
      "source": [
        "Class Counter: Dict subclass for counting hashable items.  Sometimes called a bag or multiset.  Elements are stored as dictionary keys and their counts are stored as dictionary values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2rlnaIKObiZ"
      },
      "source": [
        "#help(ngrams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZp15ZojPRyo"
      },
      "source": [
        "Function ngrams: Return the ngrams generated from a sequence of items, as an iterator.\n",
        "For example:\n",
        "    \n",
        "    >>> from nltk.util import ngrams\n",
        "    >>> list(ngrams([1,2,3,4,5], 3))\n",
        "        [(1, 2, 3), (2, 3, 4), (3, 4, 5)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W28FYXammQFg"
      },
      "source": [
        "Other interesting measures are those regarding n-grams. In the following part we define _get_ngram_counts_ , _ref_unique_ngrams_ , _self_unique_ngrams_ ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti4_jlACOjim"
      },
      "source": [
        "def get_ngram_counts(sents, max_n=4):\n",
        "    size2count = {} #empty dictionary\n",
        "    for i in range(1, max_n + 1):\n",
        "        size2count[i] = Counter([n for sent in sents for n in ngrams(sent, i)])\n",
        "    return size2count\n",
        "    # size2count is a dictionary whose keys are \"i\" and for each i a counter is\n",
        "    # applied. For key 1, this counter counts all the occurrences of the 1-grams\n",
        "    # inside the sentences, while for key 2, this counter counts all the occurrences\n",
        "    # of the 2-grams (i.e. two consecutive words) inside the sentences, and \n",
        "    # so on for all the other keys up to max_n\n",
        "\n",
        "def ref_unique_ngrams(preds, refs, max_n=4):\n",
        "    # get # of *distinct* pred ngrams that don't appear in ref\n",
        "    pct_unique = {}\n",
        "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
        "    # builds the ngrams of the generated sentences\n",
        "    ref_ngrams = get_ngram_counts(refs, max_n)\n",
        "    # builds the ngrams of the reference sentences\n",
        "    for i in range(1, max_n + 1):\n",
        "        pred_ngram_counts = set(pred_ngrams[i].keys())\n",
        "        # with the above command we save the keys of the i-th dictionary (w.r.t \n",
        "        # our predicted sentences) inside\n",
        "        # a set.\n",
        "        total = sum(pred_ngrams[i].values())\n",
        "        # with the above command we compute the sum of all the occurrences\n",
        "        # of the grams of length i (i-grams).\n",
        "        ref_ngram_counts = set(ref_ngrams[i].keys())\n",
        "        # with the above command we save the keys of the i-th dictionary (w.r.t \n",
        "        # the reference sentences) inside\n",
        "        # a set.\n",
        "        pct_unique[i] = len(pred_ngram_counts.difference(ref_ngram_counts)) / total\n",
        "        # we measure the proportion of predicted i-grams that don't appear\n",
        "        # in the reference i-grams\n",
        "    return pct_unique\n",
        "        \n",
        "def self_unique_ngrams(preds, max_n=4):\n",
        "    # get # of pred ngrams with count 1\n",
        "    pct_unique = {}\n",
        "    # empty set\n",
        "    pred_ngrams = get_ngram_counts(preds, max_n)\n",
        "    # build the set of dictionaries  where each dictionary contains the \n",
        "    # i-grams and the number of occurrences w.r.t the generated sentences (i.e.\n",
        "    # the predicted sentences).\n",
        "    for i in range(1, max_n + 1):\n",
        "        n_unique = len([k for k, v in pred_ngrams[i].items() if v == 1])\n",
        "        # n_unique is the number of i-grams that are unique \n",
        "        # in the i-th dictionary\n",
        "        total = sum(pred_ngrams[i].values())\n",
        "        pct_unique[i] = n_unique / total\n",
        "        # we measure the proportion of generated i-grams that are unique\n",
        "        # (i.e. which occure just one single time)\n",
        "    return pct_unique"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-SI5itbRmQr"
      },
      "source": [
        "_ref_unique_ngrams_: We use this function to count how many ngrams (in %) appear in preds  (which in our case is bert_sents, our generated sentences) and don't appearin refs (which in our case is wiki_data, our 5000 sentences from wiki103). The results are in table 2 in the paper.\n",
        "\n",
        "_self_unique_ngrams_: We count how many ngrams (in %) appear only 1 time in preds (bert_sents). The results are in table 2 in the paper\n",
        "\n",
        "_get_ngram_counts_: We need this function in order to define the 2 functions above. It creates a set of four dictionaries: each of them contains all the (1 or 2 or 3 or 4) ngrams with the respective number of occurrences.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iosYbHjhsnC",
        "outputId": "83150878-74c6-4cf6-8e1f-ca1093a7ecd3"
      },
      "source": [
        "\"\"\"NON FARE GIRARE QUANDO UTILIZZIAMO BERT LARGE\"\"\"\n",
        "\n",
        "max_n = 4\n",
        "print(model_version)\n",
        "\n",
        "TITLE2 = ['Model', '% unique 2-grams vs WT103', '% unique 3-grams vs WT103', '% unique 4-grams vs WT103']\n",
        "TITLE1 = ['Model', '% unique 2-grams vs Self', '% unique 3-grams vs Self', '% unique 4-grams vs Self']\n",
        "TITLE3 = ['Model', '% unique 2-grams vs TBC', '% unique 3-grams vs TBC', '% unique 4-grams vs TBC']\n",
        "# values_grams_VS_SELF is a list of 4 lists (one with the model name, one for\n",
        "# n=2, one for n=3, one for n=4) each one with 4 elements (first element is \n",
        "# the percentage of unique n-grams of BERTlarge VS ITSELF, second element is the \n",
        "# percentage of unique n-grams of BERTbase VS ITSELF, third element is the percentage\n",
        "# of unique n-grams of GPT VS ITSELF, fourth element is the percentage of unique\n",
        "# n-grams of WT103 VS ITSELF)\n",
        "# initialization:\n",
        "values_grams_VS_SELF = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
        "values_grams_VS_SELF[0] = ['BERTlarge', 'BERTbase', 'GPT', 'WT103']\n",
        "\n",
        "# values_grams_VS_WT103 is a list of 3 lists (one for\n",
        "# n=2, one for n=3, one for n=4) each one with 4 elements (first element is \n",
        "# the percentage of unique n-grams of BERTlarge VS WT103, second element is the \n",
        "# percentage of unique n-grams of BERTbase VS WT103, third element is the percentage\n",
        "# of unique n-grams of GPT VS WT103, fourth element is the percentage of unique\n",
        "# n-grams of WT103 VS WT103)\n",
        "# initialization:\n",
        "values_grams_VS_WT103 = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
        "values_grams_VS_WT103[0] = ['BERTlarge', 'BERTbase', 'GPT', 'WT103']\n",
        "\n",
        "# values_grams_VS_TBC is a list of 3 lists (one for\n",
        "# n=2, one for n=3, one for n=4) each one with 4 elements (first element is \n",
        "# the percentage of unique n-grams of BERTlarge VS TBC, second element is the \n",
        "# percentage of unique n-grams of BERTbase VS TBC, third element is the percentage\n",
        "# of unique n-grams of GPT VS TBC, fourth element is the percentage of unique\n",
        "# n-grams of WT103 VS TBC)\n",
        "# initialization:\n",
        "values_grams_VS_TBC = [[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
        "values_grams_VS_TBC[0] = ['BERTlarge', 'BERTbase', 'GPT', 'WT103']"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert-base-uncased\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccreOUDgOsi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0feff281-de5e-4d7e-bb4f-96c14f613cf0"
      },
      "source": [
        "\"\"\"THIS WHOLE CODE BLOCK HAS TO BE REPEATED FOR\n",
        "THE OTHER BERT MODEL VERSION TOO\"\"\"\n",
        "\n",
        "\n",
        "# BERT VS WT103\n",
        "pct_uniques = ref_unique_ngrams(bert_sents, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1) and (model_version == 'bert-base-uncased'): #'bert-large-uncased'\n",
        "      values_grams_VS_WT103[i-1][1] = 100 * pct_uniques[i]\n",
        "    elif (i != 1):  #'bert-large-uncased'\n",
        "      values_grams_VS_WT103[i-1][0] = 100 * pct_uniques[i]\n",
        "\n",
        "# BERT VS TBC\n",
        "pct_uniques = ref_unique_ngrams(bert_sents, tbc_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to TBC: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1) and (model_version == 'bert-base-uncased'): #'bert-large-uncased'\n",
        "      values_grams_VS_TBC[i-1][1] = 100 * pct_uniques[i]\n",
        "    elif (i != 1):  #'bert-large-uncased'\n",
        "      values_grams_VS_TBC[i-1][0] = 100 * pct_uniques[i]\n",
        "\n",
        "# BERT VS BERT\n",
        "pct_uniques = self_unique_ngrams(bert_sents, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1) and (model_version == 'bert-base-uncased'): #'bert-large-uncased'\n",
        "      values_grams_VS_SELF[i-1][1] = 100 * pct_uniques[i]\n",
        "    elif (i != 1):  #'bert-large-uncased'\n",
        "      values_grams_VS_SELF[i-1][0] = 100 * pct_uniques[i]"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT unique 1-grams relative to Wiki: 9.42\n",
            "BERT unique 2-grams relative to Wiki: 59.05\n",
            "BERT unique 3-grams relative to Wiki: 91.80\n",
            "BERT unique 4-grams relative to Wiki: 98.60\n",
            "BERT unique 1-grams relative to TBC: 12.40\n",
            "BERT unique 2-grams relative to TBC: 62.68\n",
            "BERT unique 3-grams relative to TBC: 92.53\n",
            "BERT unique 4-grams relative to TBC: 98.67\n",
            "BERT unique 1-grams relative to self: 12.38\n",
            "BERT unique 2-grams relative to self: 63.13\n",
            "BERT unique 3-grams relative to self: 92.38\n",
            "BERT unique 4-grams relative to self: 98.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "7U6QXg3qVyqo",
        "outputId": "1cd3f0db-ef0a-4ad8-802e-dca88dab3dcc"
      },
      "source": [
        "\"\"\"SAME AS THE PREVIOUS BLOCK, BUT WITH TABLE LINES COMMENTED\"\"\"\n",
        "# BERT VS WT103\n",
        "\"\"\"\n",
        "max_n = 4\n",
        "pct_uniques = ref_unique_ngrams(bert_sents, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "\n",
        "\n",
        "# BERT VS BERT\n",
        "pct_uniques = self_unique_ngrams(bert_sents, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"BERT unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "\"\"\""
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmax_n = 4\\npct_uniques = ref_unique_ngrams(bert_sents, wiki_data, max_n)\\nfor i in range(1, max_n + 1):\\n    print(\"BERT unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\\n\\n\\n# BERT VS BERT\\npct_uniques = self_unique_ngrams(bert_sents, max_n)\\nfor i in range(1, max_n + 1):\\n    print(\"BERT unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBMJXP99_2Vg"
      },
      "source": [
        "We understand from the table that the BERT with a higher number of parameters (BERT Large) gives better results than BERT with a standard number of parameters (BERT Base), infact the percentage of unique n-grams is always higher (for both n=2, n=3 and n=4), meaning more diverse generated sentences. With the same considerations we conclude that the generated words are more diverse with BERT than using GPT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Way1aShUOyMT",
        "outputId": "0a51366a-6bcf-4a0f-f902-c56a9baaef51"
      },
      "source": [
        "# GPT VS WT103\n",
        "pct_uniques = ref_unique_ngrams(openai_sents, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"GPT unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1):\n",
        "      values_grams_VS_WT103[i-1][2] = 100 * pct_uniques[i]\n",
        "\n",
        "# GPT VS TBC\n",
        "pct_uniques = ref_unique_ngrams(openai_sents, tbc_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"GPT unique %d-grams relative to TBC: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1):\n",
        "      values_grams_VS_TBC[i-1][2] = 100 * pct_uniques[i]\n",
        "\n",
        "# GPT VS GPT\n",
        "pct_uniques = self_unique_ngrams(openai_sents, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"GPT unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1):\n",
        "      values_grams_VS_SELF[i-1][2] = 100 * pct_uniques[i]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPT unique 1-grams relative to Wiki: 3.15\n",
            "GPT unique 2-grams relative to Wiki: 34.11\n",
            "GPT unique 3-grams relative to Wiki: 73.93\n",
            "GPT unique 4-grams relative to Wiki: 91.90\n",
            "GPT unique 1-grams relative to TBC: 2.10\n",
            "GPT unique 2-grams relative to TBC: 26.05\n",
            "GPT unique 3-grams relative to TBC: 66.06\n",
            "GPT unique 4-grams relative to TBC: 89.14\n",
            "GPT unique 1-grams relative to self: 4.42\n",
            "GPT unique 2-grams relative to self: 31.83\n",
            "GPT unique 3-grams relative to self: 68.60\n",
            "GPT unique 4-grams relative to self: 88.60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5I9WwAF5xs3"
      },
      "source": [
        "In the following block we fill the last row of table 2 (regarding WT103). In particular remember that the WT103 on the rows is a sample of 1000 sentences from the training dataset (we sampled after removing the titles from the original training dataset). In the column WT103 we consider a sample of 5000 words from the test set (it was given by the github part)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snUpf9qp58JR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fafbd2be-8254-4cc8-ec58-e9e16699d1e9"
      },
      "source": [
        "# WT103 (1000) VS WT103 (1000) (SELF)\n",
        "pct_uniques = self_unique_ngrams(wiki1000_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"WT103 unique %d-grams relative to self(1000): %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1):\n",
        "      values_grams_VS_SELF[i-1][3] = 100 * pct_uniques[i]\n",
        "\n",
        "# WT103 (1000) VS WT103 (5000)\n",
        "pct_uniques = ref_unique_ngrams(wiki1000_data, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"WT103 unique %d-grams relative to WT103(5000): %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1):\n",
        "      values_grams_VS_WT103[i-1][3] = 100 * pct_uniques[i]\n",
        "\n",
        "# WT103 (1000) VS TBC\n",
        "pct_uniques = ref_unique_ngrams(wiki1000_data,  tbc_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"WT103 unique %d-grams relative to TBC: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    if (i != 1):\n",
        "      values_grams_VS_TBC[i-1][3] = 100 * pct_uniques[i]"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WT103 unique 1-grams relative to self(1000): 7.05\n",
            "WT103 unique 2-grams relative to self(1000): 51.79\n",
            "WT103 unique 3-grams relative to self(1000): 85.86\n",
            "WT103 unique 4-grams relative to self(1000): 96.76\n",
            "WT103 unique 1-grams relative to WT103(5000): 7.52\n",
            "WT103 unique 2-grams relative to WT103(5000): 50.67\n",
            "WT103 unique 3-grams relative to WT103(5000): 85.55\n",
            "WT103 unique 4-grams relative to WT103(5000): 96.81\n",
            "WT103 unique 1-grams relative to TBC: 10.18\n",
            "WT103 unique 2-grams relative to TBC: 57.59\n",
            "WT103 unique 3-grams relative to TBC: 89.34\n",
            "WT103 unique 4-grams relative to TBC: 97.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnUg1u_4fula"
      },
      "source": [
        "__________\n",
        "__________\n",
        "# TABLES\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y5ExNaU4q6e"
      },
      "source": [
        "Since we want to replicate the results of the original paper, we can build a table where to put them, and we do that using the following code block. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cCC1IK_95KtH",
        "outputId": "54797034-8f76-4fae-fb7e-8636078b1570"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "# Three tables of the n-grams percentage agains SELF, WT103, TBC.\n",
        "Table2_n_grams_SELF = go.Figure(\n",
        "    data=[go.Table(\n",
        "        header=dict(values=TITLE1),\n",
        "        cells=dict(values=values_grams_VS_SELF))\n",
        "                     ])\n",
        "Table2_n_grams_SELF.show()\n",
        "# values_grams_VS_SELF is completed during the process.\n",
        "\n",
        "Table2_n_grams_WT103 = go.Figure(\n",
        "    data=[go.Table(\n",
        "        header=dict(values=TITLE2),\n",
        "        cells=dict(values=values_grams_VS_WT103))\n",
        "                     ])\n",
        "Table2_n_grams_WT103.show()\n",
        "# values_grams_VS_WT103 is completed during the process.\n",
        "\n",
        "Table2_n_grams_TBC = go.Figure(\n",
        "    data=[go.Table(\n",
        "        header=dict(values=TITLE3),\n",
        "        cells=dict(values=values_grams_VS_TBC))\n",
        "                     ])\n",
        "Table2_n_grams_TBC.show()\n",
        "# values_grams_VS_TBC is completed during the process."
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"243c594d-6991-466b-bd0e-38f3a9bd7bf5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"243c594d-6991-466b-bd0e-38f3a9bd7bf5\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '243c594d-6991-466b-bd0e-38f3a9bd7bf5',\n",
              "                        [{\"cells\": {\"values\": [[\"BERTlarge\", \"BERTbase\", \"GPT\", \"WT103\"], [0, 63.13419723259389, 31.827446600586345, 51.7874707155248], [0, 92.38087172538391, 68.59974149073676, 85.85515928508919], [0, 98.24250522891006, 88.59973384592637, 96.75639753475288]]}, \"header\": {\"values\": [\"Model\", \"% unique 2-grams vs Self\", \"% unique 3-grams vs Self\", \"% unique 4-grams vs Self\"]}, \"type\": \"table\"}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('243c594d-6991-466b-bd0e-38f3a9bd7bf5');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"78aa3616-0802-452d-9784-a0137fd6afc4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"78aa3616-0802-452d-9784-a0137fd6afc4\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '78aa3616-0802-452d-9784-a0137fd6afc4',\n",
              "                        [{\"cells\": {\"values\": [[\"BERTlarge\", \"BERTbase\", \"GPT\", \"WT103\"], [0, 59.05172413793104, 34.10582158313556, 50.665271742046244], [0, 91.79652213188798, 73.9279046388051, 85.55472351789533], [0, 98.59690913316291, 91.90004435901227, 96.80919824847287]]}, \"header\": {\"values\": [\"Model\", \"% unique 2-grams vs WT103\", \"% unique 3-grams vs WT103\", \"% unique 4-grams vs WT103\"]}, \"type\": \"table\"}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('78aa3616-0802-452d-9784-a0137fd6afc4');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"1e56c5bb-b01a-4f8d-939b-cbf0cafc12d3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"1e56c5bb-b01a-4f8d-939b-cbf0cafc12d3\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '1e56c5bb-b01a-4f8d-939b-cbf0cafc12d3',\n",
              "                        [{\"cells\": {\"values\": [[\"BERTlarge\", \"BERTbase\", \"GPT\", \"WT103\"], [0, 62.681199209312545, 26.053329610498395, 57.587138079653776], [0, 92.52766485998193, 66.06347838575327, 89.33678578839577], [0, 98.66662793399954, 89.13795652816798, 97.9416825220534]]}, \"header\": {\"values\": [\"Model\", \"% unique 2-grams vs TBC\", \"% unique 3-grams vs TBC\", \"% unique 4-grams vs TBC\"]}, \"type\": \"table\"}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1e56c5bb-b01a-4f8d-939b-cbf0cafc12d3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiCnhAAP8Ewo"
      },
      "source": [
        "We also add a table concerning Self-BLEU and one for Corpus-BLEU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUibpj258O7g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e7169f1-4868-4f6d-c8e3-c9a0ab29c60d"
      },
      "source": [
        "# Self-BLEU\n",
        "Table2_self_bleu = go.Figure(\n",
        "    data=[go.Table(\n",
        "        header=dict(values=TITLE_SELF),\n",
        "        cells=dict(values=values_self_bleu))\n",
        "                     ])\n",
        "Table2_self_bleu.show()\n",
        "# values_self_bleu is completed during the process.\n",
        "\n",
        "# Corpus - BLEU\n",
        "Table3_corpus_bleu = go.Figure(\n",
        "    data=[go.Table(\n",
        "        header=dict(values=TITLE_CORPUS),\n",
        "        cells=dict(values=values_corpus_bleu))\n",
        "                     ])\n",
        "Table3_corpus_bleu.show()\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"54f8e8a9-7e22-4d01-8444-82b8a23d01d7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"54f8e8a9-7e22-4d01-8444-82b8a23d01d7\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '54f8e8a9-7e22-4d01-8444-82b8a23d01d7',\n",
              "                        [{\"cells\": {\"values\": [[\"BERTlarge\", \"BERTbase\", \"GPT\", \"WT103\"], [0, 8.489368030007881, 38.08559860514548, 17.41572486098709]]}, \"header\": {\"values\": [\"Model\", \"Self-BLEU\"]}, \"type\": \"table\"}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('54f8e8a9-7e22-4d01-8444-82b8a23d01d7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"9cc39200-1d55-4747-8eb2-dccee68fbd97\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"9cc39200-1d55-4747-8eb2-dccee68fbd97\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '9cc39200-1d55-4747-8eb2-dccee68fbd97',\n",
              "                        [{\"cells\": {\"values\": [[\"BERTlarge\", \"BERTbase\", \"GPT\", \"WT103\"], [0, 8.509974777023338, 11.317101000095223, 15.183261709738813], [0, 7.043918694090595, 30.01719221013679, 6.039053452386266]]}, \"header\": {\"values\": [\"Model\", \"Corpus-BLEU against WT103\", \"Corpus-BLEU against TBC\"]}, \"type\": \"table\"}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9cc39200-1d55-4747-8eb2-dccee68fbd97');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwkD7Dq3_Vr5"
      },
      "source": [
        "# TEXYGEN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "104-3-zuIaCy"
      },
      "source": [
        "## Introductory commands"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YoeQObGBUNg",
        "outputId": "2fcbcc5d-5bb6-465b-bc19-52fd42a12885"
      },
      "source": [
        "!git clone https://github.com/geek-ai/Texygen.git\n",
        "%cd Texygen\n",
        "# we clone the Texygen repository from github\n",
        "# run SeqGAN with default setting"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Texygen'...\n",
            "remote: Enumerating objects: 888, done.\u001b[K\n",
            "remote: Total 888 (delta 0), reused 0 (delta 0), pack-reused 888\u001b[K\n",
            "Receiving objects: 100% (888/888), 21.85 MiB | 19.98 MiB/s, done.\n",
            "Resolving deltas: 100% (537/537), done.\n",
            "/content/Texygen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtJePk2KBXlS",
        "outputId": "cf96567d-c0a0-486a-dfe8-29ab62d5d8cd"
      },
      "source": [
        "!pip install -r requirements.txt\n",
        "#these are the libraries required for the Texygen models"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (0.4.4)\n",
            "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: tensorflow>=1.5.0 in /tensorflow-1.15.2/python3.7 (from -r requirements.txt (line 3)) (1.15.2)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.4.1)\n",
            "Requirement already satisfied: nltk>=3.2.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (3.6.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->-r requirements.txt (line 3)) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->-r requirements.txt (line 3)) (1.34.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.7 (from tensorflow>=1.5.0->-r requirements.txt (line 3)) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.7 (from tensorflow>=1.5.0->-r requirements.txt (line 3)) (1.15.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->-r requirements.txt (line 3)) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->-r requirements.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->-r requirements.txt (line 3)) (3.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->-r requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->-r requirements.txt (line 3)) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->-r requirements.txt (line 3)) (0.12.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->-r requirements.txt (line 3)) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow>=1.5.0->-r requirements.txt (line 3)) (0.36.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.7 (from tensorflow>=1.5.0->-r requirements.txt (line 3)) (1.15.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.3->-r requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.3->-r requirements.txt (line 5)) (4.41.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.3->-r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.3->-r requirements.txt (line 5)) (2019.12.20)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow>=1.5.0->-r requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.5.0->-r requirements.txt (line 3)) (57.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.5.0->-r requirements.txt (line 3)) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow>=1.5.0->-r requirements.txt (line 3)) (1.0.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow>=1.5.0->-r requirements.txt (line 3)) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow>=1.5.0->-r requirements.txt (line 3)) (4.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow>=1.5.0->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow>=1.5.0->-r requirements.txt (line 3)) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=c5461f427b0a6ca85ec0ed32e5d0ab33381538a34460fe3bdb05789798ae819c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "Successfully installed gast-0.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBrG9KdbNwd9"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "#Some functions of Texygen require the old version of tensorflow"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MT3GBMGO6qk",
        "outputId": "ca98d1a2-e210-4bdd-bda0-1c9a5a1efa86"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdWBcQ6YRN04"
      },
      "source": [
        "## Texygen tutorial:\n",
        "\n",
        "python main.py -g GAN type -t training method -d data location\n",
        "\n",
        "  -g GAN type : \n",
        "    specify the GAN type in the experiment\n",
        "\n",
        "    (GAN type = seqgan | maligan | rankgan | leakgan | gsgan | textgan | mle)\n",
        "\n",
        "  -t training method :\n",
        "    specify the traning method in the experiment\n",
        "\n",
        "    (training method = oracle | cfg | real  ;  default is oracle)\n",
        "\n",
        "  -d data location : \n",
        "    use user's own dataset only avaiable with real data training \n",
        "    (default is 'data/image_coco.txt')\n",
        "\n",
        "more details: https://github.com/geek-ai/Texygen/blob/master/docs/doc.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr9cKc2fGSG1"
      },
      "source": [
        "Now we apply the models which are inside Texygen: \n",
        "- seqgan \n",
        "- maligan \n",
        "- rankgan \n",
        "- leakgan \n",
        "- gsgan \n",
        "- textgan \n",
        "- mle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge3JRH5rIuuM"
      },
      "source": [
        "## Texygen models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ4imuHAGu9W"
      },
      "source": [
        "#!python main.py -g seqgan -t real -d '/content/Texygen/wiki_train_1000_samples.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erfEeXMZG-Dd"
      },
      "source": [
        "#!python main.py -g maligan -t real -d '/content/Texygen/wiki_train_1000_samples.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c74EDIAHAz6"
      },
      "source": [
        "#!python main.py -g rankgan -t real -d '/content/wiki_100.txt'  #'/content/Texygen/wiki_train_1000_samples.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG0voFRfHCv5"
      },
      "source": [
        "#!python main.py -g leakgan -t real -d '/content/Texygen/wiki_train_1000_samples.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP8VZrW6HH3M"
      },
      "source": [
        "#!python main.py -g gsgan -t real -d '/content/wiki_100' #'/content/Texygen/wiki_train_1000_samples.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nblqMNQMHL6U",
        "outputId": "531b10c3-86fe-48e1-d657-73b75a170377"
      },
      "source": [
        "!python main.py -g textgan -t real -d '/content/wiki_train_1000_samples.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Texygen/utils/utils.py:27: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/utils/utils.py:29: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2021-07-10 15:54:56.344612: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-07-10 15:54:56.344894: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557586a40a00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-07-10 15:54:56.344929: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-07-10 15:54:56.349430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-10 15:54:56.645305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 15:54:56.646076: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557586a40d80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-07-10 15:54:56.646109: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2021-07-10 15:54:56.647045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 15:54:56.647586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 15:54:56.658386: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 15:54:56.905280: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 15:54:57.025353: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 15:54:57.064719: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 15:54:57.308875: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 15:54:57.359145: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 15:54:57.858558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 15:54:57.858763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 15:54:57.859481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 15:54:57.860003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 15:54:57.862390: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 15:54:57.863762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 15:54:57.863791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 15:54:57.863801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 15:54:57.864628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 15:54:57.865276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 15:54:57.865828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /content/Texygen/utils/utils.py:30: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/models/textGan_MMD/Textgan.py:292: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/models/textGan_MMD/TextganDiscriminator.py:18: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/models/textGan_MMD/TextganDiscriminator.py:35: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/models/textGan_MMD/TextganDiscriminator.py:39: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/models/textGan_MMD/TextganDiscriminator.py:50: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/models/textGan_MMD/TextganDiscriminator.py:181: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/models/textGan_MMD/TextganDiscriminator.py:202: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /content/Texygen/models/textGan_MMD/TextganDiscriminator.py:208: The name tf.nn.xw_plus_b is deprecated. Please use tf.compat.v1.nn.xw_plus_b instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/models/textGan_MMD/TextganDiscriminator.py:140: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/models/textGan_MMD/TextganDiscriminator.py:131: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/Texygen/models/textGan_MMD/TextganDiscriminator.py:149: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/models/textGan_MMD/TextganDiscriminator.py:150: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/models/textGan_MMD/TextganGenerator.py:110: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "2021-07-10 15:55:09.236119: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "start pre-train generator:\n",
            "2021-07-10 15:57:35.455049: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2532425728 exceeds 10% of system memory.\n",
            "tcmalloc: large alloc 2532425728 bytes == 0x5575ad0c6000 @  0x7f577d84cb6b 0x7f577d86c379 0x7f5733e3b467 0x7f5733c28c4f 0x7f5733aee27b 0x7f5733ab3c66 0x7f5733ab4af3 0x7f5733ab4cc7 0x7f57381b9cb8 0x7f5733d6cc56 0x7f5733d5f2a5 0x7f5733d8bad8 0x7f5733d6011b 0x7f5733d6841c 0x7f5733d6b007 0x7f5733d6cf7f 0x7f5733d8bad8 0x7f5733d603b5 0x7f5733d62ece 0x7f5733d65983 0x7f5733d8d650 0x7f5733d50f1a 0x7f5733d8b342 0x7f573a03803c 0x7f573a039e83 0x7f573a03b767 0x7f5736e37cad 0x7f57374db6b3 0x7f57374dc3ca 0x7f5736e34c54 0x7f5736e34d42\n",
            "2021-07-10 15:57:36.498383: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2532425728 exceeds 10% of system memory.\n",
            "tcmalloc: large alloc 2532425728 bytes == 0x557643fe2000 @  0x7f577d84cb6b 0x7f577d86c379 0x7f5733e3b467 0x7f5733c28c4f 0x7f5733aef048 0x7f5733b03ca9 0x7f5733d8d9c4 0x7f5733d50f1a 0x7f5733d8b342 0x7f573a03803c 0x7f573a039e83 0x7f573a03b767 0x7f5736e37cad 0x7f57374db6b3 0x7f57374dc3ca 0x7f5736e34c54 0x7f5736e34d42 0x7f5736dedaee 0x557584c13c47 0x557584c13a50 0x557584c87be0 0x557584c1530a 0x557584c8360e 0x557584c827ad 0x557584b54eb1 0x557584c84bb5 0x557584c824ae 0x557584c153ea 0x557584c8360e 0x557584c827ad 0x557584c153ea\n",
            "2021-07-10 15:57:38.231647: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2532425728 exceeds 10% of system memory.\n",
            "tcmalloc: large alloc 2532425728 bytes == 0x5575ad0c6000 @  0x7f577d84cb6b 0x7f577d86c379 0x7f5733e3b467 0x7f5733c28c4f 0x7f5733aee27b 0x7f5733ab3c66 0x7f5733ab4af3 0x7f5733ab4cc7 0x7f57381b9cb8 0x7f5733d6cc56 0x7f5733d5f2a5 0x7f5733d8bad8 0x7f5733d6011b 0x7f5733d6841c 0x7f5733d6b007 0x7f5733d6cf7f 0x7f5733d8bad8 0x7f5733d603b5 0x7f5733d62ece 0x7f5733d65983 0x7f5733d8d650 0x7f5733d50f1a 0x7f5733d8b342 0x7f573a03803c 0x7f573a039e83 0x7f573a03b767 0x7f5736e37cad 0x7f57374db6b3 0x7f57374dc3ca 0x7f5736e34c54 0x7f5736e34d42\n",
            "2021-07-10 15:57:38.690227: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2532425728 exceeds 10% of system memory.\n",
            "tcmalloc: large alloc 2532425728 bytes == 0x557643fe2000 @  0x7f577d84cb6b 0x7f577d86c379 0x7f5733e3b467 0x7f5733c28c4f 0x7f5733aef048 0x7f5733b03ca9 0x7f5733d8d9c4 0x7f5733d50f1a 0x7f5733d8b342 0x7f573a03803c 0x7f573a039e83 0x7f573a03b767 0x7f5736e37cad 0x7f57374db6b3 0x7f57374dc3ca 0x7f5736e34c54 0x7f5736e34d42 0x7f5736dedaee 0x557584c13c47 0x557584c13a50 0x557584c87be0 0x557584c1530a 0x557584c8360e 0x557584c827ad 0x557584b54eb1 0x557584c84bb5 0x557584c824ae 0x557584c153ea 0x557584c8360e 0x557584c827ad 0x557584c153ea\n",
            "epoch:0\t time:23.808127641677856\n",
            "WARNING:tensorflow:From /content/Texygen/utils/metrics/DocEmbSim.py:124: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /content/Texygen/utils/metrics/DocEmbSim.py:127: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "2021-07-10 16:03:02.361858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:03:02.362324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 16:03:02.362505: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 16:03:02.362535: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 16:03:02.362558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 16:03:02.362579: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 16:03:02.362600: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 16:03:02.362620: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 16:03:02.362641: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 16:03:02.362739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:03:02.363037: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:03:02.363250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 16:03:02.364028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:03:02.364279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 16:03:02.364328: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 16:03:02.364351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 16:03:02.364372: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 16:03:02.364391: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 16:03:02.364411: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 16:03:02.364436: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 16:03:02.364457: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 16:03:02.364521: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:03:02.364789: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:03:02.365007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 16:03:02.365051: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 16:03:02.365065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 16:03:02.365076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 16:03:02.365164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:03:02.365425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:03:02.365644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "2021-07-10 16:03:13.549627: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:03:13.549946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 16:03:13.550039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 16:03:13.550070: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 16:03:13.550093: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 16:03:13.550117: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 16:03:13.550139: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 16:03:13.550159: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 16:03:13.550179: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 16:03:13.550265: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:03:13.550532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:03:13.550747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 16:03:13.550788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 16:03:13.550805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 16:03:13.550815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 16:03:13.550913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:03:13.551188: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:03:13.551412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 57.932119369506836\n",
            "time elapsed of nll-test: 5.975733995437622\n",
            "epoch:1\tEmbeddingSimilarity:-0.00022026942652534682\tnll-test:0.0\t\n",
            "epoch:1\t time:18.98748230934143\n",
            "epoch:2\t time:19.083366870880127\n",
            "epoch:3\t time:19.233667373657227\n",
            "epoch:4\t time:19.26416015625\n",
            "epoch:5\t time:19.302989959716797\n",
            "2021-07-10 16:10:43.217606: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:10:43.217929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 16:10:43.218054: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 16:10:43.218082: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 16:10:43.218104: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 16:10:43.218126: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 16:10:43.218147: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 16:10:43.218167: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 16:10:43.218188: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 16:10:43.218277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:10:43.218551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:10:43.218768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 16:10:43.218813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 16:10:43.218828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 16:10:43.218838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 16:10:43.218934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:10:43.219209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:10:43.219433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 49.44088411331177\n",
            "time elapsed of nll-test: 6.048191785812378\n",
            "epoch:6\tEmbeddingSimilarity:-0.0002195205685187245\tnll-test:0.0\t\n",
            "epoch:6\t time:19.06228756904602\n",
            "epoch:7\t time:19.01124596595764\n",
            "epoch:8\t time:19.082406044006348\n",
            "epoch:9\t time:19.017512559890747\n",
            "epoch:10\t time:18.90811800956726\n",
            "2021-07-10 16:18:10.863248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:18:10.863561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 16:18:10.863658: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 16:18:10.863682: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 16:18:10.863702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 16:18:10.863723: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 16:18:10.863750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 16:18:10.863771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 16:18:10.863792: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 16:18:10.863880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:18:10.864164: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:18:10.864373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 16:18:10.864449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 16:18:10.864472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 16:18:10.864482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 16:18:10.864578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:18:10.864849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:18:10.865082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 49.12154173851013\n",
            "time elapsed of nll-test: 6.047194957733154\n",
            "epoch:11\tEmbeddingSimilarity:-0.00021819921371887623\tnll-test:0.0\t\n",
            "epoch:11\t time:19.03496503829956\n",
            "epoch:12\t time:19.005298137664795\n",
            "epoch:13\t time:18.923141956329346\n",
            "epoch:14\t time:18.882197856903076\n",
            "epoch:15\t time:18.936028003692627\n",
            "2021-07-10 16:25:37.589959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:25:37.590324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 16:25:37.590536: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 16:25:37.590571: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 16:25:37.590594: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 16:25:37.590618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 16:25:37.590641: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 16:25:37.590675: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 16:25:37.590696: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 16:25:37.590781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:25:37.591074: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:25:37.591285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 16:25:37.591331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 16:25:37.591345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 16:25:37.591355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 16:25:37.591448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:25:37.591724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:25:37.591951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 49.02293109893799\n",
            "time elapsed of nll-test: 5.994826316833496\n",
            "epoch:16\tEmbeddingSimilarity:-0.00021847056667310063\tnll-test:0.0\t\n",
            "epoch:16\t time:18.989856481552124\n",
            "epoch:17\t time:18.8255455493927\n",
            "epoch:18\t time:19.048165559768677\n",
            "epoch:19\t time:18.982889413833618\n",
            "epoch:20\t time:18.79956293106079\n",
            "2021-07-10 16:33:04.407537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:33:04.407840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 16:33:04.407925: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 16:33:04.407955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 16:33:04.407987: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 16:33:04.408013: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 16:33:04.408034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 16:33:04.408054: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 16:33:04.408074: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 16:33:04.408158: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:33:04.408427: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:33:04.408639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 16:33:04.408690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 16:33:04.408704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 16:33:04.408715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 16:33:04.408811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:33:04.409083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:33:04.409306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 49.31644797325134\n",
            "time elapsed of nll-test: 5.868406772613525\n",
            "epoch:21\tEmbeddingSimilarity:-0.00022035800471663567\tnll-test:0.0\t\n",
            "epoch:21\t time:18.901597023010254\n",
            "epoch:22\t time:19.03984832763672\n",
            "epoch:23\t time:18.826267957687378\n",
            "epoch:24\t time:18.884727239608765\n",
            "epoch:25\t time:18.976259231567383\n",
            "2021-07-10 16:40:31.259475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:40:31.259796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 16:40:31.259890: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 16:40:31.259915: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 16:40:31.259936: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 16:40:31.259956: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 16:40:31.259991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 16:40:31.260011: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 16:40:31.260029: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 16:40:31.260113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:40:31.260386: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:40:31.260598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 16:40:31.260681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 16:40:31.260696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 16:40:31.260705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 16:40:31.260804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:40:31.261088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:40:31.261311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 48.973305463790894\n",
            "time elapsed of nll-test: 5.834512233734131\n",
            "epoch:26\tEmbeddingSimilarity:-0.0002179360541276358\tnll-test:0.0\t\n",
            "epoch:26\t time:19.10721516609192\n",
            "epoch:27\t time:18.83630657196045\n",
            "epoch:28\t time:19.227972269058228\n",
            "epoch:29\t time:18.99192214012146\n",
            "epoch:30\t time:18.91456437110901\n",
            "2021-07-10 16:47:57.196879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:47:57.197217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 16:47:57.197305: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 16:47:57.197330: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 16:47:57.197353: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 16:47:57.197373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 16:47:57.197397: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 16:47:57.197420: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 16:47:57.197441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 16:47:57.197523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:47:57.197804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:47:57.198025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 16:47:57.198072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 16:47:57.198086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 16:47:57.198096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 16:47:57.198193: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:47:57.198457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:47:57.198687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 49.02521252632141\n",
            "time elapsed of nll-test: 5.967400312423706\n",
            "epoch:31\tEmbeddingSimilarity:-0.00022007034555890514\tnll-test:0.0\t\n",
            "epoch:31\t time:18.932618856430054\n",
            "epoch:32\t time:19.240100145339966\n",
            "epoch:33\t time:19.232967376708984\n",
            "epoch:34\t time:19.002755880355835\n",
            "epoch:35\t time:18.928552865982056\n",
            "2021-07-10 16:55:24.934954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:55:24.935286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 16:55:24.935381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 16:55:24.935409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 16:55:24.935434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 16:55:24.935457: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 16:55:24.935480: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 16:55:24.935499: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 16:55:24.935519: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 16:55:24.935604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:55:24.935887: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:55:24.936108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 16:55:24.936156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 16:55:24.936170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 16:55:24.936179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 16:55:24.936406: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:55:24.936733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 16:55:24.936983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 48.947218894958496\n",
            "time elapsed of nll-test: 6.056424856185913\n",
            "epoch:36\tEmbeddingSimilarity:-0.00021680155078289566\tnll-test:0.0\t\n",
            "epoch:36\t time:18.779152393341064\n",
            "epoch:37\t time:19.049466133117676\n",
            "epoch:38\t time:18.877731800079346\n",
            "epoch:39\t time:18.862037897109985\n",
            "epoch:40\t time:19.090740203857422\n",
            "2021-07-10 17:02:51.956349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:02:51.956702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 17:02:51.956879: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 17:02:51.956913: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 17:02:51.956937: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 17:02:51.956961: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 17:02:51.956998: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 17:02:51.957020: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 17:02:51.957041: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 17:02:51.957132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:02:51.957415: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:02:51.957629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 17:02:51.957710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 17:02:51.957725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 17:02:51.957735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 17:02:51.957842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:02:51.958135: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:02:51.958367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 48.94450640678406\n",
            "time elapsed of nll-test: 5.9965105056762695\n",
            "epoch:41\tEmbeddingSimilarity:-0.0002183941565048187\tnll-test:0.0\t\n",
            "epoch:41\t time:19.116957187652588\n",
            "epoch:42\t time:19.025222301483154\n",
            "epoch:43\t time:19.065789461135864\n",
            "epoch:44\t time:18.926872968673706\n",
            "epoch:45\t time:19.10753321647644\n",
            "2021-07-10 17:10:19.254458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:10:19.254817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 17:10:19.254941: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 17:10:19.254985: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 17:10:19.255011: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 17:10:19.255037: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 17:10:19.255057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 17:10:19.255077: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 17:10:19.255097: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 17:10:19.255182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:10:19.255457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:10:19.255665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 17:10:19.255717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 17:10:19.255731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 17:10:19.255744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 17:10:19.255844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:10:19.256124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:10:19.256349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 49.24272561073303\n",
            "time elapsed of nll-test: 5.969889402389526\n",
            "epoch:46\tEmbeddingSimilarity:-0.00022041495017610665\tnll-test:0.0\t\n",
            "epoch:46\t time:18.843194007873535\n",
            "epoch:47\t time:19.091355085372925\n",
            "epoch:48\t time:19.058696031570435\n",
            "epoch:49\t time:18.893315076828003\n",
            "epoch:50\t time:19.02193284034729\n",
            "2021-07-10 17:17:48.041666: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:17:48.042014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 17:17:48.042107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 17:17:48.042135: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 17:17:48.042159: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 17:17:48.042182: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 17:17:48.042203: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 17:17:48.042224: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 17:17:48.042246: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 17:17:48.042329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:17:48.042608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:17:48.042834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 17:17:48.042881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 17:17:48.042896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 17:17:48.042906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 17:17:48.043016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:17:48.043290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:17:48.043523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 49.34296894073486\n",
            "time elapsed of nll-test: 5.897713661193848\n",
            "epoch:51\tEmbeddingSimilarity:-0.00021755079502848202\tnll-test:0.0\t\n",
            "epoch:51\t time:19.02273392677307\n",
            "epoch:52\t time:18.97919511795044\n",
            "epoch:53\t time:18.97109603881836\n",
            "epoch:54\t time:18.949058294296265\n",
            "epoch:55\t time:18.838557958602905\n",
            "2021-07-10 17:25:15.537332: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:25:15.537702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 17:25:15.537879: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 17:25:15.537908: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 17:25:15.537934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 17:25:15.537959: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 17:25:15.537993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 17:25:15.538016: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 17:25:15.538039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 17:25:15.538131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:25:15.538410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:25:15.538620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 17:25:15.538702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 17:25:15.538716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 17:25:15.538727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 17:25:15.538842: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:25:15.539156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:25:15.539395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 49.28183889389038\n",
            "time elapsed of nll-test: 6.020866394042969\n",
            "epoch:56\tEmbeddingSimilarity:-0.0002184003950663214\tnll-test:0.0\t\n",
            "epoch:56\t time:19.152225971221924\n",
            "epoch:57\t time:19.042349338531494\n",
            "epoch:58\t time:18.89968991279602\n",
            "epoch:59\t time:18.939287662506104\n",
            "epoch:60\t time:19.1406466960907\n",
            "2021-07-10 17:32:43.795879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:32:43.796224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 17:32:43.796318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 17:32:43.796347: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 17:32:43.796370: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 17:32:43.796394: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 17:32:43.796417: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 17:32:43.796438: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 17:32:43.796459: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 17:32:43.796543: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:32:43.796817: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:32:43.797037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 17:32:43.797117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 17:32:43.797132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 17:32:43.797142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 17:32:43.797298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:32:43.797572: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:32:43.797800: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 49.82220959663391\n",
            "time elapsed of nll-test: 5.959197282791138\n",
            "epoch:61\tEmbeddingSimilarity:-0.0002183891233050912\tnll-test:0.0\t\n",
            "epoch:61\t time:19.087546586990356\n",
            "epoch:62\t time:18.80804204940796\n",
            "epoch:63\t time:19.013609409332275\n",
            "epoch:64\t time:19.15391516685486\n",
            "epoch:65\t time:18.956506729125977\n",
            "2021-07-10 17:40:13.444981: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:40:13.445304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 17:40:13.445396: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 17:40:13.445422: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 17:40:13.445444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 17:40:13.445465: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 17:40:13.445485: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 17:40:13.445505: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 17:40:13.445525: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 17:40:13.445607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:40:13.445879: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:40:13.446098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 17:40:13.446144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 17:40:13.446157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 17:40:13.446167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 17:40:13.446271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:40:13.446534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:40:13.446779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 49.364047050476074\n",
            "time elapsed of nll-test: 6.160921573638916\n",
            "epoch:66\tEmbeddingSimilarity:-0.00021574197783020565\tnll-test:0.0\t\n",
            "epoch:66\t time:19.039337873458862\n",
            "epoch:67\t time:18.942298889160156\n",
            "epoch:68\t time:19.098289728164673\n",
            "epoch:69\t time:18.907374620437622\n",
            "epoch:70\t time:19.028050184249878\n",
            "2021-07-10 17:47:42.558453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:47:42.558820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 17:47:42.558997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 17:47:42.559031: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 17:47:42.559053: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 17:47:42.559074: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 17:47:42.559098: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 17:47:42.559120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 17:47:42.559142: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 17:47:42.559226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:47:42.559502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:47:42.559719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 17:47:42.559792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 17:47:42.559806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 17:47:42.559816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 17:47:42.559921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:47:42.560206: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:47:42.560432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 49.39222073554993\n",
            "time elapsed of nll-test: 5.911404132843018\n",
            "epoch:71\tEmbeddingSimilarity:-0.00021689701835145097\tnll-test:0.0\t\n",
            "epoch:71\t time:19.07777428627014\n",
            "epoch:72\t time:19.049768209457397\n",
            "epoch:73\t time:19.053635358810425\n",
            "epoch:74\t time:19.209189891815186\n",
            "epoch:75\t time:19.06340193748474\n",
            "2021-07-10 17:55:12.113233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:55:12.113560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 17:55:12.113660: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 17:55:12.113685: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 17:55:12.113712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 17:55:12.113734: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 17:55:12.113754: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 17:55:12.113776: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 17:55:12.113799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 17:55:12.113883: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:55:12.114184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:55:12.114410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 17:55:12.114456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 17:55:12.114472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 17:55:12.114482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 17:55:12.114579: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:55:12.114843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 17:55:12.115080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 49.53028130531311\n",
            "time elapsed of nll-test: 5.91685938835144\n",
            "epoch:76\tEmbeddingSimilarity:-0.00022118832433372284\tnll-test:0.0\t\n",
            "epoch:76\t time:19.104769229888916\n",
            "epoch:77\t time:19.135350465774536\n",
            "epoch:78\t time:19.002258777618408\n",
            "epoch:79\t time:18.979143619537354\n",
            "start pre-train discriminator:\n",
            "epoch:0\n",
            "2021-07-10 17:57:18.256267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "epoch:1\n",
            "epoch:2\n",
            "epoch:3\n",
            "epoch:4\n",
            "epoch:5\n",
            "epoch:6\n",
            "epoch:7\n",
            "epoch:8\n",
            "epoch:9\n",
            "epoch:10\n",
            "epoch:11\n",
            "epoch:12\n",
            "epoch:13\n",
            "epoch:14\n",
            "epoch:15\n",
            "epoch:16\n",
            "epoch:17\n",
            "epoch:18\n",
            "epoch:19\n",
            "epoch:20\n",
            "epoch:21\n",
            "epoch:22\n",
            "epoch:23\n",
            "epoch:24\n",
            "epoch:25\n",
            "epoch:26\n",
            "epoch:27\n",
            "epoch:28\n",
            "epoch:29\n",
            "epoch:30\n",
            "epoch:31\n",
            "epoch:32\n",
            "epoch:33\n",
            "epoch:34\n",
            "epoch:35\n",
            "epoch:36\n",
            "epoch:37\n",
            "epoch:38\n",
            "epoch:39\n",
            "epoch:40\n",
            "epoch:41\n",
            "epoch:42\n",
            "epoch:43\n",
            "epoch:44\n",
            "epoch:45\n",
            "epoch:46\n",
            "epoch:47\n",
            "epoch:48\n",
            "epoch:49\n",
            "epoch:50\n",
            "epoch:51\n",
            "epoch:52\n",
            "epoch:53\n",
            "epoch:54\n",
            "epoch:55\n",
            "epoch:56\n",
            "epoch:57\n",
            "epoch:58\n",
            "epoch:59\n",
            "epoch:60\n",
            "epoch:61\n",
            "epoch:62\n",
            "epoch:63\n",
            "epoch:64\n",
            "epoch:65\n",
            "epoch:66\n",
            "epoch:67\n",
            "epoch:68\n",
            "epoch:69\n",
            "epoch:70\n",
            "epoch:71\n",
            "epoch:72\n",
            "epoch:73\n",
            "epoch:74\n",
            "epoch:75\n",
            "epoch:76\n",
            "epoch:77\n",
            "epoch:78\n",
            "epoch:79\n",
            "adversarial training:\n",
            "epoch:80\t time:60.39793634414673\n",
            "2021-07-10 18:07:11.390971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:07:11.391349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 18:07:11.391524: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 18:07:11.391558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 18:07:11.391583: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 18:07:11.391610: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 18:07:11.391637: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 18:07:11.391658: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 18:07:11.391680: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 18:07:11.391776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:07:11.392078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:07:11.392292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 18:07:11.392377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 18:07:11.392392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 18:07:11.392403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 18:07:11.392512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:07:11.392796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:07:11.393037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 49.99365568161011\n",
            "time elapsed of nll-test: 5.860622882843018\n",
            "epoch:81\tEmbeddingSimilarity:-0.00021541542464560408\tnll-test:0.0\t\n",
            "epoch:81\t time:59.59294033050537\n",
            "epoch:82\t time:59.60453009605408\n",
            "epoch:83\t time:59.60201978683472\n",
            "epoch:84\t time:59.678916215896606\n",
            "epoch:85\t time:59.713144302368164\n",
            "2021-07-10 18:21:25.137047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:21:25.137394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 18:21:25.137530: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 18:21:25.137558: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 18:21:25.137584: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 18:21:25.137606: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 18:21:25.137628: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 18:21:25.137650: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 18:21:25.137674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 18:21:25.137770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:21:25.138057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:21:25.138260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 18:21:25.138309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 18:21:25.138323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 18:21:25.138333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 18:21:25.138431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:21:25.138711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:21:25.138939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 50.39392328262329\n",
            "time elapsed of nll-test: 6.013927221298218\n",
            "epoch:86\tEmbeddingSimilarity:-0.0002211247655302571\tnll-test:0.0\t\n",
            "epoch:86\t time:59.49338126182556\n",
            "epoch:87\t time:59.54927849769592\n",
            "epoch:88\t time:59.6269371509552\n",
            "epoch:89\t time:59.53890299797058\n",
            "epoch:90\t time:59.68890905380249\n",
            "2021-07-10 18:35:38.153530: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:35:38.153884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 18:35:38.154075: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 18:35:38.154107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 18:35:38.154129: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 18:35:38.154150: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 18:35:38.154170: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 18:35:38.154189: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 18:35:38.154209: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 18:35:38.154297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:35:38.154569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:35:38.154781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 18:35:38.154859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 18:35:38.154874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 18:35:38.154884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 18:35:38.155004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:35:38.155280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:35:38.155503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 50.585930585861206\n",
            "time elapsed of nll-test: 6.035764694213867\n",
            "epoch:91\tEmbeddingSimilarity:-0.00021480690073853172\tnll-test:0.0\t\n",
            "epoch:91\t time:59.78906536102295\n",
            "epoch:92\t time:59.56625986099243\n",
            "epoch:93\t time:59.55027484893799\n",
            "epoch:94\t time:59.606083154678345\n",
            "epoch:95\t time:59.568236112594604\n",
            "2021-07-10 18:49:52.054527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:49:52.054833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-10 18:49:52.054934: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-10 18:49:52.054958: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-10 18:49:52.054998: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-10 18:49:52.055021: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-10 18:49:52.055042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-10 18:49:52.055064: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-10 18:49:52.055089: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-10 18:49:52.055176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:49:52.055443: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:49:52.055648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-10 18:49:52.055694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-10 18:49:52.055709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-10 18:49:52.055719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-10 18:49:52.055825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:49:52.056103: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-10 18:49:52.056321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14257 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "time elapsed of EmbeddingSimilarity: 49.574798822402954\n",
            "time elapsed of nll-test: 5.919250249862671\n",
            "epoch:96\tEmbeddingSimilarity:-0.00021776898142460014\tnll-test:0.0\t\n",
            "epoch:96\t time:59.63972282409668\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_iBlRIEBZlb",
        "outputId": "b42bd9f4-ff08-459f-b6c5-f53978b39012"
      },
      "source": [
        "!python main.py -g mle -t real -d '/content/wiki_train_1000_samples_new.txt' #'/content/prova.txt'\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/Texygen/utils/utils.py:27: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/utils/utils.py:29: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2021-07-13 12:42:17.719746: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-07-13 12:42:17.719947: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558672aeaa00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-07-13 12:42:17.719976: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-07-13 12:42:17.721491: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-13 12:42:17.893599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-13 12:42:17.894302: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558672aead80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-07-13 12:42:17.894331: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2021-07-13 12:42:17.894504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-13 12:42:17.895035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-13 12:42:17.895336: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-13 12:42:17.896662: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-13 12:42:17.898023: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-13 12:42:17.898344: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-13 12:42:17.899638: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-13 12:42:17.900272: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-13 12:42:17.902979: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-13 12:42:17.903090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-13 12:42:17.903623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-13 12:42:17.904089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-13 12:42:17.904145: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-13 12:42:17.905266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-13 12:42:17.905298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-13 12:42:17.905313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-13 12:42:17.905435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-13 12:42:17.906009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-13 12:42:17.906515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 12799 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "WARNING:tensorflow:From /content/Texygen/utils/utils.py:30: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/models/mle/MleGenerator.py:25: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/models/mle/MleGenerator.py:134: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/models/mle/MleGenerator.py:32: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/models/mle/MleGenerator.py:54: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Texygen/models/mle/MleGenerator.py:55: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From /content/Texygen/models/mle/MleGenerator.py:99: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/Texygen/models/mle/MleGenerator.py:212: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "2021-07-13 12:42:21.027389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "start pre-train generator:\n",
            "2021-07-13 12:43:08.438530: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1787412480 exceeds 10% of system memory.\n",
            "tcmalloc: large alloc 1787412480 bytes == 0x558697a50000 @  0x7ff9d7366b6b 0x7ff9d7386379 0x7ff98d955467 0x7ff98d742c4f 0x7ff98d60827b 0x7ff98d5cdc66 0x7ff98d5ceaf3 0x7ff98d5cecc7 0x7ff991cd3cb8 0x7ff98d886c56 0x7ff98d8792a5 0x7ff98d8a5ad8 0x7ff98d87a11b 0x7ff98d88241c 0x7ff98d885007 0x7ff98d886f7f 0x7ff98d8a5ad8 0x7ff98d87a3b5 0x7ff98d87cece 0x7ff98d87f983 0x7ff98d8a7650 0x7ff98d86af1a 0x7ff98d8a5342 0x7ff993b5203c 0x7ff993b53e83 0x7ff993b55767 0x7ff990951cad 0x7ff990ff56b3 0x7ff990ff63ca 0x7ff99094ec54 0x7ff99094ed42\n",
            "2021-07-13 12:43:09.220039: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1787412480 exceeds 10% of system memory.\n",
            "tcmalloc: large alloc 1787412480 bytes == 0x5587022ec000 @  0x7ff9d7366b6b 0x7ff9d7386379 0x7ff98d955467 0x7ff98d742c4f 0x7ff98d609048 0x7ff98d61dca9 0x7ff98d8a79c4 0x7ff98d86af1a 0x7ff98d8a5342 0x7ff993b5203c 0x7ff993b53e83 0x7ff993b55767 0x7ff990951cad 0x7ff990ff56b3 0x7ff990ff63ca 0x7ff99094ec54 0x7ff99094ed42 0x7ff990907aee 0x558670443c47 0x558670443a50 0x5586704b7be0 0x55867044530a 0x5586704b360e 0x5586704b27ad 0x558670384eb1 0x5586704b4bb5 0x5586704b24ae 0x5586704453ea 0x5586704b360e 0x5586704b27ad 0x5586704453ea\n",
            "2021-07-13 12:43:10.443539: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1787412480 exceeds 10% of system memory.\n",
            "tcmalloc: large alloc 1787412480 bytes == 0x558697a50000 @  0x7ff9d7366b6b 0x7ff9d7386379 0x7ff98d955467 0x7ff98d742c4f 0x7ff98d60827b 0x7ff98d5cdc66 0x7ff98d5ceaf3 0x7ff98d5cecc7 0x7ff991cd3cb8 0x7ff98d886c56 0x7ff98d8792a5 0x7ff98d8a5ad8 0x7ff98d87a11b 0x7ff98d88241c 0x7ff98d885007 0x7ff98d886f7f 0x7ff98d8a5ad8 0x7ff98d87a3b5 0x7ff98d87cece 0x7ff98d87f983 0x7ff98d8a7650 0x7ff98d86af1a 0x7ff98d8a5342 0x7ff993b5203c 0x7ff993b53e83 0x7ff993b55767 0x7ff990951cad 0x7ff990ff56b3 0x7ff990ff63ca 0x7ff99094ec54 0x7ff99094ed42\n",
            "2021-07-13 12:43:10.776058: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 1787412480 exceeds 10% of system memory.\n",
            "tcmalloc: large alloc 1787412480 bytes == 0x5587022ec000 @  0x7ff9d7366b6b 0x7ff9d7386379 0x7ff98d955467 0x7ff98d742c4f 0x7ff98d609048 0x7ff98d61dca9 0x7ff98d8a79c4 0x7ff98d86af1a 0x7ff98d8a5342 0x7ff993b5203c 0x7ff993b53e83 0x7ff993b55767 0x7ff990951cad 0x7ff990ff56b3 0x7ff990ff63ca 0x7ff99094ec54 0x7ff99094ed42 0x7ff990907aee 0x558670443c47 0x558670443a50 0x5586704b7be0 0x55867044530a 0x5586704b360e 0x5586704b27ad 0x558670384eb1 0x5586704b4bb5 0x5586704b24ae 0x5586704453ea 0x5586704b360e 0x5586704b27ad 0x5586704453ea\n",
            "epoch:0\t time:16.835168600082397\n",
            "epoch:1\t\n",
            "epoch:1\t time:13.214566707611084\n",
            "epoch:2\t time:13.16005277633667\n",
            "epoch:3\t time:13.366889715194702\n",
            "epoch:4\t time:13.30487060546875\n",
            "epoch:5\t time:13.261307954788208\n",
            "epoch:6\t\n",
            "epoch:6\t time:13.161428451538086\n",
            "epoch:7\t time:13.284639596939087\n",
            "epoch:8\t time:13.304365873336792\n",
            "epoch:9\t time:13.340519905090332\n",
            "epoch:10\t time:13.171163558959961\n",
            "epoch:11\t\n",
            "epoch:11\t time:13.523531198501587\n",
            "epoch:12\t time:13.358283519744873\n",
            "epoch:13\t time:13.200417518615723\n",
            "epoch:14\t time:13.143805027008057\n",
            "epoch:15\t time:13.093379020690918\n",
            "epoch:16\t\n",
            "epoch:16\t time:13.297563552856445\n",
            "epoch:17\t time:13.186960697174072\n",
            "epoch:18\t time:13.237387657165527\n",
            "epoch:19\t time:13.577378511428833\n",
            "epoch:20\t time:13.27129054069519\n",
            "epoch:21\t\n",
            "epoch:21\t time:13.272919654846191\n",
            "epoch:22\t time:13.084656715393066\n",
            "epoch:23\t time:13.182525157928467\n",
            "epoch:24\t time:13.336758852005005\n",
            "epoch:25\t time:13.338847637176514\n",
            "epoch:26\t\n",
            "epoch:26\t time:13.766533136367798\n",
            "epoch:27\t time:13.15033507347107\n",
            "epoch:28\t time:13.105538368225098\n",
            "epoch:29\t time:13.310466766357422\n",
            "epoch:30\t time:13.198016166687012\n",
            "epoch:31\t\n",
            "epoch:31\t time:13.3113431930542\n",
            "epoch:32\t time:13.25663685798645\n",
            "epoch:33\t time:13.419115543365479\n",
            "epoch:34\t time:13.26735806465149\n",
            "epoch:35\t time:13.222089052200317\n",
            "epoch:36\t\n",
            "epoch:36\t time:13.13622498512268\n",
            "epoch:37\t time:13.233078479766846\n",
            "epoch:38\t time:13.228739023208618\n",
            "epoch:39\t time:13.043440103530884\n",
            "epoch:40\t time:13.154313325881958\n",
            "epoch:41\t\n",
            "epoch:41\t time:13.265862941741943\n",
            "epoch:42\t time:13.240724802017212\n",
            "epoch:43\t time:13.088424444198608\n",
            "epoch:44\t time:13.117598533630371\n",
            "epoch:45\t time:13.080785751342773\n",
            "epoch:46\t\n",
            "epoch:46\t time:13.324757099151611\n",
            "epoch:47\t time:13.184657573699951\n",
            "epoch:48\t time:13.34731674194336\n",
            "epoch:49\t time:13.214705228805542\n",
            "epoch:50\t time:13.196367740631104\n",
            "epoch:51\t\n",
            "epoch:51\t time:13.223230361938477\n",
            "epoch:52\t time:13.272674083709717\n",
            "epoch:53\t time:13.284062623977661\n",
            "epoch:54\t time:13.203058958053589\n",
            "epoch:55\t time:13.45402979850769\n",
            "epoch:56\t\n",
            "epoch:56\t time:13.150101900100708\n",
            "epoch:57\t time:13.223185062408447\n",
            "epoch:58\t time:13.147802829742432\n",
            "epoch:59\t time:13.35145902633667\n",
            "epoch:60\t time:13.409003973007202\n",
            "epoch:61\t\n",
            "epoch:61\t time:13.157483339309692\n",
            "epoch:62\t time:13.228469371795654\n",
            "epoch:63\t time:13.555173873901367\n",
            "epoch:64\t time:13.286096334457397\n",
            "epoch:65\t time:13.303513526916504\n",
            "epoch:66\t\n",
            "epoch:66\t time:13.032952547073364\n",
            "epoch:67\t time:13.17765498161316\n",
            "epoch:68\t time:13.352979183197021\n",
            "epoch:69\t time:13.208505392074585\n",
            "epoch:70\t time:13.098034143447876\n",
            "epoch:71\t\n",
            "epoch:71\t time:13.07951831817627\n",
            "epoch:72\t time:13.442742586135864\n",
            "epoch:73\t time:13.168330907821655\n",
            "epoch:74\t time:13.212751626968384\n",
            "epoch:75\t time:13.238598585128784\n",
            "epoch:76\t\n",
            "epoch:76\t time:13.535155534744263\n",
            "epoch:77\t time:13.270792007446289\n",
            "epoch:78\t time:13.087783336639404\n",
            "epoch:79\t time:13.330562591552734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y91ghKZ8Ipsk"
      },
      "source": [
        "## Texygen metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SbLcB6VWHmh"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.insert(1, os.path.join(\".\", \"Texygen/utils\"))"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OT2b8nAUpfu"
      },
      "source": [
        "import os\n",
        "from multiprocessing import Pool\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import SmoothingFunction                    \n",
        "\n",
        "# import Texygen metrics\n",
        "from utils.metrics.Metrics import Metrics\n",
        "\n",
        "from Texygen.utils.metrics.Bleu import Bleu\n",
        "from Texygen.utils.metrics.SelfBleu import SelfBleu\n",
        "from Texygen.utils.metrics.EmbSim import EmbSim\n",
        "from Texygen.utils.metrics.Nll import Nll\n",
        "from Texygen.utils.metrics.UniqueGram import UniqueGram\n"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_bJwDfreFaT"
      },
      "source": [
        "**BERT VALUES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ3IqVgdiIXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb88bba4-ecb0-47ad-cdd8-16470b68ee74"
      },
      "source": [
        "print(\"BERT-WIKI BLEU: %.2f\" % (100 *Bleu.get_score(Bleu('Bert_using_pytorch.txt', wiki103_file))))\n",
        "print(\"BERT-self-BLEU: %.2f\" % (100 *SelfBleu.get_score(SelfBleu('Bert_using_pytorch.txt')))) \n",
        "print(\"BERT-unique4grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('Bert_using_pytorch.txt',4))))\n",
        "print(\"BERT-unique3grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('Bert_using_pytorch.txt',3))))\n",
        "print(\"BERT-unique2grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('Bert_using_pytorch.txt',2))))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BERT-WIKI BLEU: 8.43\n",
            "BERT-self-BLEU: 16.27\n",
            "BERT-unique4grams: 3410.90\n",
            "BERT-unique3grams: 3383.60\n",
            "BERT-unique2grams: 2664.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q23EJZbceL9F"
      },
      "source": [
        "**GPT VALUES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrZpDMNWcv6T",
        "outputId": "8be9a5c8-4a9c-46ec-a0e8-34c185d60b80"
      },
      "source": [
        "print(\"GPT-WIKI BLEU: %.2f\" % (100 *Bleu.get_score(Bleu('openaitext.txt', wiki103_file)))) #openaitesto.txt\n",
        "print(\"GPT-self-BLEU: %.2f\" % (100 *SelfBleu.get_score(SelfBleu('openaitext.txt')))) \n",
        "print(\"GPT-unique4grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('openaitext.txt',4))))\n",
        "print(\"GPT-unique3grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('openaitext.txt',3))))\n",
        "print(\"GPT-unique2grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('openaitext.txt',2))))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPT-WIKI BLEU: 9.98\n",
            "GPT-self-BLEU: 49.60\n",
            "GPT-unique4grams: 3132.60\n",
            "GPT-unique3grams: 2688.70\n",
            "GPT-unique2grams: 1548.80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CmjIFLiePnS"
      },
      "source": [
        "**MLE VALUES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-q1Qkkbgf4uE",
        "outputId": "36a3eb59-bc8c-4fbe-ea0f-0b044f22ea03"
      },
      "source": [
        "print(\"MLE-WIKI BLEU: %.2f\" % (100 *Bleu.get_score(Bleu('/content/Texygen/save/test_file.txt', wiki103_file))))\n",
        "print(\"MLE-self-BLEU: %.2f\" % (100 *SelfBleu.get_score(SelfBleu('/content/Texygen/save/test_file.txt')))) \n",
        "print(\"MLE-unique4grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('/content/Texygen/save/test_file.txt',4))))\n",
        "print(\"MLE-unique3grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('/content/Texygen/save/test_file.txt',3))))\n",
        "print(\"MLE-unique2grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('/content/Texygen/save/test_file.txt',2))))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLE-WIKI BLEU: 12.61\n",
            "MLE-self-BLEU: 25.08\n",
            "MLE-unique4grams: 5390.45\n",
            "MLE-unique3grams: 4743.94\n",
            "MLE-unique2grams: 2420.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49IwJNf7exMF"
      },
      "source": [
        "**TEXTGAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8EgUgBbe6y5"
      },
      "source": [
        "print(\"TEXTGAN-WIKI BLEU: %.2f\" % (100 *Bleu.get_score(Bleu('/content/Texygen/save/test_file.txt', wiki103_file))))\n",
        "print(\"TEXTGAN-self-BLEU: %.2f\" % (100 *SelfBleu.get_score(SelfBleu('/content/Texygen/save/test_file.txt')))) \n",
        "print(\"TEXTGAN-uniqueNgrams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('/content/Texygen/save/test_file.txt',4))))\n",
        "print(\"TEXTGAN-uniqueNgrams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('/content/Texygen/save/test_file.txt',3))))\n",
        "print(\"TEXTGAN-uniqueNgrams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('/content/Texygen/save/test_file.txt',2))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsLu-Kg-1Jn8"
      },
      "source": [
        "ATTENTION on Self-BLEU: We observe some different results from those of the paper implementation. The reason is the fact that when using self-BLEU for the BERT model (those functions contained in the generation part of the BERT model) we compute self-BLEU by averaging the results of Corpus-BLEU for each sentence as hypothesis against all the other as references. Instead, in the self-BLEU inside the Texygen platform we have another definition of self-BLEU (slightly different, but brings to different results), infact the command sentence bleu is used.\n",
        "\n",
        "In the following we will use the Texygen implementation, but be aware of this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc1t5ZTtUqld"
      },
      "source": [
        "ATTENTION on unique n-grams: as in the case of self-BLEU, also in the unique-grams definition there are some differences of Texygen with respect to BERT measures. One of the main reasons is that the value at the denominator for the Texygen implementation corresponds to the number of sentences, whereas in the \"correct\" definition of unique-grams it should be the total number of n-grams, which is clearly a higher number (in BERT this second definition is proposed). As a consequence, in the case of Texygen we have really higher results that those obtained for BERT unique-grams definition.\n",
        "\n",
        "In the following we will use the Texygen implementation, but be aware of this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kufjGFoFsNuH"
      },
      "source": [
        "____________________\n",
        "____________________\n",
        "# COMPARISON WITH TRANSFORMER XL AND XL-NET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu8cRHd6fEpl"
      },
      "source": [
        "#!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Beq_WzXQcaGc",
        "outputId": "c6752097-7afc-408b-9d98-89aa9fd36b7f"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 43.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 43.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Installing collected packages: sacremoses, tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2PGQUQEccgR"
      },
      "source": [
        "from transformers import pipeline"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412,
          "referenced_widgets": [
            "97f2e42c4ad0443fb54074c7b10163f2",
            "a82a2033ed2c4d87a55b4943b1b79807",
            "73dad25b4ee74026b7b9c1dd2fbe53de",
            "984fab6eeae74104979b253b5bba6d90",
            "2e711981b5cc4f95a29538a153dde9bb",
            "b29e0e813c8b47f39b60a21e660eecac",
            "a25344d53dd24bcfb547d93d4555944e",
            "f4054d0270d14193bdcc4be53df32940",
            "54a8ed4c8669495ba133471d2a666d7e",
            "520e41a087ca462b8950ca30615886d7",
            "ab7a83feccfb4db78c3602c22c857135",
            "bf63514875ce438e849836fe98493c98",
            "2d2c821f8ee44afe9af64490c5461f26",
            "34b7e5e6f16549a0922ca6f827c3fdec",
            "5c77298c1f2e417b986f8135df1dcf41",
            "ec2d603f2c344259a684beba6832dff1",
            "f6f2255cc9044fd680637c7ae003bc06",
            "4e3b7e9b085542f08a14b4ac0f1b0eee",
            "ad13f591260245d2ae46c0c963e19e84",
            "0abcab9090964aba89d9196dfc9e1a0b",
            "2c7d2ec6e563464791b30c87b477ffd8",
            "69b91256452b48b5845101bb79a9b77b",
            "5817e10c0dbf45219a31ff237c8ec3f1",
            "c002a579ded949339d22948de24d88ca",
            "c0f7c73a3c114afca07baca94de99725",
            "f0096069ced449edb44706d423232738",
            "f9c1f82d4e95477b9fccb1c8537e973d",
            "ec96899043a14aa284db8a8d90d9f9d6",
            "d4e32612458d4217b8a1ed658cd868b9",
            "b289c6738a574c15b065d147e1ec8535",
            "5ef10844d1654073900dbde3977498f6",
            "7a9730d4298e45ef8cf1f95f6807dc8f",
            "9f466c8ea3e147abb01b14d46e1aa75f",
            "74c9b6ab13604d85823ecb061cf0131e",
            "7ea6ac39ed1e46c19fbb7c0c388831f5",
            "bdb61c1733684d2abb534a2b3897f5e6",
            "74db024bc472408487bb66d3b7bb904c",
            "d09702fa85f54c35b2876e2b51a58ede",
            "2d03a401fb074debba71ee51cc807a35",
            "59b4d6737dd84c96b862a23384f2b20e",
            "eccc4a015fb541558eea559ff3c1d3f5",
            "6527094f05b2445ebcdac49c76c8472c",
            "c5df7587b112422e921b2fea53e52605",
            "de24f3b7ee5540cba9cdabf677a00eaa",
            "682f24b019de4e44a04447c33ba50a8f",
            "92e97af53b9a4e02a66756cf2292a831",
            "58f58e699398417e8d7fe600a93bbeab",
            "05e33c3b29ca46828e1cee85a4c5a44f",
            "ef93fcbaf1eb405892e00f476f39e74b",
            "b26875cbcb84435f92130cfe6010cb81",
            "877b7adb5d9a4e179c57223159ba0d57",
            "186088f797f0433e9f98b0e20cc75274",
            "e2a522ae6b8447f7a5d8a02e9a7d31d3",
            "b8ac8b8affb049e38222ae57be44a524",
            "1d6fc6b716e64b719cd5accc444cb655",
            "5c1eef5dc7184fb1bf8ed248b828df08",
            "7d70a78f384a4e5b9249f63567987ae4",
            "89a74e752354461c89c9d22514744626",
            "4acc9247c1be4eb6a3fbc662973e4956",
            "b493ea3b2a5c4a50b4d1d907380b9c12",
            "bf3881f732064e74a449696c31ba70c4",
            "16c460f8a4b84e20bca66b3551cc0c7a",
            "0303512240364a00a52460b4f5b691f1",
            "232e7fa1015e41cb91909cee4b32e71b"
          ]
        },
        "id": "bLnWtAbvce_N",
        "outputId": "2ca7128c-319a-4c8b-ef95-387fc7bd1f17"
      },
      "source": [
        "transfo_xl_generator = pipeline('text-generation', model='transfo-xl-wt103')\n",
        "xlnet_generator = pipeline('text-generation', model='xlnet-base-cased')"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97f2e42c4ad0443fb54074c7b10163f2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=856.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54a8ed4c8669495ba133471d2a666d7e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1140884800.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6f2255cc9044fd680637c7ae003bc06",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=9143470.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0f7c73a3c114afca07baca94de99725",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=9143613.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f466c8ea3e147abb01b14d46e1aa75f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eccc4a015fb541558eea559ff3c1d3f5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=467042463.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef93fcbaf1eb405892e00f476f39e74b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=798011.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d70a78f384a4e5b9249f63567987ae4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1382015.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx9YZtZ6cmg3",
        "outputId": "8b255e23-44c5-4654-ab74-c37683a676ca"
      },
      "source": [
        "with open('wt40.txt') as f:\n",
        "  content = f.readlines()\n",
        "f2 = open('transfxl_gen.txt', 'w')\n",
        "f3 = open('xlnet_gen.txt', 'w')\n",
        "\n",
        "for line in content:\n",
        "  #prompt = line[:-1]\n",
        "  prompt = line.rstrip('\\n')\n",
        "  res2 = transfo_xl_generator(prompt, max_length=40, do_sample=True, temperature=0.9)\n",
        "  print('TRANSFO_XL:'+res2[0]['generated_text'])\n",
        "  res3 = transfo_xl_generator(prompt, max_length=40, do_sample=True, temperature=0.9)\n",
        "  print('XL_NET:'+res3[0]['generated_text']+'\\n')\n",
        "  f2.write(res2[0]['generated_text']+\"\\n\")\n",
        "  f3.write(res3[0]['generated_text']+\"\\n\")\n",
        "\n",
        "f2.close()\n",
        "f3.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRANSFO_XL:The film was made in an attempt to portray the characters on a more realistic level â a conscious decision by the studio to make an adventure film. In the end, the filmmakers intended for Rasputin to be\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "XL_NET:The film was made in an effort to promote the unity of East and West Asia.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRANSFO_XL:Bacteria often function as multicellular aggregates that produce cells with which to divide and divide.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "XL_NET:Bacteria often function as multicellular aggregates that contain organelles called cells in the brain. It is this cell layer that gives rise to the modern term \"apoptosis\".\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRANSFO_XL:The largest oil spill involving the empress Maria Theresa of Austria, which occurred over a span of nearly ten years, was the result of Rasputin's influence on Tsar Alexander II. \"If you and my son\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "XL_NET:The largest oil spill involving the Bolsheviks occurred in 1890, when the newly elected General Secretary of the Russian Union, Dmitry Medvedev, launched the Black Sea Sharks campaign to overthrow the Russian Bolsheviks. The campaign proved\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRANSFO_XL:The Mojave Solar Project is a vast, worldwide project to create the Solar System. The project, funded by donations from Europe, China, and America, began in the 1960s.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "XL_NET:The Mojave Solar Project is a series of lunar observatory projects conducted by the United States Geological Survey that aims to observe the Moon through space.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRANSFO_XL:At the 1933 World Scout Jamboree in Saint Petersburg, Rasputin was promoted to the rank of deacon.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "XL_NET:At the 1933 World Scout Jamboree, held in KrakÃ³w, Poland, Rasputin was asked to be an apprentice boy. On October 25, 1933, Rasputin left KrakÃ³w for the Imperial summer camp in northern Russia.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRANSFO_XL:Lindemann arrived at the Blohm & Voss dockyard in Hamburg, Germany, in 1884; he later admitted that he had been unable to imagine living in the city. Following a few months he returned to England and\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "XL_NET:Lindemann arrived at the Blohm & Voss shipyard to become head of the Wilhelm Fuchs Fuchs-Henschel plant, where he was able to work on production of atomic bombs.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TRANSFO_XL:Soon after the establishment of the Russian Empire on May 21, 1917, Rasputin joined the Imperial Russian Army and became their \"second-in-command\". When the Bolsheviks began the Siege of Riga in\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCfOxjyUgp5c"
      },
      "source": [
        "##Comparison using Texygen metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "henoT71n98lU",
        "outputId": "09931a51-007e-4fd0-93a3-d6a38646bb7d"
      },
      "source": [
        "print(\"transfoxl-WIKI BLEU: %.2f\" % (100 *Bleu.get_score(Bleu('transfxl_gen.txt', wiki103_file))))\n",
        "print(\"transfoxl-self-BLEU: %.2f\" % (100 *SelfBleu.get_score(SelfBleu('transfxl_gen.txt')))) \n",
        "print(\"transfoxl-unique4grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('transfxl_gen.txt',4))))\n",
        "print(\"transfoxl-unique3grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('transfxl_gen.txt',3))))\n",
        "print(\"transfoxl-unique2grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('transfxl_gen.txt',2))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "transfoxl-WIKI BLEU: 11.65\n",
            "transfoxl-self-BLEU: 12.26\n",
            "transfoxl-unique4grams: 2985.00\n",
            "transfoxl-unique3grams: 3030.00\n",
            "transfoxl-unique2grams: 2805.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fj2iGPMAIv0e",
        "outputId": "2942ba97-188e-4ba3-c565-21966ab5461c"
      },
      "source": [
        "print(\"xlnet-WIKI BLEU: %.2f\" % (100 *Bleu.get_score(Bleu('xlnet_gen.txt', wiki103_file))))\n",
        "print(\"xlnet-self-BLEU: %.2f\" % (100 *SelfBleu.get_score(SelfBleu('xlnet_gen.txt')))) \n",
        "print(\"xlnet-unique4grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('xlnet_gen.txt',4))))\n",
        "print(\"xlnet-unique3grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('xlnet_gen.txt',3))))\n",
        "print(\"xlnet-unique2grams: %.2f\" % (100 *UniqueGram.get_score(UniqueGram('xlnet_gen.txt',2))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xlnet-WIKI BLEU: 13.71\n",
            "xlnet-self-BLEU: 11.78\n",
            "xlnet-unique4grams: 3015.00\n",
            "xlnet-unique3grams: 3057.50\n",
            "xlnet-unique2grams: 2800.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpnbsHrCg1-L"
      },
      "source": [
        "##Comparison using metrics from the Evaluation part above\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gVkML33htIH"
      },
      "source": [
        "wiki103_file = 'datawiki103.5k.txt'\n",
        "wiki_data = prepare_wiki(wiki103_file)\n",
        "transfxl_data = prepare_data('transfxl_gen.txt')\n",
        "xlnet_data = prepare_data('xlnet_gen.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlrm8xRAiSXb",
        "outputId": "b3b233e8-bbd9-4aaf-8cad-e3ed075e5810"
      },
      "source": [
        "#TRANSFORMER-XL EVALUATION (VS WIKI AND SELF)\n",
        "\n",
        "value = corpus_bleu(transfxl_data, wiki_data)\n",
        "print(\"transfoXL-WIKI BLEU: %.2f\" % (100 * value))\n",
        "value = self_bleu(transfxl_data)\n",
        "print(\"transfoXL self-BLEU: %.2f\" % (100 * value))\n",
        "\n",
        "pct_uniques = ref_unique_ngrams(transfxl_data, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"transfoXL unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "\n",
        "pct_uniques = self_unique_ngrams(transfxl_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"transfoXL unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "transfoXL-WIKI BLEU: 12.08\n",
            "transfoXL self-BLEU: 5.25\n",
            "transfoXL unique 1-grams relative to Wiki: 18.77\n",
            "transfoXL unique 2-grams relative to Wiki: 62.11\n",
            "transfoXL unique 3-grams relative to Wiki: 90.89\n",
            "transfoXL unique 4-grams relative to Wiki: 98.76\n",
            "transfoXL unique 1-grams relative to self: 41.39\n",
            "transfoXL unique 2-grams relative to self: 86.42\n",
            "transfoXL unique 3-grams relative to self: 97.61\n",
            "transfoXL unique 4-grams relative to self: 99.43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJF7TejDmZYE",
        "outputId": "6bce01f1-e3f7-4274-acdf-213a98d5ae59"
      },
      "source": [
        "#XL-NET EVALUATION (VS WIKI AND SELF)\n",
        "\n",
        "value = corpus_bleu(xlnet_data, wiki_data)\n",
        "print(\"XLNet-WIKI BLEU: %.2f\" % (100 * value))\n",
        "value = self_bleu(xlnet_data)\n",
        "print(\"XLNet self-BLEU: %.2f\" % (100 * value))\n",
        "\n",
        "pct_uniques = ref_unique_ngrams(xlnet_data, wiki_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"XLNet unique %d-grams relative to Wiki: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "\n",
        "pct_uniques = self_unique_ngrams(xlnet_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"XLNet unique %d-grams relative to self: %.2f\" % (i, 100 * pct_uniques[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XLNet-WIKI BLEU: 14.35\n",
            "XLNet self-BLEU: 0.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "XLNet unique 1-grams relative to Wiki: 18.96\n",
            "XLNet unique 2-grams relative to Wiki: 61.54\n",
            "XLNet unique 3-grams relative to Wiki: 89.92\n",
            "XLNet unique 4-grams relative to Wiki: 98.29\n",
            "XLNet unique 1-grams relative to self: 42.44\n",
            "XLNet unique 2-grams relative to self: 85.94\n",
            "XLNet unique 3-grams relative to self: 98.17\n",
            "XLNet unique 4-grams relative to self: 99.81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cit155gmjt1",
        "outputId": "ef90ffb8-e052-4f81-a06a-22aa9a4f2d0c"
      },
      "source": [
        "#XL-NET vs TRANSFORMER-XL\n",
        "\n",
        "value = corpus_bleu(xlnet_data, transfxl_data)\n",
        "print(\"XLNet-transfoXL BLEU: %.2f\" % (100 * value))\n",
        "value = corpus_bleu(transfxl_data, xlnet_data)\n",
        "print(\"transfoXL-XLNet BLEU: %.2f\" % (100 * value))\n",
        "\n",
        "pct_uniques = ref_unique_ngrams(xlnet_data, transfxl_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"XLNet unique %d-grams relative to transfoXL: %.2f\" % (i, 100 * pct_uniques[i]))\n",
        "    pct_uniques = ref_unique_ngrams(transfxl_data, xlnet_data, max_n)\n",
        "for i in range(1, max_n + 1):\n",
        "    print(\"transfoXL unique %d-grams relative to XLNet: %.2f\" % (i, 100 * pct_uniques[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XLNet-transfoXL BLEU: 24.84\n",
            "transfoXL-XLNet BLEU: 25.87\n",
            "XLNet unique 1-grams relative to transfoXL: 31.00\n",
            "XLNet unique 2-grams relative to transfoXL: 68.68\n",
            "XLNet unique 3-grams relative to transfoXL: 81.69\n",
            "XLNet unique 4-grams relative to transfoXL: 86.63\n",
            "transfoXL unique 1-grams relative to XLNet: 29.48\n",
            "transfoXL unique 2-grams relative to XLNet: 68.68\n",
            "transfoXL unique 3-grams relative to XLNet: 81.69\n",
            "transfoXL unique 4-grams relative to XLNet: 86.63\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}